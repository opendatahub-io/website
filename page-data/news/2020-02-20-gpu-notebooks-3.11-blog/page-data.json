{"componentChunkName":"component---src-templates-create-pages-js","path":"/news/2020-02-20-gpu-notebooks-3.11-blog","result":{"data":{"markdownRemark":{"html":"<h2>GPU-enabled notebooks in Open Data Hub</h2>\n<p>Open Data Hub 0.5.0 <a href=\"https://gitlab.com/opendatahub/opendatahub-operator/-/blob/v0.5.0/docs/enabling-gpu-aicoe-jupyterhub.adoc\">introduced support for utilizing NVIDIA GPUs</a> from Jupyter notebooks. GPU are hardware accelerators that dramatically increase the performance of model training and inference for deep learning applications such as image classification, medical healthcare diagnosis, and autonomous vehicles, to name a few. These hardware accelerators can efficiently perform complex matrix multiplication operations at a rate that far exceeds standard CPU-based operations. The Open Data Hub operator and its GPU notebook support is designed to work seamlessly across OpenShift 3 and OpenShift 4 deployments.</p>\n<h2>Enablement by the Operator</h2>\n<p>There has been an evolution to GPU enablement in OpenShift that has progressed from privileged pods to special SELinux security policies to finally a model that shields the pod from any particular security context considerations. The Open Data Hub operator has <a href=\"https://gitlab.com/opendatahub/opendatahub-operator/-/blob/v0.5.1/deploy/crds/opendatahub_v1alpha1_opendatahub_cr.yaml#L27\">configuration settings</a> that instruct the JupyterHub notebook spawner as to which pod specification should be applied.</p>\n<ul>\n<li>none (current default for OpenShift 4 deployments)</li>\n<li>selinux (3.11 mode)</li>\n<li>privileged (legacy mode for 3.10 deployments)</li>\n</ul>\n<p>The <code>selinux</code> mode is the most detailed and is dependent on specific installation of SELinux policies as described <a href=\"https://github.com/zvonkok/origin-ci-gpu/blob/release-3.11/doc/How%20to%20use%20GPUs%20with%20DevicePlugin%20in%20OpenShift%203.11%20.pdf\">here</a>. The <a href=\"https://blog.openshift.com/how-to-use-gpus-with-deviceplugin-in-openshift-3-10/\">3.10 GPU enablement</a> is similar but relies on the GPU pods running privileged containers.</p>\n<h2>3.11 issue</h2>\n<p>Data scientists sometimes include various Python packages in their notebook images and pods, some of which include process and task distribution libraries. Those types of libraries typically are designed to run in non-Kubernetes platforms and may rely on access to IPC constructs that are typically not used outside container communication within a pod. However, if a notebook pod has been scheduled to a node that has multiple GPU devices available, there is certainly nothing precluding using such a distribution mechanism locally.</p>\n<p>The community recently discovered an issue with the <code>selinux</code> mode of GPU notebook enablement in OpenShift 3.11. In the spawned Jupyter notebook pod, there are two containers, nbviewer and the notebook container. When a Open Data Hub user tried to initialize the dask.distributed library in their GPU notebook, they would encounter the following error:</p>\n<pre><code>  File \"/opt/app-root/src/miniconda/envs/pytorch/lib/python3.7/multiprocessing/synchronize.py\", line 59, in __init__\n    unlink_now)\nFileNotFoundError: [Errno 2] No such file or directory\n</code></pre>\n<p>It's not apparent from the stacktrace but the underlying issue is that two key directories for IPC, <code>/dev/shm</code> and <code>/dev/mqueue</code>, receive SELinux labels from Docker that are inconsistent with the rest of the container file system. Thus, they are not writable by the dask.distributed IPC library at initialization.</p>\n<p>Workarounds:</p>\n<ul>\n<li>Use <a href=\"https://docs.openshift.com/container-platform/3.11/crio/crio_runtime.html\">CRI-O instead of Docker</a> for the container runtime. Ad-hoc testing indicated that the problem didn't occur with CRI-O.</li>\n<li>Use privileged GPU pods instead. Note that this provides elevated capabilities for a pod and should only be done after serious consideration of your own security requirements.</li>\n</ul>\n<p>Finally, there is a workaround that could be applied in the <a href=\"https://gitlab.com/opendatahub/opendatahub-operator/-/tree/v0.5.1/roles/aicoe-jupyterhub#modifying-jupyterhub-server-behavior\">JupyterHub custom ConfigMap</a>. The goal is to modify the security context for the GPU pod spec such that we ensure that a common MCS category is applied to all the containers in the pod.</p>\n<pre><code>apiVersion: v1\ndata:\n  jupyterhub_config.py: |\n    from kubernetes.client import V1Capabilities, V1SELinuxOptions\n    spawner = c.OpenShiftSpawner\n    def mcs_selinux_profile(spawner, pod):\n      # Apply profile from singleuser-profiles\n      apply_pod_profile(spawner, pod)\n      if spawner.gpu_mode and spawner.gpu_mode == \"selinux\" and \\\n           spawner.extra_resource_limits and \"nvidia.com/gpu\" in spawner.extra_resource_limits:\n        # Currently a bug in RHEL Docker 1.13 whereby /dev IPC dirs get inconsistent MCS\n        pod.spec.security_context.se_linux_options = V1SELinuxOptions(type='nvidia_container_t',level='s0')\n      return pod\n    spawner.modify_pod_hook = mcs_selinux_profile\nkind: ConfigMap\nmetadata:\n  name: jupyterhub-mcs\n</code></pre>"}},"pageContext":{"permalink":"/news/2020-02-20-gpu-notebooks-3.11-blog"}},"staticQueryHashes":["1804438722"]}