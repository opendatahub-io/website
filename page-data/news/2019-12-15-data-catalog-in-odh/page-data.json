{"componentChunkName":"component---src-templates-create-pages-js","path":"/news/2019-12-15-data-catalog-in-odh","result":{"data":{"markdownRemark":{"html":"<h2>Data Catalog</h2>\n<p>In the Open Data Hub v0.5.0 release, we introduced Data Catalog. It is a set of components to\nread data stored in Data Lakes, create tables and query them in a SQL-like style. You can find\nbelow a picture of the simplified architecture of Data Catalog:</p>\n<p><img src=\"../../assets/img/posts/2019-10-10-data-catalog-in-odh/Data_Catalog_Simplified_Diagram.png\" alt=\"alt text\" title=\"Data Catalog architecture\"></p>\n<p>These are the components that are part of Data Catalog:</p>\n<ul>\n<li>Hive Metastore, responsible for maintaining the table metadata created by the user to query the data stored in Ceph/S3</li>\n<li>Spark SQL Thrift server to enable an endpoint where clients can connect using an ODBC/JDBC connection</li>\n<li>Cloudera Hue as a Data Exploration tool to explore the Data Lake, create tables and query them. You can\nalso create dashboards using the tables managed by Hive Metastore</li>\n</ul>\n<p>In the next section we'll cover each component in details.</p>\n<h2>Hive Metastore</h2>\n<p>The Hive Metastore component is part of Apache Hive and stores all metadata related to Hive tables. A Hive table\nis a special structure where you can expose your data, whether they are unstructred, semi-structured or\nstructured data, as a table. That way you can use SQL syntax to query this data directly from where the data\nis stored without the need to use ETL workflows to insert your data into a relational or NoSQL Database.</p>\n<p>One of the advantages of using Hive tables is you don't need to write a full ETL (Extract-Transform-Load) workflow\nto have your data available to read from traditional SQL clients. It is possible to partition the data according to\nyour needs too.</p>\n<p>With Hive Metastore, you can catalog your data in order to extract the best from your Data Lake.</p>\n<h2>Spark SQL Thrift Server</h2>\n<p>The main feature in Spark SQL Thrift Server is to use the power of Spark SQL and Dataframes to query the data from a\nData Lake, by creating a query plan and run in a Spark cluster to extract all information needed.</p>\n<p>In order to use the Spark SQL features, Thrift Server exposes a ODBC/JDBC endpoint so clients like SquirrelSQL, Tableau,\nSuperset or any SQL client can connect to it and issue SQL statements using raw data stored in your Data Lake.</p>\n<h2>Cloudera Hue</h2>\n<p>Cloudera Hue is a Data Exploration tool where you can explore your Data Lake for the files stored and issue SQL statements\nin a Hive instance. Data Catalog combines Hive Metastore and Spark SQL Thrift Server to have all Hive features.</p>\n<p>With Hue, you can explore the raw data from the Data Lake, create a set of hive tables and expose them to Data Scientists\nto create models based on that data.</p>\n<h2>Sample Use Cases</h2>\n<ol>\n<li>Data Exploration: You can explore your Data Lake to look at the data structure, as well as creating a table structure to\nquery them.</li>\n<li>Data Catalogging: After exploring the data and creating tables based on them, you can document the data by creating metadata\nagainst it. With that, you can share with Data Scientists the tables so they can understand what are the features they need\nto create their models.</li>\n</ol>\n<h2>Further improvements</h2>\n<p>With Cloudera Hue in the Data Catalog architecture, it is possible to create visualizations with the data too. As for now, this\nfeature is disabled as some other components are required to make it work. The Open Data Hub team will evalute the option to include\nthis Hue feature in Data Catalog. Superset can be a good replacement for Data Visualization tasks, or Jupyterhub.</p>"}},"pageContext":{"permalink":"/news/2019-12-15-data-catalog-in-odh"}},"staticQueryHashes":["1804438722"]}