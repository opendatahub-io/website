{"componentChunkName":"component---src-templates-create-pages-js","path":"/news/2019-08-21-sentiment-analysis-blog","result":{"data":{"markdownRemark":{"html":"<h2>Table of Contents</h2>\n<ul>\n<li><a href=\"#introduction\">Introduction</a></li>\n<li><a href=\"#what-is-sentiment-analysis\">What is Sentiment analysis</a></li>\n<li><a href=\"#our-approaches\">Our Approaches</a></li>\n<li><a href=\"#open-source-nlp-tools-for-sentiment-analysis\">Open-source NLP tools for sentiment analysis</a></li>\n<li><a href=\"#deep-learning-for-sentiment-analysis\">Deep Learning for sentiment analysis</a></li>\n<li><a href=\"#bert-for-sentiment-analysis\">BERT for sentiment analysis</a></li>\n<li><a href=\"#infrastructure-requirements-for-building-the-service\">Infrastructure requirements for building the service</a></li>\n<li><a href=\"#continuous-improvement-and-incorporating-feedback\">Continuous improvement and incorporating feedback</a></li>\n</ul>\n<h2>Introduction</h2>\n<p>Traditionally, sentiment analysis and opinion mining are techniques used by organizations to ascertain customer sentiments about their products, brand and services. With growing availability of opinion-rich resources like customer service surveys, reviews, blogs and engagement metrics there is an ever increasing opportunity for actively using the available resources in order to understand customer sentiments and opinions. </p>\n<p>This also poses a challenge when it comes to narrowing down to a system which can generalize to a wide variety of use cases. Also much of the Cognitive and Artificial Intelligence (AI) systems need infrastructures to support their training, development and maintenance. </p>\n<p>This blog outlines our approach to improving the sentiment analysis service at Red Hat, through continuous learning and discusses how we evolved the system to learn, adapt and produce desirable outcomes for a multitude of use cases. Through this blog, we also demonstrate the approaches we took and our results on comparative analyses of the approaches.</p>\n<h2>What is Sentiment analysis</h2>\n<p>Sentiment Analysis is a technique to identify emotional states and polarity from human language. These tasks, pertaining to extracting sentiments from a piece of text, often about a certain topic fall under the field of Natural language processing (NLP). NLP is a range of computational techniques for the automatic analysis and representation of human language. It is closely linked to the fields of artificial intelligence (AI) and computational linguistics. </p>\n<p><img src=\"../../assets/img/posts/2019-08-21-sentiment-analysis-blog/sentiment_analysis_example.png\" alt=\"alt text\" title=\"An example of sentence classification of tweets into positive and negative sentiments.\"></p>\n<p>There are various approaches that can be taken to build a sentiment analysis service. One of the most intuitive approaches is that of building of a textual analysis system. Such a system comprises of analyzing a piece of text based on the terms appearing in the text and building a rule-based or fact-based system around categorizing pieces of text into various sentiment classes. The other approach, which is mainly how most of the state-of-the-art systems work is a machine learning and deep learning based approach. In this approach, rather than teaching a system to make a decision based on a set of rules, we expose the system to a variety of examples and train it to learn from the examples and then draw predictions on the basis of those. </p>\n<p>In the following sections, we go over details of the techniques mentioned above and explain how we leveraged each of the above techniques, into iteratively improving our system at Red Hat and narrowing down to a deep learning based approach in the end, and how it fits our use-case.</p>\n<h2>Our Approaches</h2>\n<p>In this section, we describe in detail, the approaches we took for sentiment analysis and the results we got at each step, and how we improved the sentiment analysis service iteratively.</p>\n<h4>Open-source NLP tools for Sentiment Analysis</h4>\n<p>Our initial approach to sentiment analysis was building a service which can detect sentiments from  customer reviews using three open-source NLP tools, Stanford CoreNLP,  Vader Sentiment Processor and TextBlob.</p>\n<p>The CoreNLP model is built using a Recurrent Neural Network trained on a tree based corpus called 'Stanford Sentiment Treebank' which is a fully labeled parse tree that allows for a complete analysis of the compositional effects of sentiment in language. On the other hand Vader is a lexicon based and rule based approach at sentiment analysis mainly targeted towards social media text. TextBlob is a classifier based approach, where a Naive Bayes classifier is used for a multiclass classification. </p>\n<p>The initial approach involved sentiment calculation using the CoreNLP Annotator with an additional validation step performed on the annotated results by passing sentences which are classified as negative by CoreNLP, through Vader and Textblob for negative sentiment validation(nsv). </p>\n<p>The results of the above approach are shown in the figure below. We are displaying F1-scores, which is calculated from Precision and Recall values for each sentiment. Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. Recall is the ratio of correctly predicted positive observations to the all observations in actual class. </p>\n<p><img src=\"../../assets/img/posts/2019-08-21-sentiment-analysis-blog/metrics-for-initial-approach.png\" alt=\"alt text\" title=\"F-1 scores of initially developed service using open source NLP tools\"></p>\n<p>F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account.  </p>\n<p>A core limitation of this method was, that it ignores the fact that we are in fact, treating the 3 models as competitive or alternate NLP models to treat a sentence, whereas, there is a significant difference between the 3 models in terms of their accuracy at sentence classification and their ability  to truly identify a certain label positive, negative, or neutral.\nIn order to improve this service, we created a service, which is an ensemble of the 3 models, Stanford CoreNLP, Vader and TextBlob and created an ensemble model by giving weighted scores to the 3 services. So, we  give scores to the three services, based on the precision values of the 3 services when they are used independently to calculate sentiment on a training dataset. </p>\n<p>The results of this approach are shown in the figure below. We are displaying the F1-scores of the ensembled model when tested on a test dataset.</p>\n<p><img src=\"../../assets/img/posts/2019-08-21-sentiment-analysis-blog/metrics-for-weighted-approach.png\" alt=\"alt text\" title=\"F-1 scores of service  using model stacking approach with weighted precision scores\"></p>\n<p>As we can see from the results in the above figure, as compared to the results of the primary approach as shown in the previous figure, there were improvements in the service by using a model stacking method with weighted average.</p>\n<h4>Deep Learning for Sentiment Analysis</h4>\n<p>As Young et al. point out in, for decades NLP problems were tackled using common machine learning approaches like SVMs, and logistic regression trained on very high dimensional and sparse features. However, recently, studies by Mikolov and Socher show, neural network based approaches are showing superior results in NLP using dense vectors and word embeddings. Deep learning is an application of artificial neural networks that allows computational models that are composed of multiple layers to learn representations of data. </p>\n<p>Andrew Ng., a pioneer in the field of machine learning points out -\n“for most flavors of the old generations of learning algorithms … performance will plateau. … deep learning … is the first class of algorithms … that is scalable. … performance just keeps getting better as you feed them more data.”</p>\n<p>Much after neural networks were introduced in 1958, in the late 1990s, the research community started to lose interest in neural networks mainly because they were regarded as only practical for “shallow” neural networks ( with one or two layers), since training a deep neural network is computationally very  expensive. However, recently, deep learning has produced state-of-the-art results in many application domains, starting from computer vision, speech recognition and most recently, natural language processing.</p>\n<p>The renaissance of neural networks as pointed out by Lei Zhang, can be attributed to factors like availability of computing power due to advances in hardware eg. GPUs, availability of huge amounts of training data and introduction of learning intermediate representations.</p>\n<p>And today, these very factors help us develop better systems and more advanced models to tackle NLP tasks. </p>\n<p>In order to understand how deep learning can be applied to customer reviews, we need to first think about the kind of data we are feeding into the system, which in our case is text, or an input string. But in order to apply mathematical operations like dot products, matrix multiplications etc, instead of a string input, we need to convert each word in the string into a vector. These vectors can be created in a way such as to represent the context, meaning and semantics. And, in order to create these vectors, which are called, word embeddings, we use word vector generation models like Word2Vec and GloVe. This gives us an embedding matrix, that contains vectors for each distinct word in the training corpus. We started with building a Recurrent Neural Network model (RNN) with Long short term memory units for sentiment analysis. A recurrent neural network is a bit different from a traditional feedforward neural network. The main difference is the temporality of an RNN and thus they are ideal for sequential data like sentences and text. LSTM ( Long short term memory ) units are modules that we can place inside of RNNs. At a high level, they make sure we are able to encapsulate long term dependencies in the text.</p>\n<h4>BERT for Sentiment Analysis</h4>\n<p>A big challenge in NLP is the shortage of training data. Most modern deep learning techniques benefit from large amounts of training data, that is, in hundreds of thousands and millions. However, since NLP is a very diversified field with many distinct tasks, there is a shortage of task specific datasets. This includes customer service review datasets, survey datasets, operational data etc.</p>\n<p>To close this gap, a technique called Bidirectional Encoder Representation from Transformers (BERT) was developed for training general purpose language representation models using enormous amount of unannotated text on the web ( known as pre-training) by researchers at Google.</p>\n<p><img src=\"../../assets/img/posts/2019-08-21-sentiment-analysis-blog/bert-architecture.png\" alt=\"alt text\" title=\"Pre - training general purpose language representation models on huge unannotated text and Fine tuning on customer review dataset\"></p>\n<p>The pre-trained model can then be fined tuned on smaller datasets for performing sentiment analysis.</p>\n<p>The advantage that this gives us is that, by using the principles of transfer learning, the universal properties that a language model possesses when exposed to a huge amount of data could be used in our case where there is a lack of annotated datasets.</p>\n<p>We use pre-trained BERT models and fine-tune it on our comparatively smaller dataset. This allows us to take advantage of feature extraction that happens in the front layers of the network without developing that feature extraction network from scratch.</p>\n<p><img src=\"../../assets/img/posts/2019-08-21-sentiment-analysis-blog/bert-results.png\" alt=\"alt text\" title=\"F-1 Scores of Positive and Negative sentiments run on BERT based model\"></p>\n<h5>Results using the BERT model</h5>\n<p>The BERT based model performs better than our RNN based approach and predicts positive sentiments with a 0.92 F1 Score and 0.73 F1 Score. This showed significant improvements as compared to all our previous approaches. Another advantage of this approach is that, it lets us get a single sentiment annotation for an entire customer review, by encapsulating very long term dependencies in text.  Moreover, the process of building the service highly improves for the BERT based deep learning model by using GPUs for training the model. We discuss this in more detail in the next section.</p>\n<h2>Infrastructure requirements for building the service</h2>\n<p><img src=\"../../assets/img/posts/2019-08-21-sentiment-analysis-blog/cpu-vs-gpu.png\" alt=\"alt text\" title=\"CPU vs GPU and comparison of training time\"></p>\n<p>So, as we fine-tune a sentiment analysis model, with pre-trained BERT parameters by training it on a large annotated dataset, we introduce large computational operations in terms of memory. To compute the data efficiently, we need infrastructures that can handle the computation processes in minimum time. </p>\n<p>We need GPUs to solve these problems since neural networks heavily rely on floating point matrix multiplication. Also, deep learning algorithms require a lot of data to train, thus they need large memory and high speed networks to complete in a reasonable amount of time.</p>\n<p>Although, less expensive as compared to the pre-training procedure, the fine-tuning step when run on a CPU with 4 cores and 8 Gigabytes of RAM takes about 3 hours to train. The same model when trained using an NVIDIA P100 GPU with 3584 cores and 16 Gigabytes of Memory, trains the model in only about 6 minutes.</p>\n<p>This speeds up the overall workflow of training, testing , hyperparameter tuning and thus building of the service.</p>\n<p>Also, for training the models and running predictions we use the Jupyterhub installation of the Open Data Hub which have GPU support. Jupyter notebooks allow running code, documenting, visualization in the same environment which makes the process of training and prototyping more flexible.</p>\n<h6>Links to sample code and model</h6>\n<p>The code used for training and testing and the saved model can be found in this <a href=\"https://gitlab.com/opendatahub/sample-models/tree/master/sentiment-analysis\">repository</a>.</p>\n<h2>Continuous improvement and incorporating feedback</h2>\n<p><img src=\"../../assets/img/posts/2019-08-21-sentiment-analysis-blog/feedback.png\" alt=\"alt text\" title=\"Feedback system for the sentiment analysis service\"></p>\n<p>The lifecycle of developing AI systems, does not end with building the first iteration of the system. We improve the system and continuously evolve it by making it learn from a feedback system. </p>\n<p>In our Sentiment Analysis project at Red Hat, after deployment of the service, we introduce additional steps of monitoring how our service is performing, capturing feedback from users, and including it back in building our ground truth, our model training and evaluation process.</p>\n<p>For this purpose, we built a feedback system which lets users correctly annotate the captured results from running the service on data for any false predictions that might be observed. </p>\n<p>As we see in the figure above, the results of the feedback annotation are introduced back in the training of the model in the second supervised learning phase, therefore, making our training datasets larger and more robust and enabling us to generate more context specific data which can be introduced in the training phase of the model, thus continually improving the service.</p>"}},"pageContext":{"permalink":"/news/2019-08-21-sentiment-analysis-blog"}},"staticQueryHashes":["1804438722"]}