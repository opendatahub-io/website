{"componentChunkName":"component---src-pages-news-js","path":"/news/","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"frontmatter":{"title":"Open Data Hub 0.6.1 Release Guide","date":"28 May 2020","preview":"Open Data Hub 0.6.1 was just released with many bug fixes and some new exciting features. It is also the first release where OpenShift CI helped us to review the contributions.","permalink":"/news/2020-05-28-odh-release-0.6.1-blog"},"html":"<h2>What is new?</h2>\n<p>Open Data Hub 0.6.1 is now available in OpenShift OperatorHub. It comes with many important bug fixes in the operator, some new components and integration with OpenShift CI.</p>\n<p>We have also added READMEs for all the components to make sure it is easy for users to orient themselves in the new manifests structure.</p>\n<p>The components added or (re)enabled in this version are:</p>\n<table>\n<thead>\n<tr>\n<th>Technology</th>\n<th>Version</th>\n<th>Category</th>\n<th>Deployment Method</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://github.com/opendatahub-io/odh-manifests/tree/master/ai-library\">AI Library</a></td>\n<td>0.6.0</td>\n<td>Machine Learning</td>\n<td>Manifests</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/opendatahub-io/odh-manifests/tree/master/odhseldon\">Seldon</a></td>\n<td>1.1.0</td>\n<td>Model Serving</td>\n<td>OLM</td>\n</tr>\n</tbody>\n</table>\n<p>{:class=\"table table-bordered table-striped\"}</p>\n<p>We have also enabled OpenShift CI for our odh-manifests repository to help us with PR reviews. We have a basic set of tests and are <a href=\"https://github.com/orgs/opendatahub-io/projects/4\">expanding the test set</a> to make sure that your PRs are reviewed faster in a more automated fashion.</p>\n<h2>Read More</h2>\n<p>You can read more in a blog post we have published on <a href=\"https://developers.redhat.com/blog/2020/06/02/open-data-hub-0-6-1-bug-fix-release-to-smooth-out-redesign-regressions/\">developers.redhat.com</a>.</p>"}},{"node":{"frontmatter":{"title":"Open Data Hub 0.6.0 Release Guide","date":"07 May 2020","preview":"Open Data Hub 0.6.0 was just released with a complete redesign based on Kubeflow and some component improvements and additions.","permalink":"/news/2020-05-07-odh-release-0.6.0-blog"},"html":"<h2>What is new?</h2>\n<p>Open Data Hub 0.6.0 adopted Kubeflow as upstream which lead to conversion from Ansible Operator to Kustomize for component deployment and management.</p>\n<p>All key project repositories moved to Github <a href=\"https://github.com/opendatahub-io/\">opendatahub-io organization</a>:</p>\n<ul>\n<li><a href=\"https://github.com/opendatahub-io/opendatahub-operator/\">https://github.com/opendatahub-io/opendatahub-operator/</a></li>\n<li><a href=\"https://github.com/opendatahub-io/odh-manifests/\">https://github.com/opendatahub-io/odh-manifests/</a></li>\n</ul>\n<p>Most of the components were converted to Kustomize and moved to the above mentioned repositories.</p>\n<p>Another important change is that ODH 0.6 and forward relies on Operator Lifecycle Manager to install and manage some of the components to avoid duplication of operator deployment manifests which are already available through OLM.</p>\n<p>The following is a list of tools are available in ODH 0.6.0:</p>\n<table>\n<thead>\n<tr>\n<th>Technology</th>\n<th>Version</th>\n<th>Category</th>\n<th>Deployment Method</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://github.com/opendatahub-io/odh-manifests/tree/master/airflow\">Airflow</a></td>\n<td>Alpha</td>\n<td>Workflow Management</td>\n<td>Manifests</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/opendatahub-io/odh-manifests/tree/master/grafana\">Grafana</a></td>\n<td>2.0.0</td>\n<td>Monitoring Dashboards</td>\n<td>OLM</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/opendatahub-io/odh-manifests/tree/master/jupyterhub\">JupyterHub</a></td>\n<td>3.0.7</td>\n<td>Data science tools</td>\n<td>Manifests</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/opendatahub-io/odh-manifests/tree/master/kafka\">Kafka Strimzi</a></td>\n<td>0.17.0</td>\n<td>Distributed Streaming</td>\n<td>OLM</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/opendatahub-io/odh-manifests/tree/master/odhargo\">Argo</a></td>\n<td>2.7.0</td>\n<td>Workflow Engine</td>\n<td>Manifests</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/opendatahub-io/odh-manifests/tree/master/prometheus\">Prometheus</a></td>\n<td>0.32.0</td>\n<td>Monitoring</td>\n<td>OLM</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/opendatahub-io/odh-manifests/tree/master/radanalyticsio\">Radanalytics Spark Operator</a></td>\n<td>1.0.7</td>\n<td>Operator for managing Spark cluster on OpenShift</td>\n<td>Manifests</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/opendatahub-io/odh-manifests/tree/master/superset\">Apache Superset</a></td>\n<td>0.34.0</td>\n<td>Data Exploration and Visualization Tool</td>\n<td>Manifests</td>\n</tr>\n</tbody>\n</table>\n<p>{:class=\"table table-bordered table-striped\"}</p>\n<h2>Read More</h2>\n<p>You can read more in a blog post we have published on <a href=\"https://developers.redhat.com/blog/2020/05/07/open-data-hub-0-6-brings-component-updates-and-kubeflow-architecture/\">developers.redhat.com</a>.</p>"}},{"node":{"frontmatter":{"title":"GPU-enabled notebooks in Open Data Hub","date":"20 Feb 2020","preview":"GPU support was added recently to Open Data Hub. This article will go into a bit more detail about using this capability, including a known issue in OpenShift 3.11.","permalink":"/news/2020-02-20-gpu-notebooks-3.11-blog"},"html":"<h2>GPU-enabled notebooks in Open Data Hub</h2>\n<p>Open Data Hub 0.5.0 <a href=\"https://gitlab.com/opendatahub/opendatahub-operator/-/blob/v0.5.0/docs/enabling-gpu-aicoe-jupyterhub.adoc\">introduced support for utilizing NVIDIA GPUs</a> from Jupyter notebooks. GPU are hardware accelerators that dramatically increase the performance of model training and inference for deep learning applications such as image classification, medical healthcare diagnosis, and autonomous vehicles, to name a few. These hardware accelerators can efficiently perform complex matrix multiplication operations at a rate that far exceeds standard CPU-based operations. The Open Data Hub operator and its GPU notebook support is designed to work seamlessly across OpenShift 3 and OpenShift 4 deployments.</p>\n<h2>Enablement by the Operator</h2>\n<p>There has been an evolution to GPU enablement in OpenShift that has progressed from privileged pods to special SELinux security policies to finally a model that shields the pod from any particular security context considerations. The Open Data Hub operator has <a href=\"https://gitlab.com/opendatahub/opendatahub-operator/-/blob/v0.5.1/deploy/crds/opendatahub_v1alpha1_opendatahub_cr.yaml#L27\">configuration settings</a> that instruct the JupyterHub notebook spawner as to which pod specification should be applied.</p>\n<ul>\n<li>none (current default for OpenShift 4 deployments)</li>\n<li>selinux (3.11 mode)</li>\n<li>privileged (legacy mode for 3.10 deployments)</li>\n</ul>\n<p>The <code>selinux</code> mode is the most detailed and is dependent on specific installation of SELinux policies as described <a href=\"https://github.com/zvonkok/origin-ci-gpu/blob/release-3.11/doc/How%20to%20use%20GPUs%20with%20DevicePlugin%20in%20OpenShift%203.11%20.pdf\">here</a>. The <a href=\"https://blog.openshift.com/how-to-use-gpus-with-deviceplugin-in-openshift-3-10/\">3.10 GPU enablement</a> is similar but relies on the GPU pods running privileged containers.</p>\n<h2>3.11 issue</h2>\n<p>Data scientists sometimes include various Python packages in their notebook images and pods, some of which include process and task distribution libraries. Those types of libraries typically are designed to run in non-Kubernetes platforms and may rely on access to IPC constructs that are typically not used outside container communication within a pod. However, if a notebook pod has been scheduled to a node that has multiple GPU devices available, there is certainly nothing precluding using such a distribution mechanism locally.</p>\n<p>The community recently discovered an issue with the <code>selinux</code> mode of GPU notebook enablement in OpenShift 3.11. In the spawned Jupyter notebook pod, there are two containers, nbviewer and the notebook container. When a Open Data Hub user tried to initialize the dask.distributed library in their GPU notebook, they would encounter the following error:</p>\n<pre><code>  File \"/opt/app-root/src/miniconda/envs/pytorch/lib/python3.7/multiprocessing/synchronize.py\", line 59, in __init__\n    unlink_now)\nFileNotFoundError: [Errno 2] No such file or directory\n</code></pre>\n<p>It's not apparent from the stacktrace but the underlying issue is that two key directories for IPC, <code>/dev/shm</code> and <code>/dev/mqueue</code>, receive SELinux labels from Docker that are inconsistent with the rest of the container file system. Thus, they are not writable by the dask.distributed IPC library at initialization.</p>\n<p>Workarounds:</p>\n<ul>\n<li>Use <a href=\"https://docs.openshift.com/container-platform/3.11/crio/crio_runtime.html\">CRI-O instead of Docker</a> for the container runtime. Ad-hoc testing indicated that the problem didn't occur with CRI-O.</li>\n<li>Use privileged GPU pods instead. Note that this provides elevated capabilities for a pod and should only be done after serious consideration of your own security requirements.</li>\n</ul>\n<p>Finally, there is a workaround that could be applied in the <a href=\"https://gitlab.com/opendatahub/opendatahub-operator/-/tree/v0.5.1/roles/aicoe-jupyterhub#modifying-jupyterhub-server-behavior\">JupyterHub custom ConfigMap</a>. The goal is to modify the security context for the GPU pod spec such that we ensure that a common MCS category is applied to all the containers in the pod.</p>\n<pre><code>apiVersion: v1\ndata:\n  jupyterhub_config.py: |\n    from kubernetes.client import V1Capabilities, V1SELinuxOptions\n    spawner = c.OpenShiftSpawner\n    def mcs_selinux_profile(spawner, pod):\n      # Apply profile from singleuser-profiles\n      apply_pod_profile(spawner, pod)\n      if spawner.gpu_mode and spawner.gpu_mode == \"selinux\" and \\\n           spawner.extra_resource_limits and \"nvidia.com/gpu\" in spawner.extra_resource_limits:\n        # Currently a bug in RHEL Docker 1.13 whereby /dev IPC dirs get inconsistent MCS\n        pod.spec.security_context.se_linux_options = V1SELinuxOptions(type='nvidia_container_t',level='s0')\n      return pod\n    spawner.modify_pod_hook = mcs_selinux_profile\nkind: ConfigMap\nmetadata:\n  name: jupyterhub-mcs\n</code></pre>"}},{"node":{"frontmatter":{"title":"Open Data Hub 0.5.1 Release Guide","date":"16 Feb 2020","preview":"Open Data Hub 0.5.1 was just released with exciting new tools for different AI/ML platform phases. We added JupyterHub support for CUDA GPU images & notebooks, Apache Superset for data exploration & visualization and a data catalog role for deploying","permalink":"/news/2020-02-16-odh-release-0.5.1-blog"},"html":"<h2>What is included?</h2>\n<p>Open Data Hub 0.5.1 includes many new tools that are essential to a comprehensive AI/ML end-to-end platform. Open Data Hub is a meta-operator that can be installed on Openshift Container Platform 3.11 and 4.x.</p>\n<p>The following is a list of tools added to Open Data Hub in this release:</p>\n<table>\n<thead>\n<tr>\n<th>Technology</th>\n<th>Version</th>\n<th>Category</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>JupyterHub</td>\n<td>3.0.7</td>\n<td>Data science tools</td>\n</tr>\n<tr>\n<td>Radanalytics <a href=\"https://github.com/radanalyticsio/spark-operator\">Spark Operator</a></td>\n<td>1.0.5</td>\n<td>Operator for managing Spark cluster on OpenShift</td>\n</tr>\n<tr>\n<td>Apache <a href=\"https://github.com/apache/incubator-superset\">Superset</a></td>\n<td>0.34.0</td>\n<td>Data Exploration and Visualization Tool</td>\n</tr>\n</tbody>\n</table>\n<p>{:class=\"table table-bordered table-striped\"}</p>\n<p>You can review the release notes for components added in the previous v0.5.0 release <a href=\"https://opendatahub.io/news/2019-12-17/odh-release-0.5-blog.html\">here</a></p>\n<h2>AICoE-JupyterHub</h2>\n<p>AICoE-JupyterHub now has support for greater customization of the JupyterHub deployment and Spark cluster resources. In this release, we added </p>\n<ul>\n<li>Support for modifying the <a href=\"https://gitlab.com/opendatahub/opendatahub-operator/-/tree/master/roles/aicoe-jupyterhub\">JupyterHub server behavior</a> via a custom JupyterHub config</li>\n<li>Update the kubespawner library to version 0.11.1</li>\n<li>Support for specifying the resource requests and limits of the cpu/memory allocated to the Jupyter notebook user spark cluster nodes using the default JupyterHub <a href=\"https://gitlab.com/opendatahub/opendatahub-operator/-/tree/master/roles/aicoe-jupyterhub#additional-information\">Singleuser profile</a></li>\n</ul>\n<h2>Radanalytics Spark Operator</h2>\n<ul>\n<li>Support for specifying the resource requests and limits of the cpu/memory allocated to the Spark cluster master and worker nodes</li>\n</ul>\n<h2>Apache Superset</h2>\n<ul>\n<li>Resolve issues related to connecting to the Data Catalog Thrift Server</li>\n</ul>"}},{"node":{"frontmatter":{"title":"Open Data Hub 0.5.0 Release Guide","date":"17 Dec 2019","preview":"Open Data Hub 0.5.0 was just released with exciting new tools for different AI/ML platform phases. We added JupyterHub support for CUDA GPU images & notebooks, Apache Superset for data exploration & visualization and a data catalog role for deploying","permalink":"/news/2019-12-16-odh-release-0.5-blog"},"html":"<h2>What is included?</h2>\n<p>Open Data Hub 0.5.0 includes many new tools that are essential to a comprehensive AI/ML end-to-end platform. Open Data Hub is a meta-operator that can be installed on Openshift Container Platform 3.11 and 4.x.</p>\n<p>The following is a list of tools added to Open Data Hub in this release:</p>\n<table>\n<thead>\n<tr>\n<th>Technology</th>\n<th>Version</th>\n<th>Category</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>JupyterHub CUDA GPU Images and Notebooks</td>\n<td>3.0.7</td>\n<td>Support for building CUDA GPU Images and GPU Notebook</td>\n</tr>\n<tr>\n<td>Apache <a href=\"https://github.com/apache/incubator-superset\">Superset</a></td>\n<td>0.34.0</td>\n<td>Data Exploration and Visualization Tool</td>\n</tr>\n<tr>\n<td>Data Catalog (<a href=\"https://gethue.com/\">Hue</a>, <a href=\"https://spark.apache.org/docs/latest/sql-distributed-sql-engine.html\">Spark Thrift Server</a>, Hive Metastore)</td>\n<td>Hue 4.4.1 &#x26; Spark Thrift Server 2.4 &#x26; Hive Metastore 1.2.1</td>\n<td>Deployment of Hue, Spark Thrift Server and Hive Metastore to simplify querying data lakes using Spark SQL language</td>\n</tr>\n<tr>\n<td><a href=\"https://argoproj.github.io/argo/\">Argo</a></td>\n<td>2.4.2</td>\n<td>Container native workflow engine</td>\n</tr>\n</tbody>\n</table>\n<p>{:class=\"table table-bordered table-striped\"}</p>\n<p>You can review the release notes for components added in the previous v0.4.0 release <a href=\"https://opendatahub.io/news/2019-09-16/odh-release-0.4-blog.html\">here</a></p>\n<h2>AICoE-JupyterHub CUDA GPU Images and Notebooks</h2>\n<p>AICoE-JupyterHub now has support for accessing NVIDIA GPUs from Jupyter notebooks. In this release, we added </p>\n<ul>\n<li>Documentation on how to <a href=\"https://gitlab.com/opendatahub/opendatahub-operator/blob/v0.5.0/docs/enabling-gpu-aicoe-jupyterhub.adoc\">enable GPUs nodes</a> in your OpenShift cluster</li>\n<li>Support for building CUDA GPU images and notebooks as part of the component deployment process</li>\n</ul>\n<p>You can test these new features by following the Data Engineering and Machine Learning <a href=\"https://gitlab.com/opendatahub/data-engineering-and-machine-learning-workshop\">workshop</a>. The <a href=\"https://gitlab.com/opendatahub/data-engineering-and-machine-learning-workshop/blob/master/source/notebooks/tf-training-serving.ipynb\">tf-training-serving</a> contains a demonstration of how you can create Openshift Jobs to access a cluster GPU.</p>\n<h2>Apache Superset</h2>\n<p>Apache <a href=\"https://github.com/apache/incubator-superset\">Superset</a> is a data exploration and visualization tool. Instructions for deploying and creating an example database &#x26; chart are available in <a href=\"https://gitlab.com/opendatahub/opendatahub-operator/blob/v0.5.0/docs/deploying-superset.adoc\">Deploy Superset Setup</a>.</p>\n<h2>Data Catalog (Tech Preview)</h2>\n<p>The Data Catalog is a set of components with which you can run Data Exploration on your Data Lake. These components are:</p>\n<ul>\n<li><em>Hive Metastore</em> to store metadada information about the Hive tables</li>\n<li><em>Spark SQL Thrift server</em> to expose a ODBC/JDBC endpoint to interact with the Hive Tables</li>\n<li><em>Hue</em> to view S3 object store, connect to Spark SQL Thrift server and run queries, as well as create dashboards.</li>\n</ul>\n<p>For more information on the Data Catalog, please review the Data Catalog <a href=\"https://opendatahub.io/news/2019-12-15/data-catalog-in-odh.html\">announcement</a> and <a href=\"https://opendatahub.io/docs/advanced-tutorials/data-exploration.html\">tutorial</a>. The Data Catalog is currently designated as \"Tech Preview\" as we enable support for additional features available in Hue.</p>\n<h2>Argo</h2>\n<p><a href=\"https://argoproj.github.io/\">Argo</a> has been updated to version 2.4.2. <a href=\"https://argoproj.github.io/\">Argo</a> is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes.  It is useful for defining workflows using containers, running computer intensive jobs, and orchestrating DAG container pipelines natively on Kubernetes.</p>\n<p>To learn more about deploying Argo in Open Data Hub, please visit <a href=\"https://gitlab.com/opendatahub/opendatahub-operator/blob/master/docs/deploying-argo.adoc\">link</a></p>"}},{"node":{"frontmatter":{"title":"Data Catalog in Open Data Hub","date":"15 Dec 2019","preview":"Open Data Hub 0.5.0 just released with the introduction of the Data Catalog, a set of components to explore data stored in Data Lakes using Hive-related technologies.","permalink":"/news/2019-12-15-data-catalog-in-odh"},"html":"<h2>Data Catalog</h2>\n<p>In the Open Data Hub v0.5.0 release, we introduced Data Catalog. It is a set of components to\nread data stored in Data Lakes, create tables and query them in a SQL-like style. You can find\nbelow a picture of the simplified architecture of Data Catalog:</p>\n<p><img src=\"../../assets/img/posts/2019-10-10-data-catalog-in-odh/Data_Catalog_Simplified_Diagram.png\" alt=\"alt text\" title=\"Data Catalog architecture\"></p>\n<p>These are the components that are part of Data Catalog:</p>\n<ul>\n<li>Hive Metastore, responsible for maintaining the table metadata created by the user to query the data stored in Ceph/S3</li>\n<li>Spark SQL Thrift server to enable an endpoint where clients can connect using an ODBC/JDBC connection</li>\n<li>Cloudera Hue as a Data Exploration tool to explore the Data Lake, create tables and query them. You can\nalso create dashboards using the tables managed by Hive Metastore</li>\n</ul>\n<p>In the next section we'll cover each component in details.</p>\n<h2>Hive Metastore</h2>\n<p>The Hive Metastore component is part of Apache Hive and stores all metadata related to Hive tables. A Hive table\nis a special structure where you can expose your data, whether they are unstructred, semi-structured or\nstructured data, as a table. That way you can use SQL syntax to query this data directly from where the data\nis stored without the need to use ETL workflows to insert your data into a relational or NoSQL Database.</p>\n<p>One of the advantages of using Hive tables is you don't need to write a full ETL (Extract-Transform-Load) workflow\nto have your data available to read from traditional SQL clients. It is possible to partition the data according to\nyour needs too.</p>\n<p>With Hive Metastore, you can catalog your data in order to extract the best from your Data Lake.</p>\n<h2>Spark SQL Thrift Server</h2>\n<p>The main feature in Spark SQL Thrift Server is to use the power of Spark SQL and Dataframes to query the data from a\nData Lake, by creating a query plan and run in a Spark cluster to extract all information needed.</p>\n<p>In order to use the Spark SQL features, Thrift Server exposes a ODBC/JDBC endpoint so clients like SquirrelSQL, Tableau,\nSuperset or any SQL client can connect to it and issue SQL statements using raw data stored in your Data Lake.</p>\n<h2>Cloudera Hue</h2>\n<p>Cloudera Hue is a Data Exploration tool where you can explore your Data Lake for the files stored and issue SQL statements\nin a Hive instance. Data Catalog combines Hive Metastore and Spark SQL Thrift Server to have all Hive features.</p>\n<p>With Hue, you can explore the raw data from the Data Lake, create a set of hive tables and expose them to Data Scientists\nto create models based on that data.</p>\n<h2>Sample Use Cases</h2>\n<ol>\n<li>Data Exploration: You can explore your Data Lake to look at the data structure, as well as creating a table structure to\nquery them.</li>\n<li>Data Catalogging: After exploring the data and creating tables based on them, you can document the data by creating metadata\nagainst it. With that, you can share with Data Scientists the tables so they can understand what are the features they need\nto create their models.</li>\n</ol>\n<h2>Further improvements</h2>\n<p>With Cloudera Hue in the Data Catalog architecture, it is possible to create visualizations with the data too. As for now, this\nfeature is disabled as some other components are required to make it work. The Open Data Hub team will evalute the option to include\nthis Hue feature in Data Catalog. Superset can be a good replacement for Data Visualization tasks, or Jupyterhub.</p>"}},{"node":{"frontmatter":{"title":"Open Data Hub videos available on the OpenShift YouTube channel","date":"04 Nov 2019","preview":null,"permalink":"/news/2019-11-04-openshift-youtube-announcement"},"html":"<p>Open Data Hub video tutorials are now available on the <a class=\"external-link\" href=\"https://www.youtube.com/user/rhopenshift\" target=\"_blank\"><i class=\"fas fa-external-link-alt\"></i> OpenShift</a> youtube channel under the <a class=\"external-link\" href=\"https://www.youtube.com/playlist?list=PLaR6Rq6Z4Iqcg2znnClv-xbj93Q_wcY8L\" target=\"_blank\"><i class=\"fas fa-external-link-alt\"></i> AI/ML on OpenShift </a> playlist.</p>\n<ul>\n<li/><a class=\"external-link\" href=\"https://www.youtube.com/watch?v=-T6ypF7LoKk&t=2s\" target=\"_blank\"><i class=\"fas fa-external-link-alt\"></i> Installing Open Data Hub on OpenShift 4.1</a>\n<li/><a class=\"external-link\" href=\"https://youtu.be/d6X1xvDXewM\" target=\"_blank\"><i class=\"fas fa-external-link-alt\"></i> Uploading data to Ceph via command line</a>\n<li/><a class=\"external-link\" href=\"https://youtu.be/662FccIWeOE\" target=\"_blank\"><i class=\"fas fa-external-link-alt\"></i> Fraud Detection Using Open Data Hub on Openshift</a>\n</ul>"}},{"node":{"frontmatter":{"title":"OpenShift Commons Gathering at ODSC West","date":"15 Oct 2019","preview":null,"permalink":"/news/2019-11-15-openshift-commons-announcement"},"html":"<p> Want to learn more about open source AI, Open Data Hub, and other data and AI workloads on Kubernetes and OpenShift? Curious about AI DevOps? Join us October 28 at the <a class=\"external-link\" href=\"https://commons.openshift.org/gatherings/San_Francisco_2019.html\" target=\"_blank\"><i class=\"fas fa-external-link-alt\"></i> OpenShift Commons Gathering</a> co-located with ODSC/West.</p>"}},{"node":{"frontmatter":{"title":"Open Data Hub 0.4.0 Release Guide","date":"16 Sep 2019","preview":"Open Data Hub 0.4.0 just released with exciting new tools for different AI/ML platform phases. We added Argo, AI-Library and support for the Strimzi Kafka operator","permalink":"/news/2019-09-16-odh-release-0.4-blog"},"html":"<h2>What is included?</h2>\n<p>Open Data Hub 0.4.0 includes many new tools that are essential to a comprehensive AI/ML end-to-end platform. Open Data Hub is a meta-operator that can be installed on Openshift Container Platform 3.11 and 4.</p>\n<p>The following is a list of tools added to the Open Data Hub in this release:</p>\n<table>\n<thead>\n<tr>\n<th>Technology</th>\n<th>Version</th>\n<th>Category</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Open Data Hub Operator</td>\n<td>0.4.0</td>\n<td>Meta Operator Application management</td>\n</tr>\n<tr>\n<td><a href=\"https://argoproj.github.io/argo/\">Argo</a></td>\n<td>2.3.0</td>\n<td>Container native workflow engine</td>\n</tr>\n<tr>\n<td><a href=\"https://strimzi.io/\">Strimzi Kafka Operator</a></td>\n<td>0.11.1</td>\n<td>Distributed streaming platform</td>\n</tr>\n<tr>\n<td>Open Data Hub <a href=\"https://opendatahub.io\">AI-Library</a></td>\n<td>1.0</td>\n<td>Machine learning as a service</td>\n</tr>\n</tbody>\n</table>\n<p>{:class=\"table table-bordered table-striped\"}</p>\n<p>You can review the release notes for components added in the v0.3.0 release <a href=\"https://opendatahub.io/news/2019-06-27/odh-release-0.3-blog.html\">here</a></p>\n<h2>Argo</h2>\n<p><a href=\"https://argoproj.github.io/\">Argo</a> is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes.  It is useful for defining workflows using containers, running computer intensive jobs, and running CI/CD pipelines natively on Kubernetes.</p>\n<p>To learn more about deploying Argo in the Open Data Hub, please visit <a href=\"https://gitlab.com/opendatahub/opendatahub-operator/blob/master/docs/deploying-argo.adoc\">link</a></p>\n<h2>Strimzi Kafka Operator</h2>\n<p><a href=\"https://strimzi.io\">Strimzi</a> provides a way to run an Apache Kafka cluster on Kubernetes in various deployment configurations. <a href=\"https://kafka.apache.org/\">Apache Kafka</a> is a distributed streaming platform for publishing and subscribing records as well as storing and processing streams of records.</p>\n<p>Strimzi is based on Apache Kafka 2.0.1 and consists of three main components:</p>\n<ul>\n<li>Cluster Operator\nResponsible for deploying and managing Apache Kafka clusters within OpenShift or Kubernetes cluster.</li>\n<li>Topic Operator\nResponsible for managing Kafka topics within a Kafka cluster running within OpenShift or Kubernetes cluster.</li>\n<li>User Operator\nResponsible for managing Kafka users within a Kafka cluster running within OpenShift or Kubernetes cluster.</li>\n</ul>\n<p>To learn more about using the Strimzi operator to deploy an Apache Kafka cluster in the Open Data Hub, please visit <a href=\"https://gitlab.com/opendatahub/opendatahub-operator/blob/master/docs/deploying-kafka.adoc\">link</a></p>\n<h2>Open Data Hub AI-Library</h2>\n<p>AI-Library is an open source collection of AI components that allows for rapid prototyping of ideas. AI-Library enables users to work with machine learning models without worrying about infrastructure issues, model complexity or any data science expertise.</p>\n<p>To learn more about deploying AI-Library models in the Open Data Hub, please visit <a href=\"https://gitlab.com/opendatahub/opendatahub-operator/blob/master/docs/deploying-ai-library.adoc\">link</a></p>"}},{"node":{"frontmatter":{"title":"Open Source Sentiment Analysis Modeling - An advanced approach","date":"04 Sep 2019","preview":"This blog decribes the end-to-end approach at building a Sentiment Analysis service using open source components and continually improving it.","permalink":"/news/2019-08-21-sentiment-analysis-blog"},"html":"<h2>Table of Contents</h2>\n<ul>\n<li><a href=\"#introduction\">Introduction</a></li>\n<li><a href=\"#what-is-sentiment-analysis\">What is Sentiment analysis</a></li>\n<li><a href=\"#our-approaches\">Our Approaches</a></li>\n<li><a href=\"#open-source-nlp-tools-for-sentiment-analysis\">Open-source NLP tools for sentiment analysis</a></li>\n<li><a href=\"#deep-learning-for-sentiment-analysis\">Deep Learning for sentiment analysis</a></li>\n<li><a href=\"#bert-for-sentiment-analysis\">BERT for sentiment analysis</a></li>\n<li><a href=\"#infrastructure-requirements-for-building-the-service\">Infrastructure requirements for building the service</a></li>\n<li><a href=\"#continuous-improvement-and-incorporating-feedback\">Continuous improvement and incorporating feedback</a></li>\n</ul>\n<h2>Introduction</h2>\n<p>Traditionally, sentiment analysis and opinion mining are techniques used by organizations to ascertain customer sentiments about their products, brand and services. With growing availability of opinion-rich resources like customer service surveys, reviews, blogs and engagement metrics there is an ever increasing opportunity for actively using the available resources in order to understand customer sentiments and opinions. </p>\n<p>This also poses a challenge when it comes to narrowing down to a system which can generalize to a wide variety of use cases. Also much of the Cognitive and Artificial Intelligence (AI) systems need infrastructures to support their training, development and maintenance. </p>\n<p>This blog outlines our approach to improving the sentiment analysis service at Red Hat, through continuous learning and discusses how we evolved the system to learn, adapt and produce desirable outcomes for a multitude of use cases. Through this blog, we also demonstrate the approaches we took and our results on comparative analyses of the approaches.</p>\n<h2>What is Sentiment analysis</h2>\n<p>Sentiment Analysis is a technique to identify emotional states and polarity from human language. These tasks, pertaining to extracting sentiments from a piece of text, often about a certain topic fall under the field of Natural language processing (NLP). NLP is a range of computational techniques for the automatic analysis and representation of human language. It is closely linked to the fields of artificial intelligence (AI) and computational linguistics. </p>\n<p><img src=\"../../assets/img/posts/2019-08-21-sentiment-analysis-blog/sentiment_analysis_example.png\" alt=\"alt text\" title=\"An example of sentence classification of tweets into positive and negative sentiments.\"></p>\n<p>There are various approaches that can be taken to build a sentiment analysis service. One of the most intuitive approaches is that of building of a textual analysis system. Such a system comprises of analyzing a piece of text based on the terms appearing in the text and building a rule-based or fact-based system around categorizing pieces of text into various sentiment classes. The other approach, which is mainly how most of the state-of-the-art systems work is a machine learning and deep learning based approach. In this approach, rather than teaching a system to make a decision based on a set of rules, we expose the system to a variety of examples and train it to learn from the examples and then draw predictions on the basis of those. </p>\n<p>In the following sections, we go over details of the techniques mentioned above and explain how we leveraged each of the above techniques, into iteratively improving our system at Red Hat and narrowing down to a deep learning based approach in the end, and how it fits our use-case.</p>\n<h2>Our Approaches</h2>\n<p>In this section, we describe in detail, the approaches we took for sentiment analysis and the results we got at each step, and how we improved the sentiment analysis service iteratively.</p>\n<h4>Open-source NLP tools for Sentiment Analysis</h4>\n<p>Our initial approach to sentiment analysis was building a service which can detect sentiments from  customer reviews using three open-source NLP tools, Stanford CoreNLP,  Vader Sentiment Processor and TextBlob.</p>\n<p>The CoreNLP model is built using a Recurrent Neural Network trained on a tree based corpus called 'Stanford Sentiment Treebank' which is a fully labeled parse tree that allows for a complete analysis of the compositional effects of sentiment in language. On the other hand Vader is a lexicon based and rule based approach at sentiment analysis mainly targeted towards social media text. TextBlob is a classifier based approach, where a Naive Bayes classifier is used for a multiclass classification. </p>\n<p>The initial approach involved sentiment calculation using the CoreNLP Annotator with an additional validation step performed on the annotated results by passing sentences which are classified as negative by CoreNLP, through Vader and Textblob for negative sentiment validation(nsv). </p>\n<p>The results of the above approach are shown in the figure below. We are displaying F1-scores, which is calculated from Precision and Recall values for each sentiment. Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. Recall is the ratio of correctly predicted positive observations to the all observations in actual class. </p>\n<p><img src=\"../../assets/img/posts/2019-08-21-sentiment-analysis-blog/metrics-for-initial-approach.png\" alt=\"alt text\" title=\"F-1 scores of initially developed service using open source NLP tools\"></p>\n<p>F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account.  </p>\n<p>A core limitation of this method was, that it ignores the fact that we are in fact, treating the 3 models as competitive or alternate NLP models to treat a sentence, whereas, there is a significant difference between the 3 models in terms of their accuracy at sentence classification and their ability  to truly identify a certain label positive, negative, or neutral.\nIn order to improve this service, we created a service, which is an ensemble of the 3 models, Stanford CoreNLP, Vader and TextBlob and created an ensemble model by giving weighted scores to the 3 services. So, we  give scores to the three services, based on the precision values of the 3 services when they are used independently to calculate sentiment on a training dataset. </p>\n<p>The results of this approach are shown in the figure below. We are displaying the F1-scores of the ensembled model when tested on a test dataset.</p>\n<p><img src=\"../../assets/img/posts/2019-08-21-sentiment-analysis-blog/metrics-for-weighted-approach.png\" alt=\"alt text\" title=\"F-1 scores of service  using model stacking approach with weighted precision scores\"></p>\n<p>As we can see from the results in the above figure, as compared to the results of the primary approach as shown in the previous figure, there were improvements in the service by using a model stacking method with weighted average.</p>\n<h4>Deep Learning for Sentiment Analysis</h4>\n<p>As Young et al. point out in, for decades NLP problems were tackled using common machine learning approaches like SVMs, and logistic regression trained on very high dimensional and sparse features. However, recently, studies by Mikolov and Socher show, neural network based approaches are showing superior results in NLP using dense vectors and word embeddings. Deep learning is an application of artificial neural networks that allows computational models that are composed of multiple layers to learn representations of data. </p>\n<p>Andrew Ng., a pioneer in the field of machine learning points out -\n“for most flavors of the old generations of learning algorithms … performance will plateau. … deep learning … is the first class of algorithms … that is scalable. … performance just keeps getting better as you feed them more data.”</p>\n<p>Much after neural networks were introduced in 1958, in the late 1990s, the research community started to lose interest in neural networks mainly because they were regarded as only practical for “shallow” neural networks ( with one or two layers), since training a deep neural network is computationally very  expensive. However, recently, deep learning has produced state-of-the-art results in many application domains, starting from computer vision, speech recognition and most recently, natural language processing.</p>\n<p>The renaissance of neural networks as pointed out by Lei Zhang, can be attributed to factors like availability of computing power due to advances in hardware eg. GPUs, availability of huge amounts of training data and introduction of learning intermediate representations.</p>\n<p>And today, these very factors help us develop better systems and more advanced models to tackle NLP tasks. </p>\n<p>In order to understand how deep learning can be applied to customer reviews, we need to first think about the kind of data we are feeding into the system, which in our case is text, or an input string. But in order to apply mathematical operations like dot products, matrix multiplications etc, instead of a string input, we need to convert each word in the string into a vector. These vectors can be created in a way such as to represent the context, meaning and semantics. And, in order to create these vectors, which are called, word embeddings, we use word vector generation models like Word2Vec and GloVe. This gives us an embedding matrix, that contains vectors for each distinct word in the training corpus. We started with building a Recurrent Neural Network model (RNN) with Long short term memory units for sentiment analysis. A recurrent neural network is a bit different from a traditional feedforward neural network. The main difference is the temporality of an RNN and thus they are ideal for sequential data like sentences and text. LSTM ( Long short term memory ) units are modules that we can place inside of RNNs. At a high level, they make sure we are able to encapsulate long term dependencies in the text.</p>\n<h4>BERT for Sentiment Analysis</h4>\n<p>A big challenge in NLP is the shortage of training data. Most modern deep learning techniques benefit from large amounts of training data, that is, in hundreds of thousands and millions. However, since NLP is a very diversified field with many distinct tasks, there is a shortage of task specific datasets. This includes customer service review datasets, survey datasets, operational data etc.</p>\n<p>To close this gap, a technique called Bidirectional Encoder Representation from Transformers (BERT) was developed for training general purpose language representation models using enormous amount of unannotated text on the web ( known as pre-training) by researchers at Google.</p>\n<p><img src=\"../../assets/img/posts/2019-08-21-sentiment-analysis-blog/bert-architecture.png\" alt=\"alt text\" title=\"Pre - training general purpose language representation models on huge unannotated text and Fine tuning on customer review dataset\"></p>\n<p>The pre-trained model can then be fined tuned on smaller datasets for performing sentiment analysis.</p>\n<p>The advantage that this gives us is that, by using the principles of transfer learning, the universal properties that a language model possesses when exposed to a huge amount of data could be used in our case where there is a lack of annotated datasets.</p>\n<p>We use pre-trained BERT models and fine-tune it on our comparatively smaller dataset. This allows us to take advantage of feature extraction that happens in the front layers of the network without developing that feature extraction network from scratch.</p>\n<p><img src=\"../../assets/img/posts/2019-08-21-sentiment-analysis-blog/bert-results.png\" alt=\"alt text\" title=\"F-1 Scores of Positive and Negative sentiments run on BERT based model\"></p>\n<h5>Results using the BERT model</h5>\n<p>The BERT based model performs better than our RNN based approach and predicts positive sentiments with a 0.92 F1 Score and 0.73 F1 Score. This showed significant improvements as compared to all our previous approaches. Another advantage of this approach is that, it lets us get a single sentiment annotation for an entire customer review, by encapsulating very long term dependencies in text.  Moreover, the process of building the service highly improves for the BERT based deep learning model by using GPUs for training the model. We discuss this in more detail in the next section.</p>\n<h2>Infrastructure requirements for building the service</h2>\n<p><img src=\"../../assets/img/posts/2019-08-21-sentiment-analysis-blog/cpu-vs-gpu.png\" alt=\"alt text\" title=\"CPU vs GPU and comparison of training time\"></p>\n<p>So, as we fine-tune a sentiment analysis model, with pre-trained BERT parameters by training it on a large annotated dataset, we introduce large computational operations in terms of memory. To compute the data efficiently, we need infrastructures that can handle the computation processes in minimum time. </p>\n<p>We need GPUs to solve these problems since neural networks heavily rely on floating point matrix multiplication. Also, deep learning algorithms require a lot of data to train, thus they need large memory and high speed networks to complete in a reasonable amount of time.</p>\n<p>Although, less expensive as compared to the pre-training procedure, the fine-tuning step when run on a CPU with 4 cores and 8 Gigabytes of RAM takes about 3 hours to train. The same model when trained using an NVIDIA P100 GPU with 3584 cores and 16 Gigabytes of Memory, trains the model in only about 6 minutes.</p>\n<p>This speeds up the overall workflow of training, testing , hyperparameter tuning and thus building of the service.</p>\n<p>Also, for training the models and running predictions we use the Jupyterhub installation of the Open Data Hub which have GPU support. Jupyter notebooks allow running code, documenting, visualization in the same environment which makes the process of training and prototyping more flexible.</p>\n<h6>Links to sample code and model</h6>\n<p>The code used for training and testing and the saved model can be found in this <a href=\"https://gitlab.com/opendatahub/sample-models/tree/master/sentiment-analysis\">repository</a>.</p>\n<h2>Continuous improvement and incorporating feedback</h2>\n<p><img src=\"../../assets/img/posts/2019-08-21-sentiment-analysis-blog/feedback.png\" alt=\"alt text\" title=\"Feedback system for the sentiment analysis service\"></p>\n<p>The lifecycle of developing AI systems, does not end with building the first iteration of the system. We improve the system and continuously evolve it by making it learn from a feedback system. </p>\n<p>In our Sentiment Analysis project at Red Hat, after deployment of the service, we introduce additional steps of monitoring how our service is performing, capturing feedback from users, and including it back in building our ground truth, our model training and evaluation process.</p>\n<p>For this purpose, we built a feedback system which lets users correctly annotate the captured results from running the service on data for any false predictions that might be observed. </p>\n<p>As we see in the figure above, the results of the feedback annotation are introduced back in the training of the model in the second supervised learning phase, therefore, making our training datasets larger and more robust and enabling us to generate more context specific data which can be introduced in the training phase of the model, thus continually improving the service.</p>"}},{"node":{"frontmatter":{"title":"Apache Kafka in Open Data Hub","date":"20 Aug 2019","preview":"The Open Data Hub community is happy to announce the addition of Apache Kafka to the Open Data Hub via Strimzi Operator. This makes deploying a message bus as a part of OpenDataHub a breeze.","permalink":"/news/2019-08-20-kafka-in-odh"},"html":"<h2>Apache Kafka</h2>\n<p><img src=\"../../assets/img/posts/2019-08-20-kafka-in-odh/apache_kafka.png\" alt=\"alt text\"></p>\n<p>The Open Data Hub is a reference architecture running on OpenShift that incorporates a variety of open source projects to function as an ML-as-a-service platform. Given the huge amounts of data to be ingested and processed, it's crucial to have a reliable streaming platform. To solve this problem we use Apache Kafka.</p>\n<p>Apache Kafka is an open-source stream-processing software platform. It is used for building real-time data pipelines and streaming apps. It is horizontally scalable, fault-tolerant, wicked fast, and runs in production in thousands of companies.</p>\n<p>Apache Kafka is a distributed streaming platform. What exactly does that mean?\nA streaming platform has three key capabilities:</p>\n<ol>\n<li>Publish and subscribe to streams of records, similar to a message queue or enterprise messaging system.</li>\n<li>Store streams of records in a fault-tolerant durable way.</li>\n<li>Process streams of records as they occur.</li>\n</ol>\n<p>Kafka is generally used for two broad classes of applications:</p>\n<p>Building real-time streaming data pipelines that reliably get data between systems or applications\nBuilding real-time streaming applications that transform or react to the streams of data</p>\n<h2>Strimzi Operator</h2>\n<p><img src=\"../../assets/img/posts/2019-08-20-kafka-in-odh/strimzi.png\" alt=\"alt text\" title=\"Strimzi\"></p>\n<p><a href=\"https://strimzi.io/\">Strimzi</a> is based on Apache Kafka, a popular platform for streaming data delivery and processing. Strimzi makes it easy to run Apache Kafka on Kubernetes.</p>\n<p>Strimzi provides three operators:</p>\n<ol>\n<li>Cluster Operator\nResponsible for deploying and managing Apache Kafka clusters running on a Kubernetes cluster.</li>\n<li>Topic Operator\nResponsible for managing Kafka topics within a Kafka cluster running on a Kubernetes cluster.</li>\n<li>User Operator\nResponsible for managing Kafka users within a Kafka cluster running on a Kubernetes cluster.</li>\n</ol>\n<h2>Monitoring</h2>\n<p>Kafka deployed usind ODH Operator comes pre-configured to expose metrics out of the box which are scraped using Prometheus and Visualized using Grafana. This gives us a holistic view of the Kafka cluster's health and performance.</p>\n<p><img src=\"../../assets/img/posts/2019-08-20-kafka-in-odh/kafka_metrics.png\" alt=\"alt text\" title=\"Kafka Monitoring\"></p>\n<h2>Sample Use Cases</h2>\n<ol>\n<li>Data Ingestion: Internally we have a Kafka deployment for ingesting data and back it up to Elasticsearch and Ceph S3 for analysis using Logstash and s3 Connector. We have 3 Kafka and 3 zookeeper Brokers of 10 Tb each backed by Persistent Volumes with a peak throughput of around 20k messages per second and a sustained rate of ~12k messages per second.</li>\n<li>Data Streaming: For credit monitoring we have a kafka deployment which ingests credit data as a producer in near-real time and are consumed and sent to the seldon model-serving layer for risk monitoring and fraud detection.</li>\n</ol>"}},{"node":{"frontmatter":{"title":"Monitoring for the Data Hub","date":"02 Jul 2019","preview":"The Open Data Hub community is happy to announce the addition of monitoring to the Open Data Hub via Prometheus and Grafana. This will make it much easier for IT departments and devops engineers to manage their deployments of the Open Data Hub.","permalink":"/news/2019-07-02-open-data-hub-monitoring"},"html":"<h2>Monitoring</h2>\n<p>The Open Data Hub is a reference architecture running on OpenShift that incorporates a variety of open source projects to function as a ML-as-a-service platform. Given how many different pieces there are in the platform, it is very difficult for devops engineers to get a clear picture of the health of the system. Moreover, it is not possible to make an informed decision regarding hardware requests or SLA’s without understanding how your system is performing. We are addressing this problem by using Prometheus and Grafana as the monitoring solution for the Open Data Hub.</p>\n<p><img src=\"../../assets/img/posts/2019-07-02-open-data-hub-monitoring/cluster_metrics.png\" alt=\"alt text\" title=\"Cluster Monitoring Dashboard\"></p>\n<h2>Prometheus</h2>\n<p><img src=\"../../assets/img/posts/2019-07-02-open-data-hub-monitoring/prometheus_logo.png\" alt=\"alt text\" title=\"Prometheus Logo\"></p>\n<p><a href=\"https://prometheus.io/\">Prometheus</a> is an open-source monitoring and alerting tool that was developed at SoundCloud to address the challenges posed by the micro-service oriented architecture they had developed. We were drawn to Prometheus as it offers scalability, easy configurability for targets and alerting, and is simple to run in a variety of different environments. This was especially important to us as any monitoring solution we use should be able to scale out to a large number of instances. In addition to scalably gathering metrics, Prometheus has a powerful query language (imaginatively called PromQL) that makes it easy for the layman to gain insight into the metrics via slicing and various transformations. </p>\n<h2>Grafana</h2>\n<p><img src=\"../../assets/img/posts/2019-07-02-open-data-hub-monitoring/grafana_logo.png\" alt=\"alt text\" title=\"Grafana Logo\"></p>\n<p>The front end we are using to visualize these metrics is <a href=\"https://grafana.com/\">Grafana</a>. Grafana has robust integrations in place with data sources such as Prometheus, Elasticsearch, InfluxDB, and Graphite. It also has support for a variety of different types of visualizations both out of the box and through the <a href=\"https://grafana.com/docs/administration/cli/\">grafana-cli</a>. The Grafana community has a large number of dashboards available <a href=\"https://grafana.com/dashboards\">here</a> which can easily be imported into any running instance of Grafana.</p>\n<h2>Additional Configuration</h2>\n<p>The monitoring deployment is configured out of the box to scrape ODH spark metrics as well as all pod level metrics that are being exposed on default ports. These metrics can be used to create Grafana dashboards for a quick picture of the health of your system. If your system requires any additional configuration for the alerting or custom endpoints to scrape metrics from, all you need to do is modify the prometheus.yml config map in  prometheus-objects.yaml to add a custom job. A custom job could look something like the following.</p>\n<pre><code>- job_name: 'My Custom Kafka Metrics'\n         metrics_path: '/metrics'\n         scheme: http\n         static_configs:\n           - targets:\n             - http://my-custom-kafka-endpoint.com\n</code></pre>\n<p><img src=\"../../assets/img/posts/2019-07-02-open-data-hub-monitoring/kafka.png\" alt=\"alt text\" title=\"Kafka Dashboard\"></p>"}},{"node":{"frontmatter":{"title":"Open Data Hub 0.3 Release Guide","date":"27 Jun 2019","preview":"Open Data Hub 0.3 just released with exciting new tools for different AI/ML platform phases. We added monitoring, model serving and GPU processing tools.","permalink":"/news/2019-06-27-odh-release-0.3-blog"},"html":"<h2>What is included?</h2>\n<p>Open Data Hub 0.3 includes many new tools that are essential to a comprehensive AI/ML end-to-end platform. Open Data Hub is a meta-operator that can be installed on OpenShift Container Platform 3.11 and 4. The following is a list of tools included in this release:</p>\n<table>\n<thead>\n<tr>\n<th>Technology</th>\n<th>Version</th>\n<th>Category</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Open Data Hub Operator</td>\n<td>0.3</td>\n<td>Meta Operator Application management</td>\n</tr>\n<tr>\n<td><a href=\"https://seldon.io\">Seldon</a></td>\n<td>0.2.7</td>\n<td>Model Serving and Metrics Tool</td>\n</tr>\n<tr>\n<td><a href=\"https://jupyter.org/hub\">JupyterHub</a> with GPU Support</td>\n<td>3.0.7</td>\n<td>Data science tools</td>\n</tr>\n<tr>\n<td><a href=\"http://spark.apache.org/\">Apache Spark</a></td>\n<td>2.2.3</td>\n<td>Query &#x26; ETL frameworks</td>\n</tr>\n<tr>\n<td>TwoSigma <a href=\"http://beakerx.com/\">BeakerX</a> Integration</td>\n<td>1.4.0</td>\n<td>Data science tools</td>\n</tr>\n<tr>\n<td><a href=\"https://prometheus.io/\">Prometheus</a></td>\n<td>2.3</td>\n<td>System monitoring tools</td>\n</tr>\n<tr>\n<td><a href=\"https://grafana.com/\">Grafana</a></td>\n<td>4.7</td>\n<td>System monitoring tools</td>\n</tr>\n</tbody>\n</table>\n<p>{:class=\"table table-bordered table-striped\"}</p>\n<h2>Seldon</h2>\n<p>Seldon is an open source framework that makes it easier to deploy AI/ML models on Kubernetes and OpenShift. The model can be created and trained using many tools such as Apache Spark, scikit-learn and TensorFlow. Seldon also provides metric for Prometheus scraping. Metrics can be custom model metrics or Seldon core system metrics.\nTo learn more about how to use Seldon as part of Open Data Hub 0.3 please visit <a href=\"https://gitlab.com/opendatahub/opendatahub-operator/blob/master/docs/deploying-seldon.adoc\">link</a></p>\n<h2>JupyterHub with GPU</h2>\n<p>Open Data Hub release 0.3 will include a deployment of Jupyterhub with GPU support giving  users and Data Scientists  easier access to GPU processing.  Specific documentation will be provided shortly.</p>\n<h2>Apache Spark</h2>\n<p>Apache Spark was previously included in Open Data Hub release 0.2. In release 0.3 we include Prometheus and Grafana monitoring support for Spark metrics. For example notebooks using Apache Spark please refer to the included sample notebooks in the <a href=\"https://gitlab.com/opendatahub/opendatahub-operator/blob/master/docs/deploying-seldon.adoc\">tutorial section</a></p>\n<h2>BeakerX</h2>\n<p>BeakerX is an extension to Jupyter Notebook that includes tools for plotting, creating tables and forms and many more.<br>\nTo learn more about how to use BeakerX as part of Open Data Hub 0.3 please visit <a href=\"https://gitlab.com/opendatahub/opendatahub-operator/blob/master/docs/deploying-seldon.adoc\">link</a></p>\n<h2>Prometheus and Grafana</h2>\n<p>Prometheus and Grafana are widely used open source tools for monitoring clusters and applications on Kubernetes and OpenShift.\nPrometheus provides monitoring and alerting tools, it scrapes metrics from all components that expose a REST interface supporting prometheus metrics. Grafana is a visualization tool that is the leader in time series analytics. Users can create Grafana dashboards that display metrics gathered by Prometheus into plots and graphs for analysis. Out of the box, Promotheus will scrap Apache Spark metrics withing Open Data Hub namespace.\nTo learn more about how to use Prometheus and Grafana as part of Open Data Hub 0.3 please visit\n<a href=\"https://gitlab.com/opendatahub/opendatahub-operator/blob/master/docs/deploying-monitoring.adoc\">link</a></p>"}},{"node":{"frontmatter":{"title":"Project Road Map for 2019","date":"29 Apr 2019","preview":"The Open Data Hub community has some exciting new features planned for 2019.  Driven by the release of OpenShift 4, we have officially moved to operators.  This will make deployment and management of the services easier for administrators.","permalink":"/news/2019-04-29-project-road-map-for-2019"},"html":"<h2>What's Next?</h2>\n<p><img src=\"../../assets/img/posts/2019-04-29-project-road-map-for-2019/roadmap_2019.png\" alt=\"alt text\" title=\"Open Data Hub Road Map - 2019\"></p>\n<p>The Open Data Hub community has some exciting new features planned for 2019.  Driven by the release of OpenShift 4, we have officially moved to operators.  This will make deployment and management of the services easier for administrators.  Other key highlights on the road map for this year include added support for monitoring services with Prometheus and Grafana, as well as improved machine learning model lifecycle management with tools such as MLflow and Seldon-core.</p>\n<h2>February 2019</h2>\n<h3>Version 0.1 - Initial ODH Release</h3>\n<table>\n<thead>\n<tr>\n<th>Technology</th>\n<th>Version</th>\n<th>Container Platform Version</th>\n<th>Category</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://jupyter.org/hub\">JupyterHub</a></td>\n<td>0.9.4</td>\n<td>OpenShift 3.1x, Minishift 1.3x</td>\n<td>Data science tools</td>\n</tr>\n<tr>\n<td><a href=\"http://spark.apache.org/\">Apache Spark</a></td>\n<td>2.2.3</td>\n<td>OpenShift 3.1x, Minishift 1.3x</td>\n<td>Query &#x26; ETL frameworks</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/ceph/cn\">Ceph-nano</a></td>\n<td>Latest master</td>\n<td>OpenShift 3.1x, Minishift 1.3x</td>\n<td>Storage</td>\n</tr>\n</tbody>\n</table>\n<p>{:class=\"table table-bordered table-striped\"}</p>\n<p>The <a href=\"https://gitlab.com/opendatahub/jupyterhub-ansible\">initial launch of ODH</a> includes an object store, an analytics engine for big data and a platform for spinning up data science notebooks.  This provides everything you need to get started with machine learning on OpenShift.</p>\n<h2>May 2019</h2>\n<h3>OCP 4 and Operator Support with Monitoring</h3>\n<table>\n<thead>\n<tr>\n<th>Technology</th>\n<th>Version</th>\n<th>Container Platform Version</th>\n<th>Category</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Open Data Hub Operator</td>\n<td>0.2</td>\n<td>OpenShift 3.1x and 4.x</td>\n<td>Application management</td>\n</tr>\n<tr>\n<td><a href=\"https://www.seldon.io/open-source/\">Seldon-core</a></td>\n<td>0.2.6+</td>\n<td>OpenShift 3.1x and 4.x</td>\n<td>Model Serving</td>\n</tr>\n<tr>\n<td><a href=\"https://jupyter.org/hub\">JupyterHub</a> with GPU Support</td>\n<td>0.9.4</td>\n<td>OpenShift 3.1x</td>\n<td>Data science tools</td>\n</tr>\n<tr>\n<td><a href=\"http://spark.apache.org/\">Apache Spark</a></td>\n<td>2.2.3</td>\n<td>OpenShift 3.1x and 4.x</td>\n<td>Query &#x26; ETL frameworks</td>\n</tr>\n<tr>\n<td>TwoSigma <a href=\"http://beakerx.com/\">BeakerX</a> Integration</td>\n<td>1.3.0</td>\n<td>OpenShift 3.1x and 4.x</td>\n<td>Data science tools</td>\n</tr>\n<tr>\n<td><a href=\"https://prometheus.io/\">Prometheus</a></td>\n<td>2.3</td>\n<td>OpenShift 3.1x and 4.x</td>\n<td>System monitoring tools</td>\n</tr>\n<tr>\n<td><a href=\"https://grafana.com/\">Grafana</a></td>\n<td>4.7</td>\n<td>OpenShift 3.1x and 4.x</td>\n<td>System monitoring tools</td>\n</tr>\n</tbody>\n</table>\n<p>{:class=\"table table-bordered table-striped\"}</p>\n<p>The May release of ODH brings a re-design of the deployment to take advantage of <a href=\"https://blog.openshift.com/introducing-the-operator-framework/\">Kubernetes operators</a>!   Seldon-core will provide model-serving and model-monitoring capabilities. In order to better understand system usage and workloads, Prometheus and Grafana are also being targeted with pre-configured metrics and dashboards to monitor ODH.</p>\n<p>Also, JupyterHub will be improved to allow GPUs installed in OpenShift 3.1x to be usable within Jupyter notebooks.  When a user selects a notebook and specifies GPU workloads, the tasks will automatically run on a node that is GPU-enabled and available.</p>\n<h2>August 2019</h2>\n<h3>Data Engineering + Model Lifecycle</h3>\n<table>\n<thead>\n<tr>\n<th>Technology</th>\n<th>Version</th>\n<th>Container Platform Version</th>\n<th>Category</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Open Data Hub Operator</td>\n<td>0.3</td>\n<td>OpenShift 3.1x and 4.x</td>\n<td>Application management</td>\n</tr>\n<tr>\n<td><a href=\"https://argoproj.github.io/argo/\">Argo</a></td>\n<td>2.2.1+</td>\n<td>OpenShift 3.1x and 4.x</td>\n<td>Orchestration platforms</td>\n</tr>\n<tr>\n<td><a href=\"https://mlflow.org/\">MLflow</a></td>\n<td>0.9.1+</td>\n<td>OpenShift 3.1x and 4.x</td>\n<td>Model experimentation</td>\n</tr>\n<tr>\n<td><a href=\"https://strimzi.io/\">Kafka (Strimzi / AMQ Streams)</a></td>\n<td>0.11.2+</td>\n<td>OpenShift 3.1x and 4.x</td>\n<td>Streaming and enrichment</td>\n</tr>\n<tr>\n<td><a href=\"https://spark.apache.org/docs/latest/sql-distributed-sql-engine.html#running-the-thrift-jdbcodbc-server\">Spark SQL Thrift Server</a></td>\n<td>2.2.3+</td>\n<td>OpenShift 3.1x and 4.x</td>\n<td>Query &#x26; ETL frameworks</td>\n</tr>\n<tr>\n<td>Hive Metastore</td>\n<td>1.2.1+</td>\n<td>OpenShift 3.1x and 4.x</td>\n<td>Metadata cataloging</td>\n</tr>\n<tr>\n<td><a href=\"http://gethue.com/\">Cloudera Hue</a></td>\n<td>4.4+</td>\n<td>OpenShift 3.1x and 4.x</td>\n<td>Data exploration</td>\n</tr>\n<tr>\n<td><a href=\"https://rook.io/docs/rook/v0.8/ceph-quickstart.html\">Ceph Rook</a> Operator Integration</td>\n<td>0.9.3</td>\n<td>OpenShift 3.1x and 4.x</td>\n<td>Storage</td>\n</tr>\n<tr>\n<td>Open Data Hub AI Library</td>\n<td>0.1</td>\n<td>OpenShift 3.1x and 4.x</td>\n<td>Data science tools</td>\n</tr>\n</tbody>\n</table>\n<p>{:class=\"table table-bordered table-striped\"}</p>\n<p>Version 0.3 of ODH includes added support for data engineers with Cloudera Hue, Argo, Kafka, and Spark SQL Thrift Server.  Argo is great for managing pipelines and workflows.  Ceph-nano is being replaced by Ceph Rook, the best way to deploy and manage Ceph Storage on OpenShift. For metadata and cataloging of data stored in the Ceph data lake, Hive Metastore will be added.  Spark SQL Thrift Server can be configured to enable SQL access to data stored in the Ceph data lake by leveraging Spark as the processing engine.  Hue will provide an interface for data analysts to query the data lake using metadata in Hive Metastore and the SQL capabilities of Spark SQL Thrift Server.</p>\n<p>For machine learning model lifecycles, MLflow is being added to allow model experimentation.</p>\n<h2>November 2019</h2>\n<p>We haven't thought that far out yet, but stay tuned!</p>"}},{"node":{"frontmatter":{"title":"Open Data Hub Overview","date":"04 Dec 2018","preview":"Built on OpenShift, Open Data Hub uses the leading Kubernetes platform to deliver a meta-project that integrates Open Source components into a practical service-oriented AI and ML solution.  Organizations and IT departments can deploy Open Data Hub as a centralized self-service solution for analytic and data science workloads.","permalink":"/news/2018-12-04-open-data-hub-overview"},"html":"<h2>What is the Open Data Hub?</h2>\n<p><img src=\"/assets/img/posts/2018-12-04-open-data-hub-overview/Open_Data_Hub_Deployment.png\" alt=\"alt text\" title=\"Open Data Hub\"></p>\n<p>Built on OpenShift, Open Data Hub uses the leading Kubernetes platform to deliver a meta-project that integrates Open Source components into a practical service-oriented AI and ML solution.  Organizations and IT departments can deploy Open Data Hub as a centralized self-service solution for analytic and data science workloads.</p>\n<h2>AI Library</h2>\n<p>Open Data Hub deploys with a set of pre-defined AI models available for use.  Try the services out and get some value out of your data immediately without having to accumulate massive amounts of training data.  Current models available include sentiment analysis, test flake analysis and more.</p>\n<h2>Data Science and ETL Tools</h2>\n<p><img src=\"../../assets/img/posts/2018-12-04-open-data-hub-overview/ds_tools.png\" alt=\"alt text\" title=\"Open Data Hub - Data Science\"></p>\n<p>Data scientists can use familiar tools such as Jupyter notebooks for developing complex algorithms and models.  Frameworks such as numpy, scikit-learn, Tensorflow and more are available for use.</p>\n<p>For data engineers who need to query or process their data, Spark from radanalytics.io is available for use.  If JDBC/ODBC access is required for data stored in S3, Spark Thrift Server comes pre-configured and ready to go.</p>\n<h2>Streaming and Enriching Data</h2>\n<p><img src=\"../../assets/img/posts/2018-12-04-open-data-hub-overview/streaming_tools.png\" alt=\"alt text\" title=\"Open Data Hub - Streaming and Enrichment\"></p>\n<p>Open Data Hub offers numerous services for ingesting data, including Kafka, Logstash, Fluentd and rsyslog.  At the heart of it all is the strimzi.io Kafka streaming platform, which allows data to be ingested and processed at scale.  Kafka applications and logstash micro-services can be used to enrich your inbound data before it lands in storage.</p>\n<h2>Storing Data</h2>\n<p><img src=\"../../assets/img/posts/2018-12-04-open-data-hub-overview/storage_tools.png\" alt=\"alt text\" title=\"Open Data Hub - Storage\"></p>\n<p>Data is the foundational piece of AI, and Red Hat Ceph Object Store is ideal for building your lake of unstructured data.  Ceph’s S3 API makes integration with popular big data tools simple and easy.</p>\n<p>If log analysis is more what you need, Open Data Hub provides a deployment of Elasticsearch with Kibana for visualizations.</p>\n<h2>Managing Data</h2>\n<p><img src=\"../../assets/img/posts/2018-12-04-open-data-hub-overview/dm_tools.png\" alt=\"alt text\" title=\"Open Data Hub - Manage Data\"></p>\n<p>Design your data pipelines the Kubernetes way using Argo.  Jobs can be created to train models or transform data.</p>\n<h2>Monitoring Infrastructure</h2>\n<p><img src=\"../../assets/img/posts/2018-12-04-open-data-hub-overview/monitor_tools.png\" alt=\"alt text\" title=\"Open Data Hub - Monitor\"></p>\n<p>Open Data Hub comes standard with Prometheus and Grafana for monitoring the components.  DevOps teams have predefined dashboards to help manage the infrastructure.</p>\n<h2>Community</h2>\n<p>We are an open community that fosters collaboration between organizations, vendors, developers, and academics.  <a href=\"https://gitlab.com/opendatahub\">Join us and contribute</a>!</p>\n<h2>Tools</h2>\n<p><b>Available in Open Data Hub</b></p>\n<ul>\n<li>AI Library</li>\n<li><a href=\"https://ceph.com/ceph-storage/object-storage/\">Ceph Object Storage</a></li>\n<li><a href=\"https://spark.apache.org/\">Spark</a></li>\n<li><a href=\"http://jupyter.org/hub\">JuypyterHub</a></li>\n<li><a href=\"https://jupyter.org/\">Jupyter Notebooks</a></li>\n<li><a href=\"https://www.tensorflow.org/\">Tensorflow</a></li>\n</ul>\n<p><b>Coming Soon to Open Data Hub</b></p>\n<ul>\n<li>Kafka (<a href=\"https://www.redhat.com/en/about/videos/summit-2018-introducing-amq-streams-data-streaming-apache-kafka\">AMQ Streams</a> and <a href=\"http://strimzi.io/\">Strimzi</a>)</li>\n<li><a href=\"https://docs.confluent.io/current/connect/index.html\">Kafka Connect</a></li>\n<li><a href=\"https://spark.apache.org/docs/latest/sql-distributed-sql-engine.html\">Spark Thrift Server</a></li>\n<li><a href=\"https://www.elastic.co/products/elasticsearch\">Elasticsearch</a></li>\n<li><a href=\"https://www.elastic.co/products/logstash\">Logstash</a></li>\n<li><a href=\"https://www.fluentd.org/\">Fluentd</a></li>\n<li><a href=\"https://www.rsyslog.com/\">Rsyslog</a></li>\n<li><a href=\"https://www.elastic.co/products/kibana\">Kibana</a></li>\n<li><a href=\"https://prometheus.io/\">Prometheus</a></li>\n<li><a href=\"https://grafana.com/\">Grafana</a></li>\n<li><a href=\"https://argoproj.github.io/\">Argo</a></li>\n</ul>"}},{"node":{"frontmatter":{"title":"Open Data Hub at DevConf.US 2018","date":"28 Nov 2018","preview":"The Open Data Hub team had several announcements and talks at DevConf.US 2018!  Watch the recorded sessions to learn more about the project.","permalink":"/news/open-data-hub-at-devconf-us"},"html":"<p>The Open Data Hub team had several announcements and talks at DevConf.US 2018!  Watch the recorded sessions to learn more about the project.</p>\n<p><a href=\"https://youtu.be/O16ST02CO0E?t=4308\">Open Data Hub in Mass Open Cloud</a> - Get an overview the Open Data Hub and how academia can use it in the Mass Open Cloud for artificial intelligence and machine learning research.</p>\n<p><a href=\"https://youtu.be/b1QgLBOqLKM?t=14m34s\">Enabling data exploration with JupyterHub on OpenShift</a> - See JupyterHub in action in the Open Data Hub.</p>\n<p><a href=\"https://youtu.be/b1QgLBOqLKM?t=9288\">Building AI with Ceph + OpenShift</a> - Watch a demonstration of loading data, creating a model and deploying the model to a serverless framework in the Open Data Hub.</p>"}}]}},"pageContext":{}},"staticQueryHashes":["1804438722"]}