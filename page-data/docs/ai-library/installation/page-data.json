{"componentChunkName":"component---src-templates-create-pages-js","path":"/docs/ai-library/installation","result":{"data":{"markdownRemark":{"html":"<h3>Pre-requisites</h3>\n<p>As a component of Open Data Hub, the AI Library requires the same pre-requisites as Open Data Hub.</p>\n<p>Installing ODH requires OpenShift 3.11 or 4.x. Documentation for OpenShift can be located (<a href=\"https://docs.openshift.com/container-platform\">here</a>).  All screenshots and instructions are from OpenShift 4.2.  For the purposes of this quick start, we used <a href=\"https://try.openshift.com/\">try.openshift.com</a> on AWS.  Tutorials have also been tested on <a href=\"https://code-ready.github.io/crc/\">Code Ready Containers</a> with 16GB of RAM.</p>\n<p>Installation of Open Data Hub components Seldon and Argo will require the installation of their respective CRDs as outlined in the Advanced Installation instructions for <a href=\"../administration/advanced-installation/optional.html\">Optional Components</a>.</p>\n<h4>External Components:</h4>\n<p>AI Library uses S3 Storage and has currently been tested with Ceph.</p>\n<ul>\n<li>Ceph Storage</li>\n</ul>\n<h4>Open Data Hub Components</h4>\n<p>In addition, several ODH components are required and can be installed simultaneously or beforehand.  </p>\n<ul>\n<li>Seldon</li>\n<li>Argo</li>\n<li>JupyterHub (recommended but not required)</li>\n</ul>\n<h3>Enabling the AI Library</h3>\n<p>Installation of the AI Library can be done during the initial installation of ODH or enabled afterwards.</p>\n<ol>\n<li>\n<p>Navigate to the ODH deployment.</p>\n<ul>\n<li>Navigate to <code>Installed Operators</code> </li>\n<li>Select <code>Open Data Hub Operator</code></li>\n<li>Click the <code>Open Data Hub</code> Tab</li>\n<li>Under <code>Open Data Hubs</code> select your deployment.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/ai-library/installation/1-odh-list.png\" alt=\"ODH List\" title=\"ODH List\"></li>\n</ul>\n</li>\n<li>Edit the ODH deployment's setting YAML.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/ai-library/installation/2-odh-yaml.png\" alt=\"ODH YAML\" title=\"ODH YAML\"></li>\n<li>\n<p>Set the pre-requisites and the ai-library settings to <code>odh_deploy: true</code> and set the s3 credentials appropriately.  You can  leave the other defaults or customized as you wish.  The required settings should look somewhat like the following:</p>\n<pre><code class=\"language-yaml\">aicoe-jupyterhub:\nodh_deploy: true\nseldon:\nodh_deploy: true\nargo:\nodh_deploy: true\nai-library:\nodh_deploy: true\ns3_endpoint: 'https://ceph.storage'\ns3_access: 'access-key'\ns3_secret: 'secret-key'\ns3_bucket: 'my-bucket'\ns3_region: 'blank-for-ceph'\n</code></pre>\n</li>\n<li>\n<p>Verify the installation.  </p>\n<ol>\n<li>Navigate to your project's status</li>\n<li>You should see several deployments including Seldon and AI Library services\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/ai-library/installation/2-odh-yaml.png\" alt=\"ODH YAML\" title=\"3-verify\"></li>\n<li>\n<p>Using curl, browser, or other client test a route to one of the AI Library services.  For example, on CRC to test linear regression:</p>\n<pre><code class=\"language-bash\">curl -k https://linear-regression-odh.apps-crc.testing/\n</code></pre>\n<p>A response of <code>Hello World!!</code> indicates the service is alive and ready.</p>\n</li>\n</ol>\n</li>\n</ol>\n<h3>Installing Sample Models and Data</h3>\n<p>The AI Library deploys the appropriate endpoints, but it does not deploy sample models and data by default.  Sample data and models are kept in a <a href=\"https://gitlab.com/opendatahub/sample-models.git\">sample-models GitLab repository</a>.  In order to try some of these models, they must be copied to the Ceph storage location accessible to AI Library.  Use your favorite method to do so.  Below details how to do so using the <code>s3cmd</code> tool.</p>\n<ol>\n<li>\n<p>Install <code>s3cmd</code> cli</p>\n<pre><code class=\"language-bash\">pip3 install s3cmd\n</code></pre>\n</li>\n<li>\n<p>Configure the credentials either as environment variables or in s3cmd config file</p>\n<pre><code class=\"language-bash\">export ACCESS_KEY=\nexport SECRET_KEY=\nexport HOST=\n</code></pre>\n<p>(or)</p>\n<pre><code class=\"language-bash\">s3cmd --configure\n</code></pre>\n</li>\n<li>\n<p>Check connectivity by using the following command to list existing buckets</p>\n<pre><code class=\"language-bash\">s3cmd ls --host=$HOST --access_key=$ACCESS_KEY --secret_key=$SECRET_KEY\n</code></pre>\n</li>\n<li>\n<p>Create a new bucket to copy data in to,</p>\n<pre><code class=\"language-bash\">s3cmd mb s3://AI-LIBRARY --host=$HOST --access_key=$ACCESS_KEY --secret_key=$SECRET_KEY\n</code></pre>\n</li>\n<li>\n<p>Clone the sample data set locally</p>\n<pre><code class=\"language-bash\">git clone â€‹ https://gitlab.com/opendatahub/sample-models.git\ncd sample-models\n</code></pre>\n</li>\n<li>\n<p>Sync the required directory/files to your s3 backend.</p>\n<pre><code class=\"language-bash\">s3cmd sync &#x3C;MODEL-DIRECTORY> s3://AI-LIBRARY/ --host=$HOST --access_key=$ACCESS_KEY --secret_key=$SECRET_KEY\n</code></pre>\n</li>\n<li>\n<p>Once copied, list the contents of the bucket AI-LIBRARY to check files have been copied</p>\n<pre><code class=\"language-bash\">s3cmd ls s3://AI-LIBRARY --host=$HOST --access_key=$ACCESS_KEY --secret_key=$SECRET_KEY --recursive\n</code></pre>\n</li>\n</ol>\n<p>{% include next-link.html label=\"Services\" url=\"/docs/ai-library/services.html\" %}</p>"}},"pageContext":{"permalink":"/docs/ai-library/installation"}},"staticQueryHashes":["1804438722"]}