{"componentChunkName":"component---src-templates-create-pages-js","path":"/docs/getting-started/legacy/basic-tutorial","result":{"data":{"markdownRemark":{"html":"<h3>Note</h3>\n<p>This tutorial is based on old - no longer developed - version of Open Data Hub. You can find the new version of the document <a href=\"%7B%7Bsite.baseurl%7D%7D/docs/getting-started/basic-tutorial\">here</a>.</p>\n<hr>\n<h3>Pre-requisites</h3>\n<p>This Tutorial requires a basic installation of Open Data Hub with Spark and JupyterHub as detailed in the <a href=\"%7B%7Bsite.baseurl%7D%7D/docs/getting-started/quick-installation\">quick installation</a>. The quick installation steps are also available as a <a class=\"external-link\" href=\"https://www.youtube.com/watch?v=-T6ypF7LoKk&t=2s\" target=\"_blank\"><i class=\"fas fa-external-link-alt\"></i>tutorial video</a> on the OpenShift youtube channel.</p>\n<p>All screenshots and instructions are from OpenShift 4.2.  For the purposes of this tutorial, we used <a href=\"https://try.openshift.com/\">try.openshift.com</a> on AWS.  Tutorials have also been tested on <a href=\"https://code-ready.github.io/crc/\">Code Ready Containers</a> with 16GB of RAM.</p>\n<p>The <a href=\"%7B%7Bsite.repo%7D%7D/opendatahub.io/blob/master/assets/files/tutorials/basic/basic_tutorial_notebook.ipynb\">source</a> for the following notebook is available in GitLab with comments for easy viewing.</p>\n<h3>Exploring JupyterHub and Spark</h3>\n<p>JupyterHub and Spark are installed by default with Open Data Hub.  You can create Jupyter Notebooks and connect to Spark.  This is a simple <code>hello world</code>.</p>\n<ol>\n<li>Find the route to JupyterHub.  Within your Open Data Hub Project click on Networking -> Routes\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/basic-tutorial-legavy/routes.png\" alt=\"Routes\" title=\"Routes\"></li>\n<li>For the route named <code>jupyterhub</code>, click on the location to bring up JupyterHub (typically <code>https://jupyterhub-project.apps.your-cluster.your-domain.com</code>).</li>\n<li>Sign in using your OpenShift credentials.</li>\n<li>Spawn a new server with spark functionality.  (e.g. <code>s2i-spark-minimal-notebook:3.6</code>)\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/basic-tutorial-legacy/new-notebook.png\" alt=\"New Notebook\" title=\"New Notebook\"></li>\n<li>Create a new Python 3 notebook</li>\n<li>\n<p>Copy the following code to test a basic spark connection.</p>\n<pre><code class=\"language-python\">from pyspark.sql import SparkSession, SQLContext\nimport os\nimport socket\n\n# Add the necessary Hadoop and AWS jars to access Ceph from Spark\n# Can be omitted if s3 storage access is not required\nos.environ['PYSPARK_SUBMIT_ARGS'] = f\"--conf spark.jars.ivy={os.environ['HOME']} --packages org.apache.hadoop:hadoop-aws:2.7.3,com.amazonaws:aws-java-sdk:1.7.4 pyspark-shell\"\n\n# create a spark session\nspark_cluster_url = f\"spark://{os.environ['SPARK_CLUSTER']}:7077\"\nspark = SparkSession.builder.master(spark_cluster_url).getOrCreate()\n\n# test your spark connection\nspark.range(5, numPartitions=5).rdd.map(lambda x: socket.gethostname()).distinct().collect()\n</code></pre>\n</li>\n<li>\n<p>Run the notebook.  If successful, you should see the output similar to the following:</p>\n<pre><code>['jupyterhub-nb-kube-3aadmin']\n</code></pre>\n</li>\n</ol>\n<h3>Object Storage</h3>\n<p>Let's add on to the notebook from the previous section and access data on an Object Store (such as Ceph or AWS S3) using the S3 API.  For instructions on installing Ceph, refer to the Advanced Installation <a href=\"%7B%7Bsite.baseurl%7D%7D/docs/administration/advanced-installation/object-storage.html\">documentation</a>.</p>\n<ol>\n<li>Click on the <code>+</code> button and insert a new cell below of type <code>Code</code>.</li>\n<li>\n<p>To access S3 directly, we'll use the boto3 library.  We'll download a sample data file and then upload it to our S3 storage.  In the new cell paste the following code, and edit the <code>s3_</code> variables with your own credentials.</p>\n<pre><code class=\"language-python\"># Edit this section using your own credentials\ns3_region = 'region-1' # fill in for AWS, blank for Ceph\ns3_endpoint_url = 'https://s3.storage.server'\ns3_access_key_id = 'AccessKeyId-ChangeMe'\ns3_secret_access_key = 'SecretAccessKey-ChangeMe'\ns3_bucket = 'MyBucket'\n\n# for easy download\n!pip install wget\n\nimport wget\nimport boto3\n\n# configure boto S3 connection\ns3 = boto3.client('s3',\n                  s3_region,\n                  endpoint_url = s3_endpoint_url,\n                  aws_access_key_id = s3_access_key_id,\n                  aws_secret_access_key = s3_secret_access_key)\n\n# download the sample data file\nurl = \"{{site.repo}}/opendatahub.io/raw/master/assets/files/tutorials/basic/sample_data.csv\"\nfile = wget.download(url=url, out='sample_data.csv')\n\n#upload the file to storage\ns3.upload_file(file, s3_bucket, \"sample_data.csv\")\n</code></pre>\n</li>\n<li>Run the cell.  After it completes check your S3 bucket.  You should see the <code>sample_data.csv</code>.</li>\n</ol>\n<h3>Spark + Object Storage</h3>\n<p>Now, let's access that same data file from Spark so you can analyze data.</p>\n<ol>\n<li>Now let's read the data from Spark.  First, click on the <code>+</code> button and insert a new cell of type <code>Code</code>.</li>\n<li>\n<p>Paste the following code to read the data from spark and print some data.</p>\n<pre><code class=\"language-python\">hadoopConf = spark.sparkContext._jsc.hadoopConfiguration()\nhadoopConf.set(\"fs.s3a.endpoint\", s3_endpoint_url)\nhadoopConf.set(\"fs.s3a.access.key\", s3_access_key_id)\nhadoopConf.set(\"fs.s3a.secret.key\", s3_secret_access_key)\nhadoopConf.set(\"fs.s3a.path.style.access\", \"true\")\nhadoopConf.set(\"fs.s3a.connection.ssl.enabled\", \"true\") # false if not https\n\ndata = spark.read.csv('s3a://' + s3_bucket + '/sample_data.csv',sep=\",\", header=True)\ndf = data.toPandas()\ndf.head()\n</code></pre>\n</li>\n<li>Run the cell.  The data from the <code>csv</code> file should be displayed as a Pandas data frame.</li>\n</ol>\n<p>That's it!  You have a working Jupyter notebook with access to storage and Spark.</p>\n<p>{% include next-link.html label=\"Advanced Tutorials\" url=\"/docs/advanced-tutorials/data-exploration.html\" %}</p>"}},"pageContext":{"permalink":"/docs/getting-started/legacy/basic-tutorial"}},"staticQueryHashes":["1804438722"]}