{"data":{"allDataYaml":{"nodes":[{"toc":[{"title":"Introduction","url":"/docs","children":null},{"title":"Getting Started","url":"/docs/getting-started/quick-installation","children":[{"title":"Quick Installation","url":"/docs/getting-started/quick-installation","children":null},{"title":"Basic Tutorial","url":"/docs/getting-started/basic-tutorial","children":null},{"title":"Legacy","url":null,"children":[{"title":"Quick Installation","url":"/docs/getting-started/legacy/quick-installation"},{"title":"Basic Tutorial","url":"/docs/getting-started/legacy/basic-tutorial"}]}]},{"title":"Advanced Tutorials","url":"/docs/advanced-tutorials/data-exploration","children":[{"title":"Data Exploration","url":"/docs/advanced-tutorials/data-exploration","children":null}]},{"title":"Kubeflow","url":"/docs/kubeflow","children":[{"title":"Overview","url":"/docs/kubeflow","children":null},{"title":"Installation","url":"/docs/kubeflow/installation","children":null},{"title":"Combining components","url":"/docs/kubeflow/mixing","children":null}]},{"title":"AI Library","url":"/docs/ai-library","children":[{"title":"Overview","url":"/docs/ai-library","children":null},{"title":"Installation","url":"/docs/ai-library/installation","children":null},{"title":"Services","url":"/docs/ai-library/services","children":null}]},{"title":"Administration","url":"/docs/administration/installation-customization/customization","children":[{"title":"Customizing Installation","url":"/docs/administration/installation-customization/customization","children":null},{"title":"Advanced Installation","url":"/docs/administration/advanced-installation/optional","children":[{"title":"Optional Components","url":"/docs/administration/advanced-installation/optional"},{"title":"Object Storage","url":"/docs/administration/advanced-installation/object-storage"},{"title":"GPU Enablement","url":"/docs/administration/advanced-installation/gpu"}]}]},{"title":"Architecture","url":"/docs/architecture","children":null},{"title":"Roadmap","url":"/docs/roadmap/future","children":[{"title":"Future","url":"/docs/roadmap/future","children":null},{"title":"Release Notes","url":"/docs/roadmap/release-notes","children":null}]},{"title":"Additional Resources","url":"/docs/additional","children":[{"title":"Community","url":"/docs/additional#community","children":null},{"title":"Videos","url":"/docs/additional#videos","children":null},{"title":"Audio","url":"/docs/additional#audio","children":null}]}]}]},"allMarkdownRemark":{"nodes":[{"frontmatter":{"permalink":"/docs/additional","title":"Additional Resources"},"html":"<h2>[Community]({{ '/community.html' | prepend: site.baseurl }})</h2>\n<ul>\n<li><a href=\"https://gitlab.com/opendatahub/opendatahub-community\">Meetings</a> -- Community meetings for Open Data Hub are conducted regularly.  Get the meeting information and find out more on the Open Data Hub Community Repo.</li>\n<li><a href=\"%7B%7Bsite.repo%7D%7D\">Gitlab</a> -- All Open Data Hub projects are open source.  Browse the source code.</li>\n<li>\n<p><a href=\"%7B%7B%20site.email_list%20%7D%7D\">Mailing List</a> -- Stay up to date with the latest announcements and discussion about the Open Data Hub.</p>\n<!--- Slack-->\n<!--- Meetings-->\n</li>\n</ul>\n<h2>Videos</h2>\n<p>Additional videos will be available on the OpenShift youtube channel <a class=\"external-link\" href=\"https://www.youtube.com/playlist?list=PLaR6Rq6Z4Iqcg2znnClv-xbj93Q_wcY8L\" target=\"_blank\"><i class=\"fas fa-external-link-alt\"></i>AI/ML on OpenShift </a> playlist.</p>\n<h5>Tutorials</h5>\n<ul>\n<li><a href=\"https://youtu.be/-T6ypF7LoKk\">Installing Open Data Hub on OpenShift 4.1</a></li>\n<li><a href=\"https://youtu.be/d6X1xvDXewM\">Uploading data to Ceph via command line</a></li>\n</ul>\n<h5>Presentations</h5>\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=MD1x2IT7rdg\">AI on OpenShift</a></li>\n<li><a href=\"https://youtu.be/IcQ2bhsw_kQ\">Fraud Detection using the Open Data Hub</a></li>\n</ul>\n<h5>Conference Talks</h5>\n<ul>\n<li><a href=\"https://youtu.be/NZOky2Gm0iA?list=PLU1vS0speL2bxDVhBGZOiNQotzkdxJ8ln\">ML Pipelines with Kubeflow, Argo and Open Data Hub</a></li>\n<li><a href=\"https://youtu.be/og_Abr9jZJU?list=PLU1vS0speL2bxDVhBGZOiNQotzkdxJ8ln\">Scalable Kafka Deployment on OpenShift for ML</a></li>\n<li><a href=\"https://youtu.be/WgEKfAj7PLc?list=PLU1vS0speL2bxDVhBGZOiNQotzkdxJ8ln\">MLFlow: Experiment Tracking on OpenShift</a></li>\n<li><a href=\"https://youtu.be/dkuTaxWUrfE?list=PLU1vS0speL2bxDVhBGZOiNQotzkdxJ8ln\">Scaling your Open Data Hub for Fun and Production</a></li>\n<li><a href=\"https://youtu.be/tpDV8nUv45c?list=PLU1vS0speL2bxDVhBGZOiNQotzkdxJ8ln\">An Introduction to Unsupervised Deep Learning</a></li>\n<li><a href=\"https://youtu.be/Dt81qwza-zA?list=PLU1vS0speL2bxDVhBGZOiNQotzkdxJ8ln\">Unsupervised NLP for Log Anomaly Detection</a></li>\n<li><a href=\"https://youtu.be/2QJ367chSS0?list=PLU1vS0speL2bxDVhBGZOiNQotzkdxJ8ln\">Sentiment Analysis Service in a DevOps Environment</a></li>\n<li><a href=\"https://youtu.be/K8G_0z5jbcA?list=PLU1vS0speL2bxDVhBGZOiNQotzkdxJ8ln\">Machine Learning with Open Source Infrastructure</a></li>\n<li><a href=\"https://youtu.be/5lT-GajT_Wo?list=PLU1vS0speL2bxDVhBGZOiNQotzkdxJ8ln\">AIOps: Anomaly Detection with Prometheus and Istio</a></li>\n<li><a href=\"https://youtu.be/73VZaP3Mh-M?list=PLU1vS0speL2bxDVhBGZOiNQotzkdxJ8ln\">Presto: Cloud Native SQL-on-Anything</a></li>\n<li><a href=\"https://youtu.be/KWDUkm1ZeKY?list=PLU1vS0speL2bxDVhBGZOiNQotzkdxJ8ln\">Data Science in the Open Cloud Exchange Model</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=n2IW3VIZmg4\">Ceph Object Storage for AI and ML Workloads</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=by0l3b55i7g\">Data Exploration with JupyterHub on OpenShift</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=B6E7SyxOB2M\">Building AI with Ceph and OpenShift</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=iUJ6RGfY0JQ\">Using the Massachusetts Open Cloud Data Hub to perform Data Science Experiments</a></li>\n<li><a href=\"https://youtu.be/CZwUCgkKIc4\">Using the Mass Open Cloud to perform Data Science Experiments</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=RbJurxB4RSo&#x26;feature=youtu.be\">ML Workloads with GPUs on Openshift 4</a></li>\n</ul>\n<h2>Audio</h2>\n<ul>\n<li><a href=\"https://grhpodcasts.s3.amazonaws.com/opendatahub1908.mp3\">Innovate @Open podcast</a></li>\n</ul>"},{"frontmatter":{"permalink":"/docs/api","title":"API Reference"},"html":"<h1>Lorem</h1>\n<p>Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident.</p>\n<h2>Ipsum</h2>\n<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>\n<ul>\n<li>Lorem ipsum dolor sit amet</li>\n<li>Consectetur adipiscing elit</li>\n<li>Sed do eiusmod tempor</li>\n<li>Incididunt ut labore et dolore magna aliqua</li>\n<li>Duis aute irure dolor in reprehenderit</li>\n<li>sunt in culpa qui officia deserunt</li>\n</ul>\n<pre><code class=\"language-bash\"># oc type stuff\n$ oc get all\n</code></pre>\n<h2>Dolor</h2>\n<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat, <a href=\"https://opendatahub.io\">sample link</a>. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>\n<h3>Amet</h3>\n<ol>\n<li>Lorem ipsum dolor sit amet</li>\n<li>Consectetur adipiscing elit</li>\n<li>Sed do eiusmod tempor</li>\n<li>Incididunt ut labore et dolore magna aliqua</li>\n<li>Duis aute irure dolor in reprehenderit</li>\n<li>sunt in culpa qui officia deserunt</li>\n</ol>\n<h2>Tempor</h2>\n<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat, <a href=\"https://opendatahub.io\">sample link</a>. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>\n<h2>labore:</h2>\n<p>Lorem ipsum dolor sit amet <a href=\"https://opendatahub.io\">fugiat</a> ex ea commodo consequat <a href=\"https://opendatahub.io\">laborum</a>.</p>"},{"frontmatter":{"permalink":"/docs/faq","title":"FAQ"},"html":""},{"frontmatter":{"permalink":"/docs","title":"What Is Open Data Hub"},"html":"<p>[Open Data Hub (ODH)]({{ '/' | prepend: site.baseurl }}) is an open source project based on <a href=\"https://kubeflow.org/\">Kubeflow</a> that provides open source AI tools for running large and distributed AI workloads on OpenShift Container Platform. Currently, the Open Data Hub project provides open source tools for data storage, distributed AI and Machine Learning (ML) workflows, Jupyter Notebook development environment and monitoring. The Open Data Hub project [roadmap]({{ '/docs/roadmap/future.html' | prepend: site.baseurl }}) offers a view on new tools and integration the project developers are planning to add.</p>\n<p><a href=\"docs/ai-library/installation.html\">AI Library</a> is also an open source project initiated by Red Hat AI Center of Excellence team as an effort to provide ML models as a service on OpenShift Container Platform. The development of these models as services is a community driven open source project to make AI/ML models more accessible.</p>\n<p>Open Data Hub includes several open source components, which can be individially enabled. They include:</p>\n<ul>\n<li>Apache Airflow</li>\n<li>Apache Kafka</li>\n<li>Apache Spark</li>\n<li>Apache Superset</li>\n<li>Argo</li>\n<li>Grafana</li>\n<li>JupyterHub</li>\n<li>Prometheus</li>\n<li>Seldon</li>\n</ul>\n<p>For further information refer to the [architecture]({{ '/docs/architecture.html' | prepend: site.baseurl }}) and [release notes]({{ '/docs/roadmap/release-notes.html' | prepend: site.baseurl }}).</p>\n<p>{% include next-link.html label=\"Getting Started\" url=\"/docs/getting-started/quick-installation.html\" %}</p>"},{"frontmatter":{"permalink":"/docs/other","title":"Other"},"html":"<h1>Lorem</h1>\n<p>Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident.</p>\n<h2>Ipsum</h2>\n<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>\n<ul>\n<li>Lorem ipsum dolor sit amet</li>\n<li>Consectetur adipiscing elit</li>\n<li>Sed do eiusmod tempor</li>\n<li>Incididunt ut labore et dolore magna aliqua</li>\n<li>Duis aute irure dolor in reprehenderit</li>\n<li>sunt in culpa qui officia deserunt</li>\n</ul>\n<pre><code class=\"language-bash\"># oc type stuff\n$ oc get all\n</code></pre>\n<h2>Dolor</h2>\n<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat, <a href=\"https://opendatahub.io\">sample link</a>. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>\n<h3>Amet</h3>\n<ol>\n<li>Lorem ipsum dolor sit amet</li>\n<li>Consectetur adipiscing elit</li>\n<li>Sed do eiusmod tempor</li>\n<li>Incididunt ut labore et dolore magna aliqua</li>\n<li>Duis aute irure dolor in reprehenderit</li>\n<li>sunt in culpa qui officia deserunt</li>\n</ol>\n<h2>Tempor</h2>\n<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat, <a href=\"https://opendatahub.io\">sample link</a>. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>\n<h2>labore:</h2>\n<p>Lorem ipsum dolor sit amet <a href=\"https://opendatahub.io\">fugiat</a> ex ea commodo consequat <a href=\"https://opendatahub.io\">laborum</a>.</p>"},{"frontmatter":{"permalink":"/docs/architecture","title":"Architecture"},"html":"<h2>High Level Architecture</h2>\n<p>A complete end-to-end AI platform requires services for each step of the AI workflow. In general, an AI workflow includes most of the steps shown in Figure 1  and is used by multiple AI engineering personas such as Data Engineers, Data Scientists and DevOps.</p>\n<p><strong>Figure 1: AI Workflow</strong>\n<img src=\"../assets/img/pages/arch/figure-1.png\" alt=\"AI Workflow\"></p>\n<p>The first phase of an AI workflow is initiated by Data Engineers that acquire the data from different sources and perform the required transformations. Data Engineers are also responsible to store and provide access to the transformed data to Data Scientist or Data Analysts that work on the second phase in the AI workflow. In the second phase, Data Scientists perform analysis on the transformed data and create the appropriate ML models. Once the models are trained and validated accordingly, they are ready to be served on the production platform in the last phase of the AI end-to-end workflow. In the production phase, ML models are served as services on the cluster and DevOps engineers are tasked with constantly monitoring and optimizing the services. The process does not end at this last step, Data Scientists should monitor and keep validating models based on incoming and trained data sets.</p>\n<p><strong>Red Hat® OpenShift®  Container Platform</strong>  is the leading Kubernetes based container platform providing multiple functionalities for successfully running distributed AI workloads. Functionalities such as high availability and self-healing, scaling, security, resource management and operator framework are essential to successfully providing AI/ML services. OpenShift also also supports specialized hardware such as GPUs.</p>\n<p><strong>Open Data Hub</strong>(ODH) currently provides services on OpenShift for AI data services such as data storage and ingestion/transformation. For data storage and availability, ODH provides <a href=\"https://ceph.com/\">Ceph</a>, with multi protocol support including block, file and S3 object API support, both for persistent storage within the containers and as a scalable object storage data lake that AI applications can store and access data from . Rook operator can be used to easily deploy and integrate Ceph into the OpenShift and ODH ecosystem</p>\n<p>Open Data Hub also provides services for model creation, training and validation. Apache Spark™(<a href=\"https://spark.apache.org/\">https://spark.apache.org/</a>) operator is provided as part of ODH installation for distributed data ingestion, transformation and model functionalities running natively on OpenShift. For the Data Scientist development environment, ODH provides Jupyter Hub and Jupyter Notebook images running natively distributed on OpenShift. ODH roadmap includes tools for monitoring services as discussed in the section below. These tools will include the ability for natively monitoring AI services and served models on OpenShift using Prometheus and Grafana.</p>\n<p><strong>AI Library</strong> provides REST interface access to pre-trained and validated served models for several AI based services including sentiment analysis, flake analysis and duplicate bug detection.  A complete look at the AI Library architecture is available in the  <a href=\"%7B%7B&#x27;/assets/files/pages/docs/ai-lib-arch.pdf&#x27;%20%7C%20prepend:%20site.baseurl%20%7D%7D\">architecture document</a>{:target=\"_blank\"}.</p>\n<h2>Open Data Hub Platform</h2>\n<p>Open Data Hub platform is a centralized self-service solution for analytic and data science distributed workloads. It is a collection of open source tools and services natively running on OpenShift.</p>\n<h3>Components and Considerations</h3>\n<h4>End-to-End Considerations</h4>\n<p>ODH project’s main goal is to provide an open source end-to-end AI platform on OpenShift Container Platform that is equipped to run large AI/ML distributed workloads. As discussed earlier an end-to-end AI platform includes all phases of AI processing starting from data ingestion all the way to production AI/ML hosting and monitoring. There are multiple user personas for this platform that work on different phases. Figure 2  displays a high level architecture diagram of ODH as an end-to-end AI platform running on OpenShift Container platform.</p>\n<p>All the tools and components listed below are currently being used as part of Red Hat’s internal ODH platform cluster. This internal cluster is utilized by multiple internal teams of data scientists running AI/ML workloads for functions such as Anomaly Detection and Natural Language Processing. A subset of these components and tools are included in the ODH release available today and the rest are scheduled to be integrated in future releases as described in the roadmap section below. Support for each component is provided by the source entity, for example Red Hat supports Red Hat components such as OpenShift Container Platform and Ceph while open source communities support Seldon, Jupyterhub, Prometheus and so on.</p>\n<p><strong>Figure 2: End-to-End Reference AI Architecture on OpenShift</strong>\n<img src=\"../assets/img/pages/arch/figure-2.png\" alt=\"End-to-End Reference AI Architecture on OpenShift\"> \"End-to-End Reference AI Architecture on OpenShift\")</p>\n<p><strong>Data in Motion</strong> is essential in today's enterprise backend networks where data resides in multiple locations, especially to support data stored in legacy systems. Hybrid Cloud architectures also require sharing data between different cloud systems. Tools such as Red Hat AMQ Streams, Kafka and Logstash provide robust and scalable data transfer capabilities native to the OpenShift platform. Data Engineers can use these tools to transfer required data from multiple sources.</p>\n<p><strong>Storage: Data Lake/Databases/In-Memory</strong> includes  tools for distributed file, block and object storage at scale. We include tools for both relational databases and document-oriented databases. Big data storage requires the freedom of no schema constraints while data access requires some form of ordered schema definition. Data ingestion can be easily performed using Red Hat Data Grid into distributed object storage provided by Ceph. High performance in-memory datastore solutions such as Red Hat Data Grid which is based on Infinispan are essential for fast data access needed for analysis or model training.</p>\n<p><strong>Metadata Management</strong> tools basically add informational metadata to the stored data such as databases, tables, columns, partitions, schemas and location. Currently, we have investigated Hive Metastore as a solution that provides an SQL interface to access the metadata information.</p>\n<p><strong>Data Analysis: Big Data Processing</strong> tools are needed for running large distributed AI workloads. Apache Spark™is installed as an operator on OCP providing cluster wide custom resource to launch distributed AI workloads on distributed spark clusters. These spark clusters are not shared among users, they are specific to each user providing isolation of resource usage and management. Spark clusters are also ephemeral and are deleted once the user shuts down the notebook providing efficient resource management.</p>\n<p><strong>Data Analysis: Data Exploration</strong> tools provide the  query and visualization functions for data scientists to perform initial exploration of the data. Hue provides an SQL interface to query the data and basic visualization. Kibana is also a data visualization tool for Elasticsearch indexed data.</p>\n<p><strong>Data Analysis: Streaming</strong> tools such as Kafka and Elasticsearch allow for distributed and scalable message distribution native to OpenShift.</p>\n<p><strong>Artificial Intelligence and Machine Learning: Model Lifecycle</strong> tools provide functionalities to serve the model and collect essential metrics needed to monitor the lifecycle of the model. It allows constant evaluation of model performance which can lead to the need for retraining or re-validation. <a href=\"https://www.seldon.io/\">Seldon</a> is a tool that provides model hosting and metric collection from both the model itself and the component serving the model. MLflow provides parameter tracking for models and deployment functionalities.</p>\n<p><strong>Artificial Intelligence and Machine Learning: ML Applications</strong> such as the Open Data Hub AI Library provides pre-trained models such as sentiment analysis and topic modeling. These models can be deployed and used for prediction out of the box making it effortlessly accessible to users.</p>\n<p><strong>Artificial Intelligence and Machine Learning: Interactive Notebooks</strong> provide a development workspace for data scientists and business analysts to conduct their analysis work. <a href=\"https://jupyter.org/hub\">JupyterHub</a> is a tool that provides a multi-user notebook environment that allows users to use notebooks running in their own workspace. This allows for resource management isolation. Hue is also a multiuser data analysis platform that allows querying and plotting of data.</p>\n<p><strong>Artificial Intelligence and Machine Learning: Business Intelligence</strong> tools such as <a href=\"https://superset.incubator.apache.org/\">Apache Superset</a> provide a rich set of data visualization tools and come enterprise-ready with authentication, multi-user and security integrated.</p>\n<p><strong>Security and Governance</strong> include tools for providing services, data and API security and governance. Data in storage and in motion require security for both access and encryption. The Ceph Object Gateway provides encryption of uploaded objects and options for the management of encryption keys. The Ceph Object Gateway stores that data in the Ceph Storage Cluster in encrypted form. Red Hat Single Sign-On (<a href=\"https://www.keycloak.org/\">Keycloak</a>) and OpenShift provide user authentication while Red Hat 3Scale provides an API gateway for REST Interfaces.</p>\n<p><strong>Monitoring and Orchestration</strong> provide tools for monitoring all aspects of the end-to-end AI platform. This includes but is not limited to data, messaging, API, resources availability and utilization, etc. Prometheus and Grafana offer an interface for collecting and displaying metrics. For orchestration tools we included Jenkins and Argo Workflows that provide the functionality to create and manage workflows for build and release automation. Argo is OpenShift native workflow tools that can run pods in a directed acyclic graph (DAG) workflow.</p>\n<h4>Current Included Components</h4>\n<p>The ODH platform is installed on OpenShift as a native operator and is available on the OperatorHub.io. The operator framework (<a href=\"https://operatorhub.io/getting-started\">https://operatorhub.io/getting-started</a>) is an open source toolkit that provides effective, scalable and automated native application management. Operators manage custom resources that provide specific cluster wide functionalities. The ODH operator manages the ODH platform AI/ML services cluster-wide. Some of the components within the ODH platform are also operators such as Apache Spark™. Currently ,when installing the ODH operator it includes the following components: Ceph, Apache Spark, Jupyterhub, Prometheus and Grafana.</p>\n<p><strong>Apache Spark™</strong> operator is an open source operator implementation of Apache Spark™. It is developed as part of the Radanalytics community (<a href=\"https://radanalytics.io/\">https://radanalytics.io/</a>) to provide distributed Spark cluster workloads on OpenShift. This implementation creates a Spark cluster with master and worker/executor processes. Applications send tasks to executors using the SparkContext and these executors run the tasks on the cluster nodes they are assigned to. Distributed parallel execution as provided by Spark clusters are typical and essential for the success of AI/ML workloads.</p>\n<p><strong>JupyterHub</strong> (<a href=\"https://jupyter.org/hub\">https://jupyter.org/hub</a>) is an open source multi-user notebook platform that ODH provides with multiple notebook image streams that incorporate embedded features such as Spark libraries and connectors. JupyterHub provides many features such as multi-user experience for data scientists allowing them to run notebooks in their own workspaces. Authentication can also be customized as a pluggable component to support authentication protocols such as OAuth. Data scientists can use familiar tools such as Jupyter notebooks for developing complex algorithms and models. Frameworks such as numpy, scikit-learn, Tensorflow and more are available for use.</p>\n<p><strong>Prometheus</strong> (<a href=\"https://prometheus.io/\">https://prometheus.io/</a>) is an open source monitoring and alerting tool that is widely adopted across many enterprises. Prometheus can be configured to monitor targets by scraping or pulling metrics from the target’s HTTP endpoint and storing the metric name and a set of key-value pairs in a time series database. For graphing or querying this data, Prometheus provides a web portal with rudimentary options to list and graph the data. It also provides an endpoint for more powerful visualization tools such as Grafana to query the data and create graphs. An Alert Manager is also available to create alert rules to produce alerts on specific metric conditions.</p>\n<p><strong>Grafana</strong> (<a href=\"https://grafana.com/\">https://grafana.com/</a>) is an open source tool for data visualization and monitoring. Data sources such as Prometheus can be added to Grafana for metrics collection. Users create Dashboards that include comprehensive  graphs or plots of specific metrics. It includes powerful visualization capabilities for graphs, tables, and heatmaps. Ready-made dashboards for different data types and sources are also available giving Grafana users a head start. It also has support for a wide variety of plugins so that users can incorporate community-powered visualisation tools for things such as scatter plots or pie charts.</p>\n<p><strong>Argo</strong> (<a href=\"https://argoproj.github.io/\">https://argoproj.github.io/</a>) is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes.  It is useful for defining workflows using containers, running computer intensive jobs, and running CI/CD pipelines natively on Kubernetes.</p>\n<p><strong>Apache Kafka</strong> (<a href=\"https://kafka.apache.org/\">https://kafka.apache.org/</a> is a distributed streaming platform for publishing and subscribing records as well as storing and processing streams of records.  It is deployed on ODH using <strong>Strimzi</strong> (<a href=\"https://strimzi.io\">https://strimzi.io</a>) a community supported operator.</p>\n<p><strong>Seldon</strong> (<a href=\"https://www.seldon.io\">https://www.seldon.io</a>)  is an open source framework that makes it easier to deploy AI/ML models on Kubernetes and OpenShift. The model can be created and trained using many tools such as Apache Spark, scikit-learn and TensorFlow. Seldon also provides metric for Prometheus scraping. Metrics can be custom model metrics or Seldon core system metrics. </p>\n<p><strong>BeakerX</strong> (<a href=\"http://beakerx.com/\">http://beakerx.com/</a>) is an extension to Jupyter Notebooks that includes tools for plotting, creating tables and forms and many more.</p>\n<p>{% include next-link.html label=\"Roadmap\" url=\"/docs/roadmap/future.html\" %}</p>"},{"frontmatter":{"permalink":"/docs/additional/videos","title":"Videos"},"html":""},{"frontmatter":{"permalink":"/docs/administration/monitoring","title":"Monitoring"},"html":""},{"frontmatter":{"permalink":"/docs/administration/security","title":"Security"},"html":""},{"frontmatter":{"permalink":"/docs/administration/troubleshooting","title":"Troubleshooting"},"html":""},{"frontmatter":{"permalink":"/docs/administration/upgrading","title":"Upgrading"},"html":""},{"frontmatter":{"permalink":"/docs/advanced-tutorials/ceph","title":"Ceph"},"html":""},{"frontmatter":{"permalink":"/docs/advanced-tutorials/combined","title":"Combined Tutorial"},"html":""},{"frontmatter":{"permalink":"/docs/advanced-tutorials/argo","title":"Argo"},"html":""},{"frontmatter":{"permalink":"/docs/advanced-tutorials/data-exploration","title":"Data Exploration"},"html":"<h2>Pre-requisites</h2>\n<p>This tutorial requires that you followed the <a href=\"%7B%7Bsite.baseurl%7D%7D/docs/getting-started/basic-tutorial\">basic tutorial</a>. Make sure you enabled the following components in Open Data Hub CR:</p>\n<ul>\n<li>spark-operator</li>\n<li>data-catalog</li>\n</ul>\n<p>All screenshots and instructions are from OpenShift 4.2.</p>\n<h2>Exploring Data Catalog</h2>\n<p>The Data Catalog is a set of components with which you can\nread data stored in Data Lakes, create tables and query them in a SQL-like style. You can find\nbelow a picture of the simplified architecture of Data Catalog:</p>\n<p><img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/data-catalog/architecture.png\" alt=\"Data Catalog Architecture\" title=\"Data Catalog Architecture\"></p>\n<p>These are the components that are part of Data Catalog:</p>\n<ul>\n<li>Hive Metastore, responsible for maintaining the table metadata created by the user to query the data stored in Ceph/S3</li>\n<li>Spark SQL Thrift server to enable an endpoint where clients can connect using an ODBC/JDBC connection</li>\n<li>Cloudera Hue as a Data Exploration tool to explore the Data Lake, create tables and query them. You can\nalso create dashboards using the tables managed by Hive Metastore</li>\n</ul>\n<h2>Using Data Catalog</h2>\n<ol>\n<li>Find the route to Hue. Within your Open Data Hub Project click on Networking -> Routes\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/data-catalog/routes.png\" alt=\"OpenShift routes\" title=\"OpenShift routes\"></li>\n<li>For the route named hue, click on the location to bring up Hue (typically <code>http://hue-project.apps.your-cluster.your-domain.com</code>).</li>\n<li>It will open the first-time login page where you can create the superuser for Hue.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/data-catalog/hue-user-creation.png\" alt=\"Hue user creation\" title=\"Hue user creation\"></li>\n<li>As the first login, Hue will show a tutorial about the interface. You can skip the tutorial by closing the window.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/data-catalog/tutorial.png\" alt=\"Hue tutorial\" title=\"Hue tutorial\"></li>\n<li>The Hue editor will appear in a blank textarea.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/data-catalog/editor.png\" alt=\"Hue editor\" title=\"Hue editor\"></li>\n</ol>\n<p>Now we can create a table from a file inside the Data Lake.</p>\n<h2>Creating and querying tables</h2>\n<ol>\n<li>\n<p>Let's create first a database with the following command (You can run the query by either clicking on the play button in the left or type Ctrl+Enter):</p>\n<pre><code>CREATE DATABASE opendatahub;\n</code></pre>\n</li>\n<li>In the explorer, click on the refresh button. The new database will appear:\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/data-catalog/new-database.png\" alt=\"New database created\" title=\"New database created\">{:class=\"img-auto\"}</li>\n<li>\n<p>Now let's select the database with the command:</p>\n<pre><code>USE opendatahub;\n</code></pre>\n</li>\n<li>\n<p>We will create a table from the <code>sample_data.csv</code> file used in the <code>Basic Tutorial</code> section:</p>\n<pre><code>CREATE EXTERNAL TABLE opendatahub.sample(\ntimestamp TIMESTAMP,\nname STRING,\nfield STRING,\nprimary_audience STRING,\nkey_people STRING,\noutcome STRING,\nfull_notes STRING,\nemail STRING,\nsample_date DATE,\nnotes STRING,\nlowlights STRING,\nlearnings STRING,\ntrip_region STRING,\nnumber_of_days INT,\nestimated_cost FLOAT,\nproduct_mix STRING\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES (\n\"separatorChar\" = \",\",\n\"quoteChar\" = \"\\\"\",\n\"escapeChar\" = \"\\\\\" \n)\nTBLPROPERTIES(\"skip.header.line.count\"=\"1\")\nLOCATION 's3a://&#x3C;csv-file-location>'\n</code></pre>\n<p><strong>NOTE:</strong> The <code>LOCATION</code> statement needs a path to the directory where the file is stored, not the file path.</p>\n</li>\n<li>You will see the result of table creation.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/data-catalog/table-creation.png\" alt=\"New table created\" title=\"New table created\"></li>\n<li>\n<p>We can now query the data.</p>\n<pre><code>select * from opendatahub.sample limit 10;\n</code></pre>\n</li>\n<li>Check the query results in Hue.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/data-catalog/query-results.png\" alt=\"Query results\" title=\"Query results\"></li>\n</ol>\n<p>{% include next-link.html label=\"Kubeflow\" url=\"/docs/kubeflow.html\" %}</p>"},{"frontmatter":{"permalink":"/docs/advanced-tutorials/jupyterhub","title":"JupyterHub"},"html":""},{"frontmatter":{"permalink":"/docs/advanced-tutorials/hdfs","title":"Accessing Data in HDFS"},"html":""},{"frontmatter":{"permalink":"/docs/advanced-tutorials/seldon","title":"Seldon"},"html":""},{"frontmatter":{"permalink":"/docs/advanced-tutorials/kafka","title":"Kafka"},"html":""},{"frontmatter":{"permalink":"/docs/advanced-tutorials/spark","title":"Spark"},"html":""},{"frontmatter":{"permalink":"/docs/advanced-tutorials/warehouse","title":"Accessing Data in a Warehouse"},"html":""},{"frontmatter":{"permalink":"/docs/ai-library/custom-models","title":"Custom Models"},"html":"<!--{% include next-link.html label=\"API\" url=\"/docs/ai-library/api.html\" %}-->\n<p>{% include next-link.html label=\"Administration\" url=\"/docs/administration/advanced-installation/object-storage.html\" %}</p>"},{"frontmatter":{"permalink":"/docs/ai-library/api","title":"API"},"html":"<h3>Association Rule Learning</h3>\n<p>Route: </p>\n<h6>Sample Command:</h6>\n<pre><code></code></pre>\n<p>{% include next-link.html label=\"Administration\" url=\"/docs/administration/advanced-installation/object-storage.html\" %}</p>"},{"frontmatter":{"permalink":"/docs/ai-library","title":"Overview"},"html":"<h3>What is AI Library?</h3>\n<p>The AI Library is an open source collection of AI components\nmachine learning algorithms and solutions to common use cases to allow rapid prototyping.  To achieve this, it uses components of Open Data Hub, namely Seldon for serving, Argo workflows for automated training jobs, and Ceph for storage.  A more in depth look, you can read the <a href=\"%7B%7B&#x27;/assets/files/pages/docs/ai-lib-arch.pdf&#x27;%20%7C%20prepend:%20site.baseurl%20%7D%7D\">architecture document</a>{:target=\"_blank\"}.</p>\n<p><img src=\"%7B%7B&#x27;/assets/img/pages/docs/ai-library/ai-lib-physical-view.png&#x27;%20%7C%20prepend:%20site.baseurl%20%7D%7D\" alt=\"AI Library Physical View\" title=\"AI Library Physical View\">{:class=\"img-auto\"}</p>\n<p>The number of services grows often.  For an in depth look at the services, refer to the API.  Currently, the available services include:</p>\n<ul>\n<li>Anomaly Detection</li>\n<li>Association Rule Learning</li>\n<li>Correlation Analysis</li>\n<li>Regression</li>\n<li>Flake Analysis</li>\n<li>Duplicate Bug Detection</li>\n<li>Fraud Detection</li>\n<li>Topic Modeling</li>\n<li>Matrix Factorization</li>\n<li>Sentiment Analysis</li>\n</ul>\n<p>{% include next-link.html label=\"Installation\" url=\"/docs/ai-library/installation.html\" %}</p>"},{"frontmatter":{"permalink":"/docs/ai-library/installation","title":"Installation"},"html":"<h3>Pre-requisites</h3>\n<p>As a component of Open Data Hub, the AI Library requires the same pre-requisites as Open Data Hub.</p>\n<p>Installing ODH requires OpenShift 3.11 or 4.x. Documentation for OpenShift can be located (<a href=\"https://docs.openshift.com/container-platform\">here</a>).  All screenshots and instructions are from OpenShift 4.2.  For the purposes of this quick start, we used <a href=\"https://try.openshift.com/\">try.openshift.com</a> on AWS.  Tutorials have also been tested on <a href=\"https://code-ready.github.io/crc/\">Code Ready Containers</a> with 16GB of RAM.</p>\n<p>Installation of Open Data Hub components Seldon and Argo will require the installation of their respective CRDs as outlined in the Advanced Installation instructions for <a href=\"../administration/advanced-installation/optional.html\">Optional Components</a>.</p>\n<h4>External Components:</h4>\n<p>AI Library uses S3 Storage and has currently been tested with Ceph.</p>\n<ul>\n<li>Ceph Storage</li>\n</ul>\n<h4>Open Data Hub Components</h4>\n<p>In addition, several ODH components are required and can be installed simultaneously or beforehand.  </p>\n<ul>\n<li>Seldon</li>\n<li>Argo</li>\n<li>JupyterHub (recommended but not required)</li>\n</ul>\n<h3>Enabling the AI Library</h3>\n<p>Installation of the AI Library can be done during the initial installation of ODH or enabled afterwards.</p>\n<ol>\n<li>\n<p>Navigate to the ODH deployment.</p>\n<ul>\n<li>Navigate to <code>Installed Operators</code> </li>\n<li>Select <code>Open Data Hub Operator</code></li>\n<li>Click the <code>Open Data Hub</code> Tab</li>\n<li>Under <code>Open Data Hubs</code> select your deployment.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/ai-library/installation/1-odh-list.png\" alt=\"ODH List\" title=\"ODH List\"></li>\n</ul>\n</li>\n<li>Edit the ODH deployment's setting YAML.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/ai-library/installation/2-odh-yaml.png\" alt=\"ODH YAML\" title=\"ODH YAML\"></li>\n<li>\n<p>Set the pre-requisites and the ai-library settings to <code>odh_deploy: true</code> and set the s3 credentials appropriately.  You can  leave the other defaults or customized as you wish.  The required settings should look somewhat like the following:</p>\n<pre><code class=\"language-yaml\">aicoe-jupyterhub:\nodh_deploy: true\nseldon:\nodh_deploy: true\nargo:\nodh_deploy: true\nai-library:\nodh_deploy: true\ns3_endpoint: 'https://ceph.storage'\ns3_access: 'access-key'\ns3_secret: 'secret-key'\ns3_bucket: 'my-bucket'\ns3_region: 'blank-for-ceph'\n</code></pre>\n</li>\n<li>\n<p>Verify the installation.  </p>\n<ol>\n<li>Navigate to your project's status</li>\n<li>You should see several deployments including Seldon and AI Library services\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/ai-library/installation/2-odh-yaml.png\" alt=\"ODH YAML\" title=\"3-verify\"></li>\n<li>\n<p>Using curl, browser, or other client test a route to one of the AI Library services.  For example, on CRC to test linear regression:</p>\n<pre><code class=\"language-bash\">curl -k https://linear-regression-odh.apps-crc.testing/\n</code></pre>\n<p>A response of <code>Hello World!!</code> indicates the service is alive and ready.</p>\n</li>\n</ol>\n</li>\n</ol>\n<h3>Installing Sample Models and Data</h3>\n<p>The AI Library deploys the appropriate endpoints, but it does not deploy sample models and data by default.  Sample data and models are kept in a <a href=\"https://gitlab.com/opendatahub/sample-models.git\">sample-models GitLab repository</a>.  In order to try some of these models, they must be copied to the Ceph storage location accessible to AI Library.  Use your favorite method to do so.  Below details how to do so using the <code>s3cmd</code> tool.</p>\n<ol>\n<li>\n<p>Install <code>s3cmd</code> cli</p>\n<pre><code class=\"language-bash\">pip3 install s3cmd\n</code></pre>\n</li>\n<li>\n<p>Configure the credentials either as environment variables or in s3cmd config file</p>\n<pre><code class=\"language-bash\">export ACCESS_KEY=\nexport SECRET_KEY=\nexport HOST=\n</code></pre>\n<p>(or)</p>\n<pre><code class=\"language-bash\">s3cmd --configure\n</code></pre>\n</li>\n<li>\n<p>Check connectivity by using the following command to list existing buckets</p>\n<pre><code class=\"language-bash\">s3cmd ls --host=$HOST --access_key=$ACCESS_KEY --secret_key=$SECRET_KEY\n</code></pre>\n</li>\n<li>\n<p>Create a new bucket to copy data in to,</p>\n<pre><code class=\"language-bash\">s3cmd mb s3://AI-LIBRARY --host=$HOST --access_key=$ACCESS_KEY --secret_key=$SECRET_KEY\n</code></pre>\n</li>\n<li>\n<p>Clone the sample data set locally</p>\n<pre><code class=\"language-bash\">git clone ​ https://gitlab.com/opendatahub/sample-models.git\ncd sample-models\n</code></pre>\n</li>\n<li>\n<p>Sync the required directory/files to your s3 backend.</p>\n<pre><code class=\"language-bash\">s3cmd sync &#x3C;MODEL-DIRECTORY> s3://AI-LIBRARY/ --host=$HOST --access_key=$ACCESS_KEY --secret_key=$SECRET_KEY\n</code></pre>\n</li>\n<li>\n<p>Once copied, list the contents of the bucket AI-LIBRARY to check files have been copied</p>\n<pre><code class=\"language-bash\">s3cmd ls s3://AI-LIBRARY --host=$HOST --access_key=$ACCESS_KEY --secret_key=$SECRET_KEY --recursive\n</code></pre>\n</li>\n</ol>\n<p>{% include next-link.html label=\"Services\" url=\"/docs/ai-library/services.html\" %}</p>"},{"frontmatter":{"permalink":"/docs/ai-library/services","title":"Services"},"html":"<h3>Requesting Predictions</h3>\n<p>The AI services each have a route to the Seldon service.  To create a prediction a <code>POST</code> request to the route under the path <code>/api/v0.1/predictions</code>.  For instance, the flake analysis route on a CRC cluster in the Project `odh' will look something like:</p>\n<pre><code>https://regrclassifier-odh.apps-crc.testing/api/v0.1/predictions\n</code></pre>\n<p>Models vary according to each service, but often, data is passed in through a field <code>strData</code> and parsed by the model accordingly.  Within this <code>strData</code> field, most models require the a list of key value pairs separated by commas.  For this reason, strData must have any other commas stripped out.  Two common keys are <code>model</code> and <code>data</code>.  A simple JSON body would look like:</p>\n<pre><code class=\"language-json\">{\"strData\":\"model=healthpredictor/model.pkl, data=66:150\"}\n</code></pre>\n<p>Putting that together in a curl command:</p>\n<pre><code class=\"language-bash\">curl 'https://regrclassifier-odh.apps-crc.testing/api/v0.1/predictions' \\\n  -X POST \\\n  -H 'Content-Type: application/json' \\\n  -H 'Accept: application/json' \\\n  -d '{\"strData\":\"model=healthpredictor/model.pkl, data=66:150\"}'\n</code></pre>\n<p>Sample Output:</p>\n<pre><code class=\"language-json\">{\n  \"meta\": {\n    \"puid\": \"k8h9uir10qdbsmv1qigp2qnk5l\",\n    \"tags\": {},\n    \"routing\": {},\n    \"requestPath\": {\n      \"c-regrclassifier\": \"quay.io/opendatahub/regression_predict\"\n    },\n    \"metrics\": []\n  },\n  \"data\": {\n    \"names\": [],\n    \"tensor\": {\n      \"shape\": [\n        1\n      ],\n      \"values\": [\n        2.2885771963212247\n      ]\n    }\n  }\n}\n</code></pre>\n<h3>Service Documentation</h3>\n<p>As each service varies, please visit the documentation for the service and model.</p>\n<ul>\n<li><a href=\"https://gitlab.com/opendatahub/ai-library/tree/master/anomaly_detection\">Anomaly Detection</a></li>\n<li><a href=\"https://gitlab.com/opendatahub/ai-library/tree/master/association_rule_learning\">Association Rule Learning</a></li>\n<li><a href=\"https://gitlab.com/opendatahub/ai-library/tree/master/correlation_analysis\">Correlation Analysis</a></li>\n<li>\n<p>Duplicate Bug Prediction</p>\n<ul>\n<li><a href=\"https://gitlab.com/opendatahub/ai-library/tree/master/duplicate_bug_predict\">Predictions</a></li>\n<li><a href=\"https://gitlab.com/opendatahub/ai-library/tree/master/duplicate_bug_train\">Training</a></li>\n</ul>\n</li>\n<li>\n<p>Flake Analysis</p>\n<ul>\n<li><a href=\"https://gitlab.com/opendatahub/ai-library/tree/master/flakes_predict\">Prediction</a></li>\n<li><a href=\"https://gitlab.com/opendatahub/ai-library/tree/master/flakes_train\">Training</a></li>\n</ul>\n</li>\n<li><a href=\"https://gitlab.com/opendatahub/ai-library/tree/master/fraud_detection\">Fraud Detection</a></li>\n<li><a href=\"https://gitlab.com/opendatahub/ai-library/tree/master/matrix_factorization\">Matrix Factorization</a></li>\n<li>\n<p><a href=\"https://gitlab.com/opendatahub/ai-library/tree/master/regression_predict\">Regression Analysis</a></p>\n<ul>\n<li><a href=\"https://gitlab.com/opendatahub/ai-library/tree/master/regression_predict\">Prediction</a></li>\n<li><a href=\"https://gitlab.com/opendatahub/ai-library/tree/master/regression_train\">Training</a></li>\n</ul>\n</li>\n<li><a href=\"https://gitlab.com/opendatahub/ai-library/tree/master/sentiment_analysis\">Sentiment Analysis</a></li>\n<li><a href=\"https://gitlab.com/opendatahub/ai-library/tree/master/topic_model\">Topic Modeling</a></li>\n</ul>\n<p>{% include next-link.html label=\"Administration\" url=\"/docs/administration/installation-customization/customization.html\" %}</p>"},{"frontmatter":{"permalink":"/docs/kubeflow/installation","title":"Installation"},"html":"<h3>Pre-requisites</h3>\n<p>To install Kubeflow 0.7 on OpenShift 4.2 the following are the prerequisites:<br>\n1.<strong>Code Ready Container (CRC)</strong>:  A CRC-generated OpenShift cluster that with the following specifications:        Recommended:<br>\n<code>16GB memory</code><br>\n<code>6 cpu</code><br>\n<code>45G disk space</code><br>\nMinimum:<br>\n<code>10GB memory</code><br>\n<code>6 cpu</code><br>\n<code>30G disk space (default for CRC)</code>  </p>\n<p>   <strong>NOTE</strong>: At the minimum specs, the CRC OpenShift cluster may be unresponsive for ~20mins while Kubeflow components are being deployed.<br>\nWhen installing Kubeflow on a CRC cluster, there is an extra overlay (named <code>crc</code>) to enable the <code>metadata</code> component in <code>kfctl_openshift.yaml</code>.  It is commented out by default.  Uncomment to enable it.   </p>\n<p>  OR<br>\n<strong>OpenShift 4.2</strong>: An available OpenShift 4.2 cluster or you can try a cluster on <a href=\"https://try.openshift.com\">try.openshift.com</a>  </p>\n<p>2.<strong>kfctl</strong>: The installation tool <code>kfctl</code> is needed to install/uninstall Kubeflow. Download the tool from <a href=\"https://github.com/kubeflow/kubeflow/releases/\">github</a>, the version <code>0.7.0</code> is required for the installation.</p>\n<h3>Install Kubeflow with Istio Enabled</h3>\n<p>To install Kubeflow 0.7 on OpenShift 4.2 please follow the steps below:  </p>\n<ol>\n<li>Clone the opendatahub-manifests fork repo which defaults to the branch <code>v0.7.0-branch-openshift</code><br>\n<code>git clone https://github.com/opendatahub-io/manifests.git</code><br>\n<code>cd manifests</code></li>\n<li>Install using the Openshift configuration file and local downloaded manifests since at the time of writing this document we ran into this kubeflow <a href=\"https://github.com/kubeflow/kubeflow/issues/4678\">bug</a> that would not allow downloading the manifests during a build process.<br>\n<code>sed -i 's#uri: .*#uri: '$PWD'#' ./kfdef/kfctl_openshift.yaml</code><br>\n<code>kfctl build --file=kfdef/kfctl_openshift.yaml</code><br>\n<code>kfctl apply --file=./kfdef/kfctl_openshift.yaml</code>  </li>\n<li>Verify installation<br>\n<code>oc get pods</code></li>\n<li>Launch the Kubeflow portal<br>\n<code>oc get routes -n istio-system istio-ingressgateway -o jsonpath='http://{.spec.host}/'</code><br>\n<code>http://&#x3C;istio ingress route>/</code>  </li>\n</ol>\n<h3>Delete A Kubeflow installation</h3>\n<p>To delete a Kubeflow installation please follow these steps:<br>\n<code>kfctl delete --file=./kfdef/&#x3C;kfctl file name>.yaml</code><br>\n<code>rm -rf kfdef/kustomize/</code><br>\n<code>oc delete mutatingwebhookconfigurations admission-webhook-mutating-webhook-configuration</code><br>\n<code>oc delete mutatingwebhookconfigurations inferenceservice.serving.kubeflow.org</code><br>\n<code>oc delete mutatingwebhookconfigurations katib-mutating-webhook-config</code><br>\n<code>oc delete mutatingwebhookconfigurations mutating-webhook-configurations</code>  </p>\n<p>{% include next-link.html label=\"Combining Components\" url=\"/docs/kubeflow/mixing.html\" %}</p>"},{"frontmatter":{"permalink":"/docs/kubeflow/mixing","title":"Combining Open Data Hub and Kubeflow Components"},"html":"<h3>Combining Open Data Hub and Kubeflow Components</h3>\n<p>With Open Data Hub Operator being built on top of Kubeflow operator, users are enabled to deploy both Open Data Hub and Kubeflow by the same operator. It is also our goal to provide the capability to mix components from both projects.</p>\n<h4>How to Deploy Components From Both Projects</h4>\n<p>We are maintaining an example KFDef manifest which includes components from both projects. You can find it in <a href=\"https://github.com/opendatahub-io/odh-manifests/blob/master/kfdef/kfctl_openshift_mix.yaml\">opendatahub-io/manifests</a> repository.</p>\n<p>To use it, simply follow the <a href=\"%7B%7Bsite.baseurl%7D%7D/docs/getting-started/basic-tutorial\">basic tutorial</a>, but instead of the default KFDef manifest, deploy the one linked above:</p>\n<pre><code>oc apply -f https://raw.githubusercontent.com/opendatahub-io/odh-manifests/master/kfdef/kfctl_openshift_mix.yaml\n</code></pre>\n<h4>How Does it Work</h4>\n<p>KFDef manifest allows you to list multiple repository references. This is useful for mixing components from ODH and Kubeflow, but also for <a href=\"%7B%7Bsite.baseurl%7D%7D/docs/administration/installation-customization/customization\">the deployment customization</a>.</p>\n<p>As you can see, we are providing references to <a href=\"https://github.com/opendatahub-io/odh-manifests/\">odh-manifests</a> repository (as <code>manifests</code>) and to <a href=\"https://github.com/opendatahub-io/manifests/\">manifests</a> repository (as <code>kf-manifests</code>). With these two references in your KFDef, you can simply mix and match components from both of them</p>\n<pre><code>...\n  - kustomizeConfig:\n      repoRef:\n        name: manifests\n        path: odhargo/cluster\n    name: odhargo-cluster\n  - kustomizeConfig:\n      repoRef:\n        name: manifests\n        path: odhargo/odhargo\n    name: odhargo\n  - kustomizeConfig:\n      repoRef:\n        name: kf-manifests\n        path: tf-training/tf-job-crds\n    name: tf-job-crds\n  - kustomizeConfig:\n      repoRef:\n        name: kf-manifests\n        path: tf-training/tf-job-operator\n    name: tf-job-operator\n  repos:\n  - name: kf-manifests\n    uri: https://github.com/opendatahub-io/manifests/tarball/v1.0-branch-openshift  \n  - name: manifests\n    uri: https://github.com/opendatahub-io/odh-manifests/tarball/master\n...\n</code></pre>\n<p>As you can see above, there are 2 components mentioned - Argo and TF Job Operator. Argo uses <code>manifests</code> as a source repository and TF Job Operator uses <code>kf-manifests</code>.</p>\n<h3>Verified Components</h3>\n<table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Version</th>\n<th>Issues</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://github.com/opendatahub-io/manifests/blob/v1.0-branch-openshift/tf-training/\">TF Job</a></td>\n<td>v1</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p>{% include next-link.html label=\"AI Library\" url=\"/docs/ai-library.html\" %}</p>"},{"frontmatter":{"permalink":"/docs/kubeflow","title":"Overview"},"html":"<h3>What is Kubeflow?</h3>\n<p><a href=\"https://www.kubeflow.org/\">Kubeflow</a> is an open source AI/ML project focused on model training, serving, pipelines, and metadata. As part of the Open Data Hub project we worked on enabling Kubeflow 0.7 on Openshift 4.2. The following Kubeflow components are included in the installation</p>\n<h4>Components</h4>\n<ul>\n<li>Central Dashboard</li>\n<li>Jupyterhub</li>\n<li>Katib</li>\n<li>Pipelines </li>\n<li>Training: Pytorch, tf-jobs</li>\n<li>Serving: Seldon</li>\n<li>Istio </li>\n</ul>\n<p>{% include next-link.html label=\"Installation\" url=\"/docs/kubeflow/installation.html\" %}</p>"},{"frontmatter":{"permalink":"/docs/roadmap/release-notes","title":"Release Notes"},"html":"<h3>Open Data Hub version v0.6.1 - May 2020</h3>\n<table>\n<thead>\n<tr>\n<th>Technology</th>\n<th>Version</th>\n<th>Category</th>\n<th>Deployment Method</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://github.com/opendatahub-io/odh-manifests/tree/master/ai-library\">AI Library</a></td>\n<td>0.6.0</td>\n<td>Machine Learning</td>\n<td>Manifests</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/opendatahub-io/odh-manifests/tree/master/odhseldon\">Seldon</a></td>\n<td>1.1.0</td>\n<td>Model Serving</td>\n<td>OLM</td>\n</tr>\n</tbody>\n</table>\n<p>{:class=\"table table-bordered\"}</p>\n<h3>Open Data Hub version v0.6.0 - May 2020 (redesign on top of Kubeflow)</h3>\n<table>\n<thead>\n<tr>\n<th>Technology</th>\n<th>Version</th>\n<th>Category</th>\n<th>Deployment Method</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://github.com/opendatahub-io/odh-manifests/tree/master/airflow\">Airflow</a></td>\n<td>Alpha</td>\n<td>Workflow Management</td>\n<td>Manifests</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/opendatahub-io/odh-manifests/tree/master/grafana\">Grafana</a></td>\n<td>2.0.0</td>\n<td>Monitoring Dashboards</td>\n<td>OLM</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/opendatahub-io/odh-manifests/tree/master/jupyterhub\">JupyterHub</a></td>\n<td>3.0.7</td>\n<td>Data science tools</td>\n<td>Manifests</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/opendatahub-io/odh-manifests/tree/master/kafka\">Kafka Strimzi</a></td>\n<td>0.17.0</td>\n<td>Distributed Streaming</td>\n<td>OLM</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/opendatahub-io/odh-manifests/tree/master/odhargo\">Argo</a></td>\n<td>2.7.0</td>\n<td>Workflow Engine</td>\n<td>Manifests</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/opendatahub-io/odh-manifests/tree/master/prometheus\">Prometheus</a></td>\n<td>0.32.0</td>\n<td>Monitoring</td>\n<td>OLM</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/opendatahub-io/odh-manifests/tree/master/radanalyticsio\">Radanalytics Spark Operator</a></td>\n<td>1.0.7</td>\n<td>Operator for managing Spark cluster on OpenShift</td>\n<td>Manifests</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/opendatahub-io/odh-manifests/tree/master/superset\">Apache Superset</a></td>\n<td>0.34.0</td>\n<td>Data Exploration and Visualization Tool</td>\n<td>Manifests</td>\n</tr>\n</tbody>\n</table>\n<p>{:class=\"table table-bordered table-striped\"}</p>\n<h3>Open Data Hub version v0.5.1 - February 2020</h3>\n<table>\n<thead>\n<tr>\n<th>Technology</th>\n<th>Version</th>\n<th>Category</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>JupyterHub</td>\n<td>3.0.7</td>\n<td>Update kubespawner to version 0.11.1 and enable user customization of the jupyterhub config</td>\n</tr>\n<tr>\n<td>Radanalytics <a href=\"https://github.com/radanalyticsio/spark-operator\">Spark Operator</a></td>\n<td>1.0.5</td>\n<td>Adds support for customizing the Spark cluster resource requests and limits for cpu/memory</td>\n</tr>\n<tr>\n<td>Apache <a href=\"https://github.com/apache/incubator-superset\">Superset</a></td>\n<td>0.34.0</td>\n<td>Resolve issues related to connecting to the Data Catalog Thrift Server</td>\n</tr>\n</tbody>\n</table>\n<p>{:class=\"table table-bordered\"}</p>\n<h3>Open Data Hub version v0.5.0 - December 2019</h3>\n<table>\n<thead>\n<tr>\n<th>Technology</th>\n<th>Version</th>\n<th>Category</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>JupyterHub CUDA GPU Images and Notebooks</td>\n<td>3.0.7</td>\n<td>Support for building CUDA GPU Images and GPU Notebook</td>\n</tr>\n<tr>\n<td>Apache <a href=\"https://github.com/apache/incubator-superset\">Superset</a></td>\n<td>0.34.0</td>\n<td>Data Exploration and Visualization Tool</td>\n</tr>\n<tr>\n<td>Data Catalog (<a href=\"https://gethue.com/\">Hue</a>, <a href=\"https://spark.apache.org/docs/latest/sql-distributed-sql-engine.html\">Spark Thrift Server</a>, Hive Metastore)</td>\n<td>Hue 4.4.1 &#x26; Spark 2.4.3 &#x26; Spark Thrift Server 2.4 &#x26; Hive Metastore 1.2.1</td>\n<td>Deployment of Hue, Spark Thrift Server and Hive Metastore to simplify querying data lakes using Spark SQL language</td>\n</tr>\n<tr>\n<td><a href=\"https://argoproj.github.io/argo/\">Argo</a></td>\n<td>2.4.2</td>\n<td>Container native workflow engine</td>\n</tr>\n</tbody>\n</table>\n<p>{:class=\"table table-bordered\"}</p>\n<h3>Open Data Hub version v0.4.0 - September 2019</h3>\n<table>\n<thead>\n<tr>\n<th>Technology</th>\n<th>Version</th>\n<th>Category</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://argoproj.github.io/argo/\">Argo</a></td>\n<td>2.3.0</td>\n<td>Container native workflow engine</td>\n</tr>\n<tr>\n<td><a href=\"https://strimzi.io/\">Strimzi Kafka Operator</a></td>\n<td>0.11.1</td>\n<td>Distributed streaming platform</td>\n</tr>\n<tr>\n<td>Open Data Hub AI-Library</td>\n<td>1.0</td>\n<td>Machine learning as a service</td>\n</tr>\n</tbody>\n</table>\n<p>{:class=\"table table-bordered\"}</p>\n<h3>Open Data Hub version v0.3.0 - June 2019</h3>\n<table>\n<thead>\n<tr>\n<th>Technology</th>\n<th>Version</th>\n<th>Category</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Seldon</td>\n<td>0.2.7</td>\n<td>Model Serving and Metrics Tool</td>\n</tr>\n<tr>\n<td>JupyterHub with GPU Support</td>\n<td>3.0.7</td>\n<td>Data science tools</td>\n</tr>\n<tr>\n<td>Apache Spark</td>\n<td>2.2.3</td>\n<td>Query &#x26; ETL frameworks</td>\n</tr>\n<tr>\n<td>TwoSigma BeakerX Integration</td>\n<td>1.4.0</td>\n<td>Data science tools</td>\n</tr>\n<tr>\n<td>Prometheus</td>\n<td>2.3</td>\n<td>System monitoring tools</td>\n</tr>\n<tr>\n<td>Grafana</td>\n<td>4.7</td>\n<td>System monitoring tools</td>\n</tr>\n</tbody>\n</table>\n<p>{:class=\"table table-bordered\"}</p>\n<h3>Open Data Hub version v0.2.0 - May 2019</h3>\n<table>\n<thead>\n<tr>\n<th>Technology</th>\n<th>Version</th>\n<th>Category</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>JupyterHub with GPU Support</td>\n<td>3.0.7</td>\n<td>Data science tools</td>\n</tr>\n<tr>\n<td>Apache Spark</td>\n<td>2.2.3</td>\n<td>Query &#x26; ETL frameworks</td>\n</tr>\n<tr>\n<td>TwoSigma BeakerX Integration</td>\n<td>1.4.0</td>\n<td>Data science tools</td>\n</tr>\n</tbody>\n</table>\n<p>{:class=\"table table-bordered\"}</p>\n<p>{% include next-link.html label=\"Additional Resources\" url=\"/docs/additional.html\" %}</p>"},{"frontmatter":{"permalink":"/docs/administration/advanced-installation/gpu","title":"GPU Enablement"},"html":"<p>Open Data Hub has support for accessing NVIDIA GPUs from Jupyter notebooks. Currently,\nthe Open Data Hub team has verified that JupyterHub can successfully access the GPU in OpenShift 3.11 clusters.\nOpenShift 4.x GPU enablement is still in development. These instructions will be updated as soon as it is available.</p>\n<h3>Prerequisites</h3>\n<ul>\n<li>\n<p>OpenShift cluster with GPU(s) enabled</p>\n<ul>\n<li>Enabling GPUs in OpenShift 3.11 is outside of the scope of Open Data Hub. The document <a href=\"https://github.com/zvonkok/origin-ci-gpu/blob/release-3.11/doc/How%20to%20use%20GPUs%20with%20DevicePlugin%20in%20OpenShift%203.11%20.pdf\">How to use GPUs with DevicePlugin in OpenShift 3.11</a> can provide guidance, but is not official.</li>\n<li>\n<p>Enabling GPUs in OpenShift 4.x can be achieved by deploying Node Feature Discovery (NFD) Operator and NVIDIA GPU Operator.</p>\n<p>The <a href=\"https://github.com/openshift/cluster-nfd-operator\">Node Feature Discovery</a> operator is responsible for discovering and labeling hardware (GPU(s) in this case) features available on each node.\nThe <a href=\"https://github.com/NVIDIA/gpu-operator\">NVIDIA GPU Operator</a> will setup and install the necessary drivers to enable the use of GPU(s) as compute resource.</p>\n<ol>\n<li>Deploy the Node Feature Discovery operator using the OpenShift OperatorHub WebUI.\nThe blog on <a href=\"https://blog.openshift.com/creating-a-gpu-enabled-node-with-openshift-4-2-in-amazon-ec2\">Creating a GPU-enabled node with OpenShift 4.2 in Amazon EC2</a> has a \"Deploy the Node Feature Discovery Operator\" section that demonstrates how to deploy the NFD operator and create a <code>NodeFeatureDiscovery</code> custom resource</li>\n<li>\n<p>Before deploying the NVIDIA GPU Operator confirm that all of the nodes with GPUs have the appropriate hardware labels.</p>\n<pre><code>$ oc describe node &#x3C;GPU NODE NAME> -o json \n\nName:    ip-10-0-137-200.ec2.internal\nRoles:   worker\nLabels:  beta.kubernetes.io/arch=amd64\n          beta.kubernetes.io/instance-type=p2.xlarge\n          beta.kubernetes.io/os=linux\n          failure-domain.beta.kubernetes.io/region=us-east-1\n          failure-domain.beta.kubernetes.io/zone=us-east-1a\n          feature.node.kubernetes.io/cpu-cpuid.ADX=true\n          feature.node.kubernetes.io/cpu-cpuid.AESNI=true\n          feature.node.kubernetes.io/cpu-cpuid.AVX=true\n          feature.node.kubernetes.io/cpu-cpuid.AVX2=true\n          feature.node.kubernetes.io/cpu-cpuid.FMA3=true\n          feature.node.kubernetes.io/cpu-cpuid.HLE=true\n          feature.node.kubernetes.io/cpu-cpuid.RTM=true\n          feature.node.kubernetes.io/cpu-hardware_multithreading=true\n          feature.node.kubernetes.io/cpu-pstate.turbo=true\n          feature.node.kubernetes.io/kernel-selinux.enabled=true\n          feature.node.kubernetes.io/kernel-version.full=4.18.0-147.3.1.el8_1.x86_64\n          feature.node.kubernetes.io/kernel-version.major=4\n          feature.node.kubernetes.io/kernel-version.minor=18\n          feature.node.kubernetes.io/kernel-version.revision=0\n          feature.node.kubernetes.io/pci-1013.present=true\n          feature.node.kubernetes.io/pci-10de.present=true\n          feature.node.kubernetes.io/pci-1d0f.present=true\n          feature.node.kubernetes.io/storage-nonrotationaldisk=true\n          feature.node.kubernetes.io/system-os_release.ID=rhcos\n          feature.node.kubernetes.io/system-os_release.VERSION_ID=4.3\n          feature.node.kubernetes.io/system-os_release.VERSION_ID.major=4\n          feature.node.kubernetes.io/system-os_release.VERSION_ID.minor=3\n          ...\n</code></pre>\n</li>\n<li>Deploy the NVIDIA GPU operator using the OpenShift OperatorHub WebUI.\nThe guide <a href=\"https://docs.nvidia.com/datacenter/kubernetes/openshift-on-gpu-install-guide/index.html\">OpenShift on GPU install</a> has a <a href=\"https://docs.nvidia.com/datacenter/kubernetes/openshift-on-gpu-install-guide/index.html#openshift-gpu-support\">GPU support</a> section covering all the steps to deploy the NVIDIA GPU operator.</li>\n<li>\n<p>Once the Special Resource Operator finishes the installation of the appropriate drivers, you should see the number of gpus available as a resource on the GPU enabled nodes.</p>\n<pre><code>$ oc get node &#x3C;GPU NODE NAME> -o json | jq .status.allocatable\n{\n  \"cpu\": \"3500m\",\n  \"hugepages-1Gi\": \"0\",\n  \"hugepages-2Mi\": \"0\",\n  \"memory\": \"15804984Ki\",\n  \"pods\": \"250\",\n  \"nvidia.com/gpus\": \"1\"\n}\n</code></pre>\n</li>\n</ol>\n</li>\n</ul>\n</li>\n</ul>\n<h3>Configuring the JupyterHub component</h3>\n<p>To properly configure the <code>aicoe-jupytherhub</code> component to use GPUs, 2 things must be done</p>\n<ol>\n<li>Deploy GPU Compatible Jupyter Notebook Images by setting <code>notebook_images.deploy_cuda_notebooks: True</code>.</li>\n<li>Configure JupyterHub Server to spawn user pods with GPU access by setting <code>gpu_mode</code> to empty/<code>null</code>, <code>selinux</code>, or <code>privileged</code></li>\n</ol>\n<p>These settings will deploy a set of <code>BuildConfigs</code> which prepare <strong>CUDA enabled Python s2i images</strong> which are then used for building <code>s2i-tensorflow-notebook-gpu</code> image available in JupyterHub Spawner UI. This image has Tensorflow GPU pre-installed and enables users to leverage GPUs available in a cluster with the Tensorflow library.</p>\n<p>The build chain template can be found upstream at <a href=\"https://github.com/thoth-station/tensorflow-build-s2i/tree/master/cuda\">https://github.com/thoth-station/tensorflow-build-s2i/tree/master/cuda</a></p>\n<h4>OpenShift 4.x</h4>\n<p>Leave the <code>gpu_mode</code> empty or set it to <code>null</code>.</p>\n<pre><code class=\"language-yaml\">aicoe-jupyterhub:\n    gpu_mode: \"\"\n    notebook_images:\n        deploy_cuda_notebooks: True\n</code></pre>\n<h4>OpenShift 3.11 (Standard)</h4>\n<p>If the configuration of the cluster is based on the documentation <a href=\"https://github.com/zvonkok/origin-ci-gpu/blob/release-3.11/doc/How%20to%20use%20GPUs%20with%20DevicePlugin%20in%20OpenShift%203.11%20.pdf\">How to use GPUs with DevicePlugin in OpenShift 3.11</a>, set gpu_mode to value <code>selinux</code></p>\n<pre><code class=\"language-yaml\">aicoe-jupyterhub:\n    gpu_mode: \"selinux\"\n    notebook_images:\n        deploy_cuda_notebooks: True\n</code></pre>\n<h4>OpenShift 3.11 (Other)</h4>\n<p>In the case GPU enablement in the cluster is configured differently, containers may be required to run in privileged mode to gain access to GPUs. In this case set <code>gpu_mode</code> to value <code>privileged</code></p>\n<pre><code class=\"language-yaml\">aicoe-jupyterhub:\n    gpu_mode: \"privileged\"\n    notebook_images:\n        deploy_cuda_notebooks: True\n</code></pre>\n<h3>Verifying GPU Availability</h3>\n<p>To spawn a Jupyter Notebook with GPU support, set the <code>GPU</code> field to a number greater than 0.  From inside the notebook, run the following command to verify it's availability.</p>\n<pre><code class=\"language-python\">import tensorflow as tf\ntf.test.is_gpu_available()\n</code></pre>\n<h4>Additional resources</h4>\n<ul>\n<li><a href=\"https://blog.openshift.com/how-to-use-gpus-with-deviceplugin-in-openshift-3-10\">How to use GPUs with DevicePlugin in OpenShift 3.10</a></li>\n<li><a href=\"https://docs.openshift.com/container-platform/3.11/dev_guide/device_plugins.html\">OpenShift 3.11 - Using Device Plug-ins</a></li>\n<li><a href=\"https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus\">Kubernetes GPU Documentation</a></li>\n<li><a href=\"https://docs.openshift.com/container-platform/3.11/admin_guide/manage_scc.html\">OpenShift 3.11 Cluster Administration / Managing Security Context Constraints</a></li>\n<li><a href=\"https://github.com/AICoE/jupyterhub-ocp-oauth\">JupyterHub deployment using OpenShift OAuth authenticator</a></li>\n</ul>\n<p>{% include next-link.html label=\"Architecture\" url=\"/docs/architecture.html\" %}</p>"},{"frontmatter":{"permalink":"/docs/roadmap/future","title":"Roadmap"},"html":"<p>{% include roadmap.html %}</p>\n<p>{% include next-link.html label=\"Release Notes\" url=\"/docs/roadmap/release-notes.html\" %}</p>"},{"frontmatter":{"permalink":"/docs/administration/advanced-installation/object-storage","title":"Installing Object Storage"},"html":"<h4>Ceph installation with the Rook operator</h4>\n<p>To install Ceph with the Rook operator, please follow these instructions (this does require cluster-admin permissions):</p>\n<ol>\n<li>\n<p>Download the files for Rook Ceph v0.9.3</p>\n<ul>\n<li>\n<p>If you want to modify the source Rook Ceph files directly, clone the Rook operator and checkout the v0.9.3 branch.</p>\n<pre><code class=\"language-bash\">git clone https://github.com/rook/rook.git\ncd rook\ngit checkout -b rook-0.9.3 v0.9.3\ncd cluster/examples/kubernetes/ceph/\n</code></pre>\n</li>\n<li>\n<p>Here are links to modified versions of the files that will install a basic Rook Ceph cluster with object storage enabled</p>\n<ul>\n<li><a href=\"{{ '/assets/files/pages/arch/rook/v0.9.3/scc.yaml' | prepend: site.baseurl }}\" download>scc.yaml</a></li>\n<li><a href=\"{{ '/assets/files/pages/arch/rook/v0.9.3/operator.yaml' | prepend: site.baseurl }}\" download>operator.yaml</a></li>\n<li><a href=\"{{ '/assets/files/pages/arch/rook/v0.9.3/cluster.yaml' | prepend: site.baseurl }}\" download>cluster.yaml</a></li>\n<li><a href=\"{{ '/assets/files/pages/arch/rook/v0.9.3/toolbox.yaml' | prepend: site.baseurl }}\" download>toolbox.yaml</a></li>\n<li><a href=\"{{ '/assets/files/pages/arch/rook/v0.9.3/object.yaml' | prepend: site.baseurl }}\" download>object.yaml</a></li>\n<li><a href=\"{{ '/assets/files/pages/arch/rook/v0.9.3/object-user.yaml' | prepend: site.baseurl }}\" download>object-user.yaml</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>Edit <code>operator.yaml</code> and set the environment variables for <code>FLEXVOLUME_DIR_PATH</code> and <code>ROOK_HOSTPATH_REQUIRES_PRIVILEGED</code> to allow the Rook operator to use OpenShift hostpath storage</p>\n<pre><code class=\"language-yaml\"></code></pre>\n</li>\n<li>name: FLEXVOLUME<em>DIR</em>PATH\nvalue: \"/etc/kubernetes/kubelet-plugins/volume/exec\"</li>\n<li>\n<p>name: ROOK<em>HOSTPATH</em>REQUIRES_PRIVILEGED\nvalue: \"true\"</p>\n<pre><code></code></pre>\n</li>\n<li>\n<p>Configure the necessary security contexts, and deploy the rook operator, this will create a new namespace, <code>rook-ceph-system</code>, and deploy the pods in it:</p>\n<pre><code class=\"language-bash\">oc create -f scc.yaml\noc create -f operator.yaml\n</code></pre>\n</li>\n<li>\n<p>Check that all the pods are running in <code>rook-ceph-system</code></p>\n<pre><code class=\"language-bash\">oc get pods -n rook-ceph-system\nNAME READY STATUS RESTARTS AGE\nrook-ceph-agent-j4zms 1/1 Running 0 33m\nrook-ceph-agent-qghgc 1/1 Running 0 33m\nrook-ceph-agent-tjzv6 1/1 Running 0 33m\nrook-ceph-agent-w2rnk 1/1 Running 0 33m\nrook-ceph-operator-567f8cbb6-f5rsj 1/1 Running 0 33m\nrook-discover-gghsw 1/1 Running 0 33m\nrook-discover-jd226 1/1 Running 0 33m\nrook-discover-lgfrx 1/1 Running 0 33m\nrook-discover-lnjwp 1/1 Running 0 33m\n</code></pre>\n</li>\n<li>\n<p>Edit the <code>object.yaml</code> file and replace port <code>80</code> with <code>8080</code></p>\n<pre><code class=\"language-yaml\">gateway:\n# type of the gateway (s3)\ntype: s3\n# A reference to the secret in the rook namespace where the ssl certificate is stored\nsslCertificateRef:\n# The port that RGW pods will listen on (http)\nport: 8080\n</code></pre>\n</li>\n<li>\n<p>Once the operator is ready, you can create a Ceph cluster, and Ceph object service. The toolbox service is also handy to deploy for checking the health of the Ceph cluster.</p>\n<pre><code class=\"language-bash\">oc create -f cluster.yaml\noc create -f toolbox.yaml\noc create -f object.yaml\n</code></pre>\n</li>\n<li>\n<p>Check all the pods are running in <code>rook-ceph</code> namespace before proceeding</p>\n<pre><code class=\"language-bash\">oc get pods -n rook-ceph\nNAME READY STATUS RESTARTS AGE\nrook-ceph-mgr-a-5b6fcf7c6-cx676 1/1 Running 0 6m56s\nrook-ceph-mon-a-54d9bc6c97-kvfv6 1/1 Running 0 8m38s\nrook-ceph-mon-b-74699bf79f-2xlzz 1/1 Running 0 8m22s\nrook-ceph-mon-c-5c54856487-769fx 1/1 Running 0 7m47s\nrook-ceph-osd-0-7f4c45fbcd-7g8hr 1/1 Running 0 6m16s\nrook-ceph-osd-1-55855bf495-dlfpf 1/1 Running 0 6m15s\nrook-ceph-osd-2-776c77657c-sgf5n 1/1 Running 0 6m12s\nrook-ceph-osd-3-97548cc45-4xm4q 1/1 Running 0 5m58s\nrook-ceph-osd-prepare-ip-10-0-138-84-gc26q 0/2 Completed 0 6m29s\nrook-ceph-osd-prepare-ip-10-0-141-184-9bmdt 0/2 Completed 0 6m29s\nrook-ceph-osd-prepare-ip-10-0-149-16-nh4tm 0/2 Completed 0 6m29s\nrook-ceph-osd-prepare-ip-10-0-173-174-mzzhq 0/2 Completed 0 6m28s\nrook-ceph-rgw-my-store-d6946dcf-q8k69 1/1 Running 0 5m33s\nrook-ceph-tools-cb5655595-4g4b2 1/1 Running 0 8m46s\n</code></pre>\n</li>\n<li>\n<p>Next, you will need to create a set of S3 credentials, the resulting credentials will be stored in a secrets file under the <code>rook-ceph</code> namespace. There isn’t currently a way to cross-share secrets between OpenShift namespaces, so you will need to copy the secret to whichever namespace is running the applications you want to interact with the object store. You can also copy the AccessKey and SecretKey from the second command.</p>\n<pre><code class=\"language-bash\">oc create -f object-user.yaml\noc get secrets -n rook-ceph rook-ceph-object-user-my-store-odh-user -o json\n</code></pre>\n</li>\n<li>From the OpenShift console, create a route to the rook service, <code>rook-ceph-rgw-my-store</code>, in the <code>rook-ceph</code> namespace to expose the endpoint. This endpoint url will be used to access the S3 interface from the example notebooks.</li>\n</ol>\n<p>{% include next-link.html label=\"GPU Enablement\" url=\"/docs/administration/advanced-installation/gpu.html\" %}</p>"},{"frontmatter":{"permalink":"/docs/administration/installation-customization/customization","title":"Customizing the Installation"},"html":"<p>This document explains the preferred way of customizing Open Data Hub and Kubeflow deployments.</p>\n<h2>How to Customize the Installation</h2>\n<p>There are essentially three ways to customize the deployment. All of them will involve working with git and <a href=\"https://github.com/opendatahub-io/odh-manifests\">odh-manifests</a> repository.</p>\n<ul>\n<li>Fork odh-manifests and maintain changes there</li>\n<li>Create your own manifests repository to overwrite the component resources</li>\n<li>Fork odh-manifests repository and extend it with overlays for components</li>\n</ul>\n<p>Each of the above requires you to have a git repository with some Kustomize and OpenShift resources which you will reference from the KFDef resource, but each of them requires different level of effort to create and maintain.</p>\n<p>We will only discuss the last one (overlays) here, since that is the one we promote and recommend for maintaining the customizations. If you would like to learn more about the other approaches, you can read <a href=\"https://developers.redhat.com/blog/2020/07/23/open-data-hub-and-kubeflow-installation-customization/\">the blog post</a> we published on this topic.</p>\n<h3>Customizing a Deployment Using Overlays</h3>\n<p>Overlays provide a great way to offer an optional customization to a component. It allows you to modify or delete/exclude existing resources or add new ones. You can simply enable and disable overlays in KFDef custom resource by adding to or removing them from overlays list at the component level. </p>\n<p>The way overlays generally work in the context of Open Data Hub manifests is that they add another layer on top of the <code>base</code> resources for the component. This layer can leverage any of the Kustomize functionality to extend the <code>base</code>. We provide many overlays across the odh-manifests repository. Some of them add resources (e.g. <a href=\"https://github.com/opendatahub-io/odh-manifests/tree/master/jupyterhub/jupyterhub/overlays/build\">build configurations</a>), some of them modify resources to customize the deployment for a specific environment (e.g. <a href=\"https://github.com/opendatahub-io/odh-manifests/tree/master/jupyterhub/jupyterhub/overlays/storage-class\">parameterizing a storage class</a>).</p>\n<p>Overlays add a bit of complexity and it takes a bit of learning to become productive when using them, but it is well worth due to the flexibility and long term maintainablity you gain by leveraging them for your customizations.</p>\n<h4>Example</h4>\n<p>The example for the overlays approach can be found in <a href=\"https://github.com/vpavlin/odh-manifests-overlays\">odh-manifests-overlays</a> repository. At a first sight it looks like a copy of odh-manifests repository - it also shares its Git history.</p>\n<p>It has one commit in addition which adds an <a href=\"https://github.com/vpavlin/odh-manifests-overlays/commit/HEAD\">overlay to JupyterHub component</a>. This overlay modefies a JupyterHub ConfigMap and adds an additional JupyterHub Singleuser Profiles ConfigMap.</p>\n<p>To test this out, you will need <a href=\"%7B%7Bsite.baseurl%7D%7D/docs/getting-started/quick-installation.html\">Open Data Hub operator deployed</a>. Then you can simply upload the <a href=\"\">customized KFDef resource</a> to the cluster.</p>\n<pre><code>oc apply -f https://raw.githubusercontent.com/vpavlin/odh-manifests-overlays/master/kfdef/kfctl_openshift_custom.yaml\n</code></pre>\n<p>You should see the customized ConfigMap appearing in the cluster in a bit.</p>\n<pre><code>oc describe cm jupyterhub-cfg\n</code></pre>\n<p>You can also view the added ConfigMap by running the following command.</p>\n<pre><code>oc describe cm jupyterhub-additional-singleuser-profiles\n</code></pre>\n<p>{% include next-link.html label=\"Advanced Installation\" url=\"/docs/administration/advanced-installation/optional.html\" %}</p>"},{"frontmatter":{"permalink":"/docs/administration/advanced-installation/optional","title":"Pre-requisites for Optional Components"},"html":"<p>Before installing Argo, Seldon, Kafka, or AI Library, there are pre-requisites that must be installed before the Open Data Hub operator.  You must install several <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#customresourcedefinitions\">Custom Resource Definitions (CRDs)</a> either through command line, or through the console.</p>\n<h3>CLI Installation</h3>\n<ol>\n<li>\n<p>From the command line, use the <code>oc</code> command line tool to log in as a user with <code>cluster-admin</code> privileges.  For a developer installation from <a href=\"https://try.openshift.com/\">try.openshift.com</a> including AWS and CRC, the <code>kubeadmin</code> user will work.</p>\n<pre><code class=\"language-bash\">$ oc login https://api.crc.testing:6443 -u kubeadmin -p ***********\nLogin successful.\n\nYou have access to 53 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n</code></pre>\n</li>\n<li>\n<p>To prepare to install <strong>Argo</strong>, install the <a href=\"https://gitlab.com/opendatahub/opendatahub-operator/raw/v0.5.1/deploy/crds/argo-crd.yaml\">workflow CRD</a>. </p>\n<pre><code class=\"language-bash\">$ oc apply -f https://gitlab.com/opendatahub/opendatahub-operator/raw/v0.5.1/deploy/crds/argo-crd.yaml\ncustomresourcedefinition.apiextensions.k8s.io/workflows.argoproj.io created\n</code></pre>\n</li>\n<li>\n<p>To prepare to install <strong>Seldon</strong> and [AI Library]({{ '/docs/ai-library/installation.html' | prepend: site.baseurl }}), install the <a href=\"https://gitlab.com/opendatahub/opendatahub-operator/raw/v0.5.1/deploy/crds/seldon-deployment-crd.json\">SeldonDeployment CRD</a></p>\n<pre><code class=\"language-bash\">$ oc apply -f https://gitlab.com/opendatahub/opendatahub-operator/raw/v0.5.1/deploy/crds/seldon-deployment-crd.json\ncustomresourcedefinition.apiextensions.k8s.io/seldondeployments.machinelearning.seldon.io created\n</code></pre>\n</li>\n<li>In preparation to deploy a <strong>Kafka</strong> cluster, install <a href=\"https://strimzi.io/\">Strimzi</a>.</li>\n</ol>\n<h3>Web Console Installation</h3>\n<ol>\n<li>From the OpenShift console, log in as a user with <code>cluster-admin</code> privileges.  For a developer installation from <a href=\"https://try.openshift.com/\">try.openshift.com</a> including AWS and CRC, the <code>kubeadmin</code> user will work.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/quick-installation/login.png\" alt=\"Log in to OpenShift\" title=\"Log in to OpenShift\"></li>\n<li>To install CRDs, find the list of CRDs under <code>Administration</code> -> <code>Custom Resource Definitions</code> and click <code>Create Custom Resource Definitions</code>\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/adv-install/admin-crd.png\" alt=\"Admin CRDs\" title=\"Admin CRDs\"></li>\n<li>To prepare to install <strong>Argo</strong>, install the workflow CRD.  Click <code>Create Custom Resource Definitions</code> copy the <a href=\"https://gitlab.com/opendatahub/opendatahub-operator/blob/v0.5.1/deploy/crds/argo-crd.yaml\">argo-crd.yaml</a> into the editing window, and click <code>Create</code>.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/adv-install/create-argo-crd.png\" alt=\"Argo CRD\" title=\"Argo CRD\"></li>\n<li>To prepare to install <strong>Seldon</strong> and [AI Library]({{ '/docs/ai-library/installation.html' | prepend: site.baseurl }}), install the SeldonDeployment CRD.  Click <code>Create Custom Resource Definitions</code> copy the <a href=\"https://gitlab.com/opendatahub/opendatahub-operator/blob/v0.5.1/deploy/crds/seldon-deployment-crd.json\">seldon-deployment-crd.json</a> into the editing window, and click <code>Create</code>.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/adv-install/create-seldon-crd.png\" alt=\"Seldon CRD\" title=\"Seldon CRD\"></li>\n<li>In preparation to deploy a <strong>Kafka</strong> cluster, install <a href=\"https://strimzi.io/\">Strimzi</a>.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/adv-install/install-strimzi.png\" alt=\"Strimzi Install\" title=\"Strimzi Install\"></li>\n</ol>"},{"frontmatter":{"permalink":"/docs/getting-started/legacy/basic-tutorial","title":"Basic Tutorial"},"html":"<h3>Note</h3>\n<p>This tutorial is based on old - no longer developed - version of Open Data Hub. You can find the new version of the document <a href=\"%7B%7Bsite.baseurl%7D%7D/docs/getting-started/basic-tutorial\">here</a>.</p>\n<hr>\n<h3>Pre-requisites</h3>\n<p>This Tutorial requires a basic installation of Open Data Hub with Spark and JupyterHub as detailed in the <a href=\"%7B%7Bsite.baseurl%7D%7D/docs/getting-started/quick-installation\">quick installation</a>. The quick installation steps are also available as a <a class=\"external-link\" href=\"https://www.youtube.com/watch?v=-T6ypF7LoKk&t=2s\" target=\"_blank\"><i class=\"fas fa-external-link-alt\"></i>tutorial video</a> on the OpenShift youtube channel.</p>\n<p>All screenshots and instructions are from OpenShift 4.2.  For the purposes of this tutorial, we used <a href=\"https://try.openshift.com/\">try.openshift.com</a> on AWS.  Tutorials have also been tested on <a href=\"https://code-ready.github.io/crc/\">Code Ready Containers</a> with 16GB of RAM.</p>\n<p>The <a href=\"%7B%7Bsite.repo%7D%7D/opendatahub.io/blob/master/assets/files/tutorials/basic/basic_tutorial_notebook.ipynb\">source</a> for the following notebook is available in GitLab with comments for easy viewing.</p>\n<h3>Exploring JupyterHub and Spark</h3>\n<p>JupyterHub and Spark are installed by default with Open Data Hub.  You can create Jupyter Notebooks and connect to Spark.  This is a simple <code>hello world</code>.</p>\n<ol>\n<li>Find the route to JupyterHub.  Within your Open Data Hub Project click on Networking -> Routes\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/basic-tutorial-legavy/routes.png\" alt=\"Routes\" title=\"Routes\"></li>\n<li>For the route named <code>jupyterhub</code>, click on the location to bring up JupyterHub (typically <code>https://jupyterhub-project.apps.your-cluster.your-domain.com</code>).</li>\n<li>Sign in using your OpenShift credentials.</li>\n<li>Spawn a new server with spark functionality.  (e.g. <code>s2i-spark-minimal-notebook:3.6</code>)\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/basic-tutorial-legacy/new-notebook.png\" alt=\"New Notebook\" title=\"New Notebook\"></li>\n<li>Create a new Python 3 notebook</li>\n<li>\n<p>Copy the following code to test a basic spark connection.</p>\n<pre><code class=\"language-python\">from pyspark.sql import SparkSession, SQLContext\nimport os\nimport socket\n\n# Add the necessary Hadoop and AWS jars to access Ceph from Spark\n# Can be omitted if s3 storage access is not required\nos.environ['PYSPARK_SUBMIT_ARGS'] = f\"--conf spark.jars.ivy={os.environ['HOME']} --packages org.apache.hadoop:hadoop-aws:2.7.3,com.amazonaws:aws-java-sdk:1.7.4 pyspark-shell\"\n\n# create a spark session\nspark_cluster_url = f\"spark://{os.environ['SPARK_CLUSTER']}:7077\"\nspark = SparkSession.builder.master(spark_cluster_url).getOrCreate()\n\n# test your spark connection\nspark.range(5, numPartitions=5).rdd.map(lambda x: socket.gethostname()).distinct().collect()\n</code></pre>\n</li>\n<li>\n<p>Run the notebook.  If successful, you should see the output similar to the following:</p>\n<pre><code>['jupyterhub-nb-kube-3aadmin']\n</code></pre>\n</li>\n</ol>\n<h3>Object Storage</h3>\n<p>Let's add on to the notebook from the previous section and access data on an Object Store (such as Ceph or AWS S3) using the S3 API.  For instructions on installing Ceph, refer to the Advanced Installation <a href=\"%7B%7Bsite.baseurl%7D%7D/docs/administration/advanced-installation/object-storage.html\">documentation</a>.</p>\n<ol>\n<li>Click on the <code>+</code> button and insert a new cell below of type <code>Code</code>.</li>\n<li>\n<p>To access S3 directly, we'll use the boto3 library.  We'll download a sample data file and then upload it to our S3 storage.  In the new cell paste the following code, and edit the <code>s3_</code> variables with your own credentials.</p>\n<pre><code class=\"language-python\"># Edit this section using your own credentials\ns3_region = 'region-1' # fill in for AWS, blank for Ceph\ns3_endpoint_url = 'https://s3.storage.server'\ns3_access_key_id = 'AccessKeyId-ChangeMe'\ns3_secret_access_key = 'SecretAccessKey-ChangeMe'\ns3_bucket = 'MyBucket'\n\n# for easy download\n!pip install wget\n\nimport wget\nimport boto3\n\n# configure boto S3 connection\ns3 = boto3.client('s3',\n                  s3_region,\n                  endpoint_url = s3_endpoint_url,\n                  aws_access_key_id = s3_access_key_id,\n                  aws_secret_access_key = s3_secret_access_key)\n\n# download the sample data file\nurl = \"{{site.repo}}/opendatahub.io/raw/master/assets/files/tutorials/basic/sample_data.csv\"\nfile = wget.download(url=url, out='sample_data.csv')\n\n#upload the file to storage\ns3.upload_file(file, s3_bucket, \"sample_data.csv\")\n</code></pre>\n</li>\n<li>Run the cell.  After it completes check your S3 bucket.  You should see the <code>sample_data.csv</code>.</li>\n</ol>\n<h3>Spark + Object Storage</h3>\n<p>Now, let's access that same data file from Spark so you can analyze data.</p>\n<ol>\n<li>Now let's read the data from Spark.  First, click on the <code>+</code> button and insert a new cell of type <code>Code</code>.</li>\n<li>\n<p>Paste the following code to read the data from spark and print some data.</p>\n<pre><code class=\"language-python\">hadoopConf = spark.sparkContext._jsc.hadoopConfiguration()\nhadoopConf.set(\"fs.s3a.endpoint\", s3_endpoint_url)\nhadoopConf.set(\"fs.s3a.access.key\", s3_access_key_id)\nhadoopConf.set(\"fs.s3a.secret.key\", s3_secret_access_key)\nhadoopConf.set(\"fs.s3a.path.style.access\", \"true\")\nhadoopConf.set(\"fs.s3a.connection.ssl.enabled\", \"true\") # false if not https\n\ndata = spark.read.csv('s3a://' + s3_bucket + '/sample_data.csv',sep=\",\", header=True)\ndf = data.toPandas()\ndf.head()\n</code></pre>\n</li>\n<li>Run the cell.  The data from the <code>csv</code> file should be displayed as a Pandas data frame.</li>\n</ol>\n<p>That's it!  You have a working Jupyter notebook with access to storage and Spark.</p>\n<p>{% include next-link.html label=\"Advanced Tutorials\" url=\"/docs/advanced-tutorials/data-exploration.html\" %}</p>"},{"frontmatter":{"permalink":"/docs/getting-started/basic-tutorial","title":"Basic Tutorial"},"html":"<h3>Pre-requisites</h3>\n<p>This Tutorial requires a basic installation of Open Data Hub with Spark and JupyterHub as detailed in the <a href=\"%7B%7Bsite.baseurl%7D%7D/docs/getting-started/quick-installation\">quick installation</a>. The quick installation steps are also available as a <a class=\"external-link\" href=\"https://www.youtube.com/watch?v=-T6ypF7LoKk&t=2s\" target=\"_blank\"><i class=\"fas fa-external-link-alt\"></i>tutorial video</a> on the OpenShift youtube channel.</p>\n<p>All screenshots and instructions are from OpenShift 4.4.  For the purposes of this tutorial, we used <a href=\"https://try.openshift.com/\">try.openshift.com</a> on AWS.  Tutorials have also been tested on <a href=\"https://code-ready.github.io/crc/\">Code Ready Containers</a> with 16GB of RAM.</p>\n<p>The <a href=\"%7B%7Bsite.repo%7D%7D/opendatahub.io/blob/master/assets/files/tutorials/basic/basic_tutorial_notebook.ipynb\">source</a> for the following notebook is available in GitLab with comments for easy viewing.</p>\n<h3>Exploring JupyterHub and Spark</h3>\n<p>JupyterHub and Spark are installed by default with Open Data Hub.  You can create Jupyter Notebooks and connect to Spark.  This is a simple <code>hello world</code> example.</p>\n<ol>\n<li>Find the route to JupyterHub.  Within your Open Data Hub Project click on <code>Networking</code> -> <code>Routes</code>\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/basic-tutorial/routes.png\" alt=\"Routes\" title=\"Routes\"></li>\n<li>For the route named <code>jupyterhub</code>, click on the location to bring up JupyterHub (typically <code>https://jupyterhub-project.apps.your-cluster.your-domain.com</code>).</li>\n<li>Sign in using your OpenShift credentials.</li>\n<li>Spawn a new server with spark functionality.  (e.g. <code>s2i-spark-minimal-notebook:py36-spark2.4.5-hadoop2.7.3</code>)\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/basic-tutorial/new-notebook.png\" alt=\"New Notebook\" title=\"New Notebook\"></li>\n<li>Create a new Python 3 notebook</li>\n<li>\n<p>Copy the following code to test a basic spark connection.</p>\n<pre><code class=\"language-python\">from pyspark.sql import SparkSession, SQLContext\nimport os\nimport socket\n\n# create a spark session\nspark_cluster_url = f\"spark://{os.environ['SPARK_CLUSTER']}:7077\"\nspark = SparkSession.builder.master(spark_cluster_url).getOrCreate()\n\n# test your spark connection\nspark.range(5, numPartitions=5).rdd.map(lambda x: socket.gethostname()).distinct().collect()\n</code></pre>\n</li>\n<li>\n<p>Run the notebook.  If successful, you should see the output similar to the following:</p>\n<pre><code>['spark-cluster-kube-3aadmin-w-gx7rm', 'spark-cluster-kube-3aadmin-w-xvl55']\n</code></pre>\n</li>\n</ol>\n<h3>Object Storage</h3>\n<p>Let's add on to the notebook from the previous section and access data on an Object Store (such as Ceph or AWS S3) using the S3 API.  For instructions on installing Ceph, refer to the Advanced Installation <a href=\"%7B%7Bsite.baseurl%7D%7D/docs/administration/advanced-installation/object-storage.html\">documentation</a>.</p>\n<ol>\n<li>Click on the <code>+</code> button and insert a new cell of type <code>Code</code>.</li>\n<li>\n<p>To access S3 directly, we'll use the <code>boto3</code> library.  We'll download a sample data file and then upload it to our S3 storage.  In the new cell paste the following code, and edit the <code>s3_</code> variables with your own credentials.</p>\n<pre><code class=\"language-python\"># Edit this section using your own credentials\ns3_region = 'region-1' # fill in for AWS, blank for Ceph\ns3_endpoint_url = 'https://s3.storage.server'\ns3_access_key_id = 'AccessKeyId-ChangeMe'\ns3_secret_access_key = 'SecretAccessKey-ChangeMe'\ns3_bucket = 'MyBucket'\n\n# for easy download\n!pip install wget\n\nimport wget\nimport boto3\n\n# configure boto S3 connection\ns3 = boto3.client('s3',\n                  s3_region,\n                  endpoint_url = s3_endpoint_url,\n                  aws_access_key_id = s3_access_key_id,\n                  aws_secret_access_key = s3_secret_access_key)\n\n# download the sample data file\nurl = \"{{site.repo}}/opendatahub.io/raw/master/assets/files/tutorials/basic/sample_data.csv\"\nfile = wget.download(url=url, out='sample_data.csv')\n\n#upload the file to storage\ns3.upload_file(file, s3_bucket, \"sample_data.csv\")\n</code></pre>\n</li>\n<li>Run the cell.  After it completes check your S3 bucket.  You should see the <code>sample_data.csv</code>.</li>\n</ol>\n<h3>Spark + Object Storage</h3>\n<p>Now, let's access that same data file from Spark so you can analyze the data.</p>\n<ol>\n<li>Let's read the data using Spark. First, click on the <code>+</code> button and insert a new cell of type <code>Code</code>.</li>\n<li>\n<p>Paste the following code to read the data using Spark and print the first few rows of data.</p>\n<pre><code class=\"language-python\">hadoopConf = spark.sparkContext._jsc.hadoopConfiguration()\nhadoopConf.set(\"fs.s3a.endpoint\", s3_endpoint_url)\nhadoopConf.set(\"fs.s3a.access.key\", s3_access_key_id)\nhadoopConf.set(\"fs.s3a.secret.key\", s3_secret_access_key)\nhadoopConf.set(\"fs.s3a.path.style.access\", \"true\")\nhadoopConf.set(\"fs.s3a.connection.ssl.enabled\", \"true\") # false if not https\n\ndata = spark.read.csv('s3a://' + s3_bucket + '/sample_data.csv',sep=\",\", header=True)\ndf = data.toPandas()\ndf.head()\n</code></pre>\n</li>\n<li>Run the cell. The data from the <code>csv</code> file should be displayed as a Pandas data frame.</li>\n</ol>\n<p>That's it!  You have a working Jupyter notebook workspace with access to S3 storage and Spark.</p>\n<p>{% include next-link.html label=\"Advanced Tutorials\" url=\"/docs/advanced-tutorials/data-exploration.html\" %}</p>"},{"frontmatter":{"permalink":"/docs/getting-started/legacy/quick-installation","title":"Quick Installation"},"html":"<h3>Note</h3>\n<p>This tutorial is based on old - no longer developed - version of Open Data Hub. You can find the new version of the document <a href=\"%7B%7Bsite.baseurl%7D%7D/docs/getting-started/quick-installation\">here</a>.</p>\n<hr>\n<p>The steps are also available in a <a class=\"external-link\" href=\"https://www.youtube.com/watch?v=-T6ypF7LoKk&t=2s\" target=\"_blank\"><i class=\"fas fa-external-link-alt\"></i>tutorial video</a> available on the OpenShift youtube channel.</p>\n<h3>Pre-requisites</h3>\n<p>Installing ODH requires OpenShift 3.11 or 4.x. Documentation for OpenShift can be located (<a href=\"https://docs.openshift.com/container-platform/4.1/welcome/index.html\">here</a>).  All screenshots and instructions are from OpenShift 4.2.  For the purposes of this quick start, we used <a href=\"https://try.openshift.com/\">try.openshift.com</a> on AWS.  Tutorials have also been tested on <a href=\"https://code-ready.github.io/crc/\">Code Ready Containers</a> with 16GB of RAM.</p>\n<p>We will not be installing optional components such as Argo, Seldon, AI Library, and Kafka.  For these components, there are additional pre-requisites detailed in the [advanced installation]({{ '/docs/administration/advanced-installation/optional.html' | prepend: site.baseurl }}).  These additional pre-requisites must be installed before the Open Data Hub Operator if you intend to install these optional components.</p>\n<h3>Installing the Open Data Hub Operator</h3>\n<p>The Open Data Hub operator is available in the OpenShift 4.x Community Operators section. You can install it from the OpenShift webui by following the steps below:</p>\n<ol>\n<li>From the OpenShift console, log in as a user with <code>cluster-admin</code> privileges.  For a developer installation from <a href=\"https://try.openshift.com/\">try.openshift.com</a> including AWS and CRC, the <code>kubeadmin</code> user will work.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/quick-installation-legacy/login.png\" alt=\"Log in to OpenShift\" title=\"Log in to OpenShift\"></li>\n<li>Create a new namespace for your installation of Open Data Hub.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/quick-installation-legacy/create-namespace.png\" alt=\"Create Namespace\" title=\"Create Namespace\"></li>\n<li>\n<p>Find <code>Open Data Hub</code> in the <code>OperatorHub</code> catalog.</p>\n<ol>\n<li>Select the new namespace if not already selected.</li>\n<li>Under <code>Operators</code>, select <code>OperatorHub</code> for a list of community operators.</li>\n<li>Filter for <code>Open Data Hub</code> or look under <code>Big Data</code> for the icon for <code>Open Data Hub</code>.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/quick-installation-legacy/operator-hub.png\" alt=\"OperatorHub\" title=\"OperatorHub\"></li>\n</ol>\n</li>\n<li>Click the <code>Install</code> button and follow the installation instructions to install the Open Data Hub operator.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/quick-installation-legacy/install.png\" alt=\"Install\" title=\"Install\"></li>\n<li>To view the status of the Open Data Hub operator installation, find the Open Data Hub Operator under <code>Operators</code> -> <code>Installed Operators</code> (inside the namespace you created earlier). Once the STATUS field displays <code>InstallSucceeded</code>, you can proceed to create a new Open Data Hub deployment.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/quick-installation-legacy/installed-operators.png\" alt=\"Installed Operators\" title=\"Installed Operators\"></li>\n</ol>\n<h3>Create a New Open Data Hub Deployment</h3>\n<p>The Open Data Hub operator will create new Open Data Hub deployments and manage its components.  Let's create a new Open Data Hub deployment.</p>\n<ol>\n<li>Find the Open Data Hub Operator under <code>Installed Operators</code> (inside the namespace you created earlier)\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/quick-installation-legacy/installed-operators.png\" alt=\"Installed Operators\" title=\"Installed Operators\"></li>\n<li>Click on the Open Data Hub Operator to bring up the detail.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/quick-installation-legacy/odh-operator.png\" alt=\"Open Data Hub Operator\" title=\"Open Data Hub Operator\"></li>\n<li>Click <code>Create Instance</code> to create a new deployment.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/quick-installation-legacy/new-deployment.png\" alt=\"Create New ODH\" title=\"Create New ODH\"></li>\n<li>\n<p>Here you'll be presented with a YAML file to customize your deployment.  Most options are disabled, and for this tutorial we'll leave them that way and modify some of the parameters to make sure the components for JupyterHub and Spark fit within our cluster resource constraints.  Take note of some parameters:</p>\n<ul>\n<li>\n<p>the name of your deployment <code>example-opendatahub</code></p>\n<pre><code class=\"language-yaml\">metadata:\nname: example-opendatahub\n</code></pre>\n</li>\n<li>\n<p>the deployed components designated by <code>odh_deploy</code>:</p>\n<pre><code class=\"language-yaml\">spec:\naicoe-jupyterhub:\nodh_deploy: true\n# Set the Jupyter notebook pod to 1CPU and 2Gi of memory\nnotebook_cpu: 1\nnotebook_memory: 1Gi\nspark:\n  image: \"quay.io/opendatahub/spark-cluster-image:spark22python36\"\n  master:\n    instances: 1\n    # Reduce the master node to 1CPU and 1GB \n    resources:\n      limits:\n        memory: 1Gi\n        cpu: 1\n      requests:\n        memory: 512Mi\n        cpu: 500m\n  worker:\n    # Disable creation of the spark worker node in the cluster\n    instances: 0\n    resources:\n      limits:\n        memory: 1Gi\n        cpu: 1\n      requests:\n        memory: 256Mi\n        cpu: 200m\nspark-operator:\nodh_deploy: true\n# Reduce the memory requirements\nmonitoring:\nodh_deploy: false\n</code></pre>\n</li>\n</ul>\n</li>\n<li>Leave the YAML intact and click <code>Create</code>.  If you accepted the default name, this will trigger an Open Data Hub deployment named <code>example-opendatahub</code> with JupyterHub and Spark.</li>\n<li>Verify the installation by viewing the Open Data Hub tab within the operator details.  You Should see <code>example-opendatahub</code> listed.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/quick-installation-legacy/odh-list.png\" alt=\"ODH List\" title=\"ODH List\"></li>\n<li>Verify the installation by viewing the project workload.  JupyterHub, Spark, and Prometheus should all be running.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/quick-installation-legacy/verify-install.png\" alt=\"Verify Status\" title=\"Verify Status\"></li>\n</ol>\n<p>{% include next-link.html label=\"Basic Tutorial\" url=\"/docs/getting-started/basic-tutorial.html\" %}</p>"},{"frontmatter":{"permalink":"/docs/kubeflow/components/jupyter","title":"Jupyter Notebooks"},"html":"<p>{% include next-link.html label=\"Hyperparamater Tuning\" url=\"/docs/kubeflow/components/hyperparameter.html\" %}</p>"},{"frontmatter":{"permalink":"/docs/getting-started/quick-installation","title":"Quick Installation"},"html":"<p>The steps are also available in a <a class=\"external-link\" href=\"https://www.youtube.com/watch?v=-T6ypF7LoKk&t=2s\" target=\"_blank\"><i class=\"fas fa-external-link-alt\"></i>tutorial video</a> available on the OpenShift youtube channel.</p>\n<h3>Pre-requisites</h3>\n<p>Installing ODH Beta requires 4.x. Documentation for OpenShift can be located (<a href=\"https://docs.openshift.com/container-platform/4.4/welcome/index.html\">here</a>). All screenshots and instructions are from OpenShift 4.4.  For the purposes of this quick start, we used <a href=\"https://try.openshift.com/\">try.openshift.com</a> on AWS.  Tutorials have also been tested on <a href=\"https://code-ready.github.io/crc/\">Code Ready Containers</a> with 16GB of RAM.</p>\n<p>We will not be installing optional components such as Argo, Seldon, AI Library, or Kafka to avoid using too much resources in case your cluster is small.</p>\n<h3>Installing the Open Data Hub Operator</h3>\n<p>The Open Data Hub operator is available in the OpenShift 4.x Community Operators section. You can install it from the OpenShift webui by following the steps below:</p>\n<ol>\n<li>From the OpenShift console, log in as a user with <code>cluster-admin</code> privileges.  For a developer installation from <a href=\"https://try.openshift.com/\">try.openshift.com</a> including AWS and CRC, the <code>kubeadmin</code> user will work.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/quick-installation/login.png\" alt=\"Log in to OpenShift\" title=\"Log in to OpenShift\"></li>\n<li>Create a new namespace for your installation of Open Data Hub.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/quick-installation/create-namespace.png\" alt=\"Create Namespace\" title=\"Create Namespace\"></li>\n<li>\n<p>Find <code>Open Data Hub</code> in the <code>OperatorHub</code> catalog.</p>\n<ol>\n<li>Select the new namespace if not already selected.</li>\n<li>Under <code>Operators</code>, select <code>OperatorHub</code> for a list of community operators.</li>\n<li>Filter for <code>Open Data Hub</code> or look under <code>Big Data</code> for the icon for <code>Open Data Hub</code>.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/quick-installation/operator-hub.png\" alt=\"OperatorHub\" title=\"OperatorHub\"></li>\n</ol>\n</li>\n<li>Click the <code>Install</code> button and follow the installation instructions to install the Open Data Hub operator.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/quick-installation/install.png\" alt=\"Install\" title=\"Install\"></li>\n<li>The subscription creation view will offer a few options including <em>Update Channel</em>, keep the <code>Beta</code> channel selected.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/quick-installation/channels.png\" alt=\"Select Channel\" title=\"Install\"></li>\n<li>To view the status of the Open Data Hub operator installation, find the Open Data Hub Operator under <code>Operators</code> -> <code>Installed Operators</code> (inside the namespace you created earlier). Once the STATUS field displays <code>InstallSucceeded</code>, you can proceed to create a new Open Data Hub deployment.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/quick-installation/installed-operators.png\" alt=\"Installed Operators\" title=\"Installed Operators\"></li>\n</ol>\n<h3>Create a New Open Data Hub Deployment</h3>\n<p>The Open Data Hub operator will create new Open Data Hub deployments and manage its components.  Let's create a new Open Data Hub deployment.</p>\n<ol>\n<li>Find the Open Data Hub Operator under <code>Installed Operators</code> (inside the namespace you created earlier)\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/quick-installation/installed-operators.png\" alt=\"Installed Operators\" title=\"Installed Operators\"></li>\n<li>Click on the Open Data Hub Operator to bring up the detail.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/quick-installation/odh-operator.png\" alt=\"Open Data Hub Operator\" title=\"Open Data Hub Operator\"></li>\n<li>Click <code>Create Instance</code> to create a new deployment.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/quick-installation/new-deployment.png\" alt=\"Create New ODH\" title=\"Create New ODH\"></li>\n<li>\n<p>Here you'll be presented with a YAML file to customize your deployment.  Most options are disabled, and for this tutorial we'll leave them that way to make sure the components for JupyterHub and Spark fit within our cluster resource constraints. Take note of some parameters:</p>\n<ul>\n<li>\n<p>the name of your deployment <code>opendatahub</code></p>\n<pre><code class=\"language-yaml\">metadata:\n  name: opendatahub\n</code></pre>\n</li>\n<li>\n<p>only the components listed in the <code>KFDef</code> resource will be deployed:</p>\n<pre><code class=\"language-yaml\">spec:\napplications:\n  - kustomizeConfig:\n      repoRef:\n        name: manifests\n        path: odh-common\n    name: odh-common\n  - kustomizeConfig:\n      repoRef:\n        name: manifests\n        path: radanalyticsio/spark/cluster\n    name: radanalyticsio-cluster\n  - kustomizeConfig:\n      repoRef:\n        name: manifests\n        path: radanalyticsio/spark/operator\n    name: radanalyticsio-spark-operator\n  - kustomizeConfig:\n      parameters:\n        - name: s3_endpoint_url\n          value: s3.odh.com\n      repoRef:\n        name: manifests\n        path: jupyterhub/jupyterhub\n    name: jupyterhub\n  - kustomizeConfig:\n      overlays:\n        - additional\n      repoRef:\n        name: manifests\n        path: jupyterhub/notebook-images\n    name: notebook-images\n</code></pre>\n</li>\n</ul>\n</li>\n<li>Update the <code>spec</code> of the resource to match the above and click <code>Create</code>.  If you accepted the default name, this will trigger an Open Data Hub deployment named <code>opendatahub</code> with JupyterHub and Spark.</li>\n<li>Verify the installation by viewing the Open Data Hub tab within the operator details.  You Should see <code>opendatahub</code> listed.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/quick-installation/odh-list.png\" alt=\"ODH List\" title=\"ODH List\"></li>\n<li>Verify the installation by viewing the project workload.  JupyterHub and Spark Operator should be running.\n<img src=\"%7B%7Bsite.baseurl%7D%7D/assets/img/pages/docs/quick-installation/verify-install.png\" alt=\"Verify Status\" title=\"Verify Status\"></li>\n</ol>\n<p>{% include next-link.html label=\"Basic Tutorial\" url=\"/docs/getting-started/basic-tutorial.html\" %}</p>"},{"frontmatter":{"permalink":"/docs/kubeflow/components/serving","title":"Serving"},"html":"<p>{% include next-link.html label=\"Training\" url=\"/docs/kubeflow/components/training.html\" %}</p>"},{"frontmatter":{"permalink":"/docs/kubeflow/components/pipelines","title":"Pipelines"},"html":"<p>{% include next-link.html label=\"Serving\" url=\"/docs/kubeflow/components/serving.html\" %}</p>"},{"frontmatter":{"permalink":"/docs/kubeflow/components/training","title":"Training"},"html":"<p>{% include next-link.html label=\"Miscellaneous\" url=\"/docs/kubeflow/components/misc.html\" %}</p>"},{"frontmatter":{"permalink":"/docs/kubeflow/components/misc","title":"Miscellaneous"},"html":"<p>{% include next-link.html label=\"AI Library\" url=\"/docs/ai-library.html\" %}</p>"},{"frontmatter":{"permalink":"/docs/kubeflow/components/hyperparameter","title":"Hyperparameter Tuning"},"html":"<p>{% include next-link.html label=\"Pipelines\" url=\"/docs/kubeflow/components/pipelines.html\" %}</p>"}]}}}