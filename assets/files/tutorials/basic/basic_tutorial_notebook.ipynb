{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring JupyterHub and Spark\n",
    "\n",
    "JupyterHub and Spark are installed by default with Open Data Hub.  You can create Jupyter Notebooks and connect to Spark.\n",
    "\n",
    "Running the next cell should connect to Spark and the output:\n",
    "```\n",
    "['jupyterhub-nb-kube-3aadmin']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import os\n",
    "import socket\n",
    "    \n",
    "# Add the necessary Hadoop and AWS jars to access Ceph from Spark\n",
    "# Can be omitted if s3 storage access is not required\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f\"--conf spark.jars.ivy={os.environ['HOME']} --packages org.apache.hadoop:hadoop-aws:2.7.3,com.amazonaws:aws-java-sdk:1.7.4 pyspark-shell\"\n",
    "\n",
    "# create a spark session\n",
    "spark_cluster_url = f\"spark://{os.environ['SPARK_CLUSTER']}:7077\"\n",
    "spark = SparkSession.builder.master(spark_cluster_url).getOrCreate()\n",
    "    \n",
    "# test your spark connection\n",
    "spark.range(5, numPartitions=5).rdd.map(lambda x: socket.gethostname()).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Storage\n",
    "\n",
    "Let's access data on an Object Store (such as Ceph or AWS S3) using the S3 API.  For instructions on installing Ceph,refer to the Advanced Installation [documentation](https://opendatahub.io/docs/administration/advanced-installation/object-storage.html).\n",
    "\n",
    "To access S3 directly, we'll use the boto3 library.  We'll download a sample data file with `wget` and then upload itto our S3 storage using `boto3`.\n",
    "\n",
    "After running the following cell, you should see the `sample_data.csv` available in your S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit this section using your own credentials\n",
    "s3_region = 'region-1' # AWS region or blank for Ceph\n",
    "s3_endpoint_url = 'https://s3.storage.server'\n",
    "s3_access_key_id = 'AccessKeyId-ChangeMe'\n",
    "s3_secret_access_key = 'SecretAccessKey-ChangeMe'\n",
    "s3_bucket = 'my-bucket'\n",
    "\n",
    "# for easy download\n",
    "!pip install wget\n",
    "    \n",
    "import wget\n",
    "import boto3\n",
    "\n",
    "# configure boto S3 connection\n",
    "s3 = boto3.client('s3',\n",
    "                  s3_region, \n",
    "                  endpoint_url = s3_endpoint_url,\n",
    "                  aws_access_key_id = s3_access_key_id,\n",
    "                  aws_secret_access_key = s3_secret_access_key)\n",
    "\n",
    "# download the sample data file\n",
    "url = \"https://gitlab.com/opendatahub/opendatahub.io/raw/master/assets/files/tutorials/basic/sample_data.csv\"\n",
    "file = wget.download(url=url, out='sample_data.csv')\n",
    "    \n",
    "#upload the file to storage\n",
    "s3.upload_file(file, s3_bucket, \"sample_data.csv\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark + Object Storage\n",
    "\n",
    "Now, let's access that same data file from Spark so you can analyze data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoopConf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.endpoint\", s3_endpoint_url)\n",
    "hadoopConf.set(\"fs.s3a.access.key\", s3_access_key_id)\n",
    "hadoopConf.set(\"fs.s3a.secret.key\", s3_secret_access_key)\n",
    "hadoopConf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hadoopConf.set(\"fs.s3a.connection.ssl.enabled\", \"true\") # false if not https\n",
    "    \n",
    "data = spark.read.csv('s3a://' + s3_bucket + '/sample_data.csv',sep=\",\", header=True)\n",
    "df = data.toPandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
